#!/usr/bin/env python3
"""
SAM 3D GUI - Interactive SAM Annotation Web Interface
ëŒ€í™”í˜• SAM annotation: point í´ë¦­ìœ¼ë¡œ fg/bg ì§€ì • + ë¹„ë””ì˜¤ propagation
"""

import sys
import gradio as gr
import numpy as np
import cv2
import torch
from pathlib import Path
from typing import Optional, Tuple, List, Dict
import json
import os
import logging

# Add src directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

# Set environment variable to skip SAM3D init (which requires missing module)
os.environ['LIDRA_SKIP_INIT'] = '1'

# ==========================================
# ë¡œê¹… ì„¤ì •
# ==========================================
def setup_logging():
    """ë””ë²„ê·¸ ëª¨ë“œì— ë”°ë¥¸ ë¡œê¹… ì„¤ì •"""
    debug_mode = os.environ.get('SAM3D_DEBUG', '0') == '1'

    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    log_level = logging.DEBUG if debug_mode else logging.INFO

    # í¬ë§· ì„¤ì •
    if debug_mode:
        log_format = '%(asctime)s [%(levelname)s] %(name)s:%(lineno)d - %(message)s'
    else:
        log_format = '%(asctime)s [%(levelname)s] %(message)s'

    # ê¸°ë³¸ ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=log_level,
        format=log_format,
        datefmt='%H:%M:%S',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )

    # ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ë ˆë²¨ ì¡°ì • (ë„ˆë¬´ verbose ë°©ì§€)
    if not debug_mode:
        logging.getLogger('PIL').setLevel(logging.WARNING)
        logging.getLogger('matplotlib').setLevel(logging.WARNING)
        logging.getLogger('urllib3').setLevel(logging.WARNING)

    logger = logging.getLogger('sam3d_gui')
    logger.setLevel(log_level)

    if debug_mode:
        logger.info("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
        logger.debug(f"Python: {sys.version}")
        logger.debug(f"PyTorch: {torch.__version__}")
        logger.debug(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            logger.debug(f"CUDA device: {torch.cuda.get_device_name(0)}")
            logger.debug(f"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    return logger

# ë¡œê±° ì´ˆê¸°í™”
logger = setup_logging()

from sam3d_processor import SAM3DProcessor
from config_loader import ModelConfig
from lite_annotator import LiteAnnotator
from augmentation import DataAugmentor, generate_augmentation_configs

# Load configuration
try:
    config = ModelConfig()
    print(f"âœ“ Config loaded from: {Path(__file__).parent.parent / 'config' / 'model_config.yaml'}")
except Exception as e:
    print(f"Warning: Failed to load config: {e}")
    config = None

# SAM 2 imports
# Try to import SAM2 from installed package (via pip install)
try:
    from sam2.sam2_image_predictor import SAM2ImagePredictor
    from sam2.sam2_video_predictor import SAM2VideoPredictor
    SAM2_AVAILABLE = True
    print("âœ“ SAM2 package found (installed via pip)")
except ImportError:
    # Fallback: Try legacy path-based import
    SAM2_PATH = Path.home() / 'dev/segment-anything-2'
    if SAM2_PATH.exists():
        sys.path.insert(0, str(SAM2_PATH))
        try:
            from sam2.sam2_image_predictor import SAM2ImagePredictor
            from sam2.sam2_video_predictor import SAM2VideoPredictor
            SAM2_AVAILABLE = True
            print(f"âœ“ SAM2 found at legacy path: {SAM2_PATH}")
        except ImportError:
            SAM2ImagePredictor = None
            SAM2VideoPredictor = None
            SAM2_AVAILABLE = False
            print("Warning: SAM 2 not found. Interactive segmentation will use fallback method.")
    else:
        SAM2ImagePredictor = None
        SAM2VideoPredictor = None
        SAM2_AVAILABLE = False
        print("Warning: SAM 2 not found. Interactive segmentation will use fallback method.")

class SAMInteractiveWebApp:
    """
    SAM 3D GUI - í†µí•© ì›¹ ì¸í„°í˜ì´ìŠ¤

    ëª¨ë“œ 1: ëŒ€í™”í˜• Annotation (Interactive Mode)
    - Point annotation (foreground/background)
    - ìˆ˜ë™ ì„¸ê·¸ë©˜í…Œì´ì…˜ â†’ Propagation â†’ ê²°ê³¼

    ëª¨ë“œ 2: ì¼ê´„ ì²˜ë¦¬ (Batch Mode)
    - ë‹¤ì¤‘ ë¹„ë””ì˜¤ ì¼ê´„ ì²˜ë¦¬, ì„¸ì…˜ ê´€ë¦¬

    ëª¨ë“œ 3: Lite Annotator
    - íš¨ìœ¨ì  ë‹¨ì¼ í”„ë ˆì„ annotation
    """

    # SAM2 ì²´í¬í¬ì¸íŠ¸ ê¸°ë³¸ ê²½ë¡œ
    SAM2_CHECKPOINT_PATH = Path(__file__).parent.parent / "checkpoints" / "sam2" / "sam2_hiera_large.pt"
    SAM2_DOWNLOAD_URL = "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt"

    def __init__(self):
        # Config-based initialization
        self.config = config

        # SAM 3D processor ì´ˆê¸°í™”
        if config:
            sam3d_checkpoint = config.sam3d_checkpoint_dir
            self.processor = SAM3DProcessor(sam3d_checkpoint_path=sam3d_checkpoint)
        else:
            self.processor = SAM3DProcessor()

        # SAM2 predictor ì´ˆê¸°í™” (Interactive Modeìš©)
        self.sam2_predictor = None
        self.sam2_video_predictor = None
        self.sam2_device = None
        if SAM2_AVAILABLE and config:
            try:
                print("Loading SAM 2 for interactive segmentation...")
                checkpoint = Path(config.sam2_checkpoint)
                model_cfg = config.sam2_config
                device = config.sam2_device

                # Auto-detect device
                if device == "auto":
                    if torch.cuda.is_available():
                        device = "cuda"
                        gpu_name = torch.cuda.get_device_name(0)
                        print(f"âœ“ CUDA detected: {gpu_name}")
                    else:
                        device = "cpu"
                        print("Warning: CUDA not available, using CPU")
                elif device == "cuda" and not torch.cuda.is_available():
                    device = "cpu"
                    print("Warning: CUDA not available, using CPU")

                self.sam2_device = device

                if checkpoint.exists():
                    from sam2.build_sam import build_sam2, build_sam2_video_predictor

                    # Image predictor for single-frame segmentation
                    sam2_model = build_sam2(model_cfg, str(checkpoint), device=device)
                    self.sam2_predictor = SAM2ImagePredictor(sam2_model)

                    # Video predictor for memory-based tracking
                    self.sam2_video_predictor = build_sam2_video_predictor(model_cfg, str(checkpoint), device=device)

                    print(f"âœ“ SAM 2 loaded: {config.cfg.sam2.name} on {device}")
                    print(f"âœ“ SAM 2 Video Predictor initialized for propagation")
                else:
                    print(f"Warning: SAM 2 checkpoint not found at {checkpoint}")
            except Exception as e:
                print(f"Warning: Failed to load SAM 2: {e}")
                import traceback
                traceback.print_exc()
                self.sam2_predictor = None
                self.sam2_video_predictor = None

        # ìƒíƒœ ê´€ë¦¬
        self.video_path = None
        self.frames = []
        self.current_frame_idx = 0
        self.annotations = {
            'foreground': [],  # [(x, y), ...]
            'background': []   # [(x, y), ...]
        }
        self.masks = []  # ê° í”„ë ˆì„ì˜ ë§ˆìŠ¤í¬
        self.current_mask = None
        self.tracking_result = None

        # í˜„ì¬ ë¡œë“œëœ ì„¸ì…˜ ê²½ë¡œ (ë®ì–´ì“°ê¸°ìš©)
        self.current_session_path = None

        # Default paths from config
        if config:
            self.default_data_dir = config.default_data_dir
            self.default_output_dir = config.output_dir
        else:
            # Fallback: data one level above project root, output inside
            project_root = Path(__file__).parent.parent
            self.default_data_dir = str(project_root.parent / "data" / "markerless_mouse")
            self.default_output_dir = str(project_root / "outputs")

        # Data Augmentor ì´ˆê¸°í™”
        self.augmentor = DataAugmentor()
        self.augmentation_preview = None

        # LiteAnnotator ì´ˆê¸°í™” (Tab 3: Lite Mode)
        self.lite_annotator = None
        if SAM2_AVAILABLE:
            try:
                print("Initializing Lite Annotator...")
                # Try to find SAM2 base path
                sam2_base_path = None

                # Option 1: Legacy path
                legacy_path = Path.home() / 'dev/segment-anything-2'
                if legacy_path.exists():
                    sam2_base_path = legacy_path

                # Option 2: Use None (LiteAnnotator will use installed package)
                self.lite_annotator = LiteAnnotator(
                    sam2_base_path=sam2_base_path,
                    device=self.sam2_device if self.sam2_device else "auto"
                )
                print("âœ“ Lite Annotator initialized")
            except Exception as e:
                print(f"Warning: Failed to initialize Lite Annotator: {e}")
                self.lite_annotator = None

    def check_sam2_available(self) -> Tuple[bool, str]:
        """
        SAM2 ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸

        Returns:
            (available, status_message)
        """
        if not SAM2_AVAILABLE:
            return False, "SAM2 íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `pip install sam2` ì‹¤í–‰ í•„ìš”"

        if self.sam2_predictor is None or self.sam2_video_predictor is None:
            checkpoint = self.SAM2_CHECKPOINT_PATH
            if not checkpoint.exists():
                return False, f"SAM2 ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤: {checkpoint}"
            return False, "SAM2 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        return True, f"SAM2 ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ({self.sam2_device})"

    def download_sam2_checkpoint(self, progress_callback=None) -> Tuple[bool, str]:
        """
        SAM2 ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ

        Args:
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (0.0 ~ 1.0)

        Returns:
            (success, message)
        """
        import urllib.request
        import ssl

        checkpoint_path = self.SAM2_CHECKPOINT_PATH
        checkpoint_dir = checkpoint_path.parent

        # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
        if checkpoint_path.exists():
            return True, f"ì²´í¬í¬ì¸íŠ¸ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {checkpoint_path}"

        try:
            # ë””ë ‰í† ë¦¬ ìƒì„±
            checkpoint_dir.mkdir(parents=True, exist_ok=True)

            print(f"ğŸ“¥ SAM2 ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            print(f"   URL: {self.SAM2_DOWNLOAD_URL}")
            print(f"   ì €ì¥ ìœ„ì¹˜: {checkpoint_path}")

            # SSL context ì„¤ì •
            ssl_context = ssl.create_default_context()

            # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ ë‹¤ìš´ë¡œë“œ
            def reporthook(block_num, block_size, total_size):
                if total_size > 0:
                    downloaded = block_num * block_size
                    percent = min(downloaded / total_size, 1.0)
                    if progress_callback:
                        progress_callback(percent)
                    # 10% ë‹¨ìœ„ë¡œ ì¶œë ¥
                    if int(percent * 10) > int((downloaded - block_size) / total_size * 10):
                        print(f"   ë‹¤ìš´ë¡œë“œ ì§„í–‰: {percent*100:.0f}%")

            urllib.request.urlretrieve(
                self.SAM2_DOWNLOAD_URL,
                str(checkpoint_path),
                reporthook=reporthook
            )

            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = checkpoint_path.stat().st_size / (1024 * 1024)
            print(f"âœ… SAM2 ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_size:.1f} MB")

            return True, f"SAM2 ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({file_size:.1f} MB)"

        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ ë¶€ë¶„ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì‚­ì œ
            if checkpoint_path.exists():
                checkpoint_path.unlink()
            return False, f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}"

    def load_sam2_models(self) -> Tuple[bool, str]:
        """
        SAM2 ëª¨ë¸ ë¡œë“œ (ì²´í¬í¬ì¸íŠ¸ê°€ ìˆì–´ì•¼ í•¨)

        Returns:
            (success, message)
        """
        if not SAM2_AVAILABLE:
            return False, "SAM2 íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        checkpoint = self.SAM2_CHECKPOINT_PATH

        # configì—ì„œ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° (ìˆìœ¼ë©´)
        if self.config:
            config_checkpoint = Path(self.config.sam2_checkpoint)
            if config_checkpoint.exists():
                checkpoint = config_checkpoint

        if not checkpoint.exists():
            return False, f"SAM2 ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint}"

        try:
            from sam2.build_sam import build_sam2, build_sam2_video_predictor

            # Device ì„¤ì •
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.sam2_device = device

            model_cfg = self.config.sam2_config if self.config else "sam2_hiera_l.yaml"

            print(f"ğŸ”„ SAM2 ëª¨ë¸ ë¡œë”© ì¤‘... (device: {device})")

            # Image predictor
            sam2_model = build_sam2(model_cfg, str(checkpoint), device=device)
            self.sam2_predictor = SAM2ImagePredictor(sam2_model)

            # Video predictor
            self.sam2_video_predictor = build_sam2_video_predictor(model_cfg, str(checkpoint), device=device)

            # Lite Annotatorì— ê³µìš© predictor ì „ë‹¬
            if self.lite_annotator is not None:
                self.lite_annotator.set_predictor(self.sam2_predictor, "shared-large")
                print(f"  â””â”€ Lite Annotatorì— ê³µìš© predictor ì „ë‹¬ë¨")

            print(f"âœ… SAM2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True, f"SAM2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (device: {device})"

        except Exception as e:
            import traceback
            traceback.print_exc()
            return False, f"SAM2 ë¡œë“œ ì‹¤íŒ¨: {str(e)}"

    def _get_sam2_status_markdown(self) -> str:
        """SAM2 ìƒíƒœë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ëª¨ë¸ ì •ë³´ í¬í•¨)"""
        # ëª¨ë¸ ì •ë³´
        model_name = "Hiera Large"
        model_size = "~897MB"

        if self.sam2_predictor is not None and self.sam2_video_predictor is not None:
            return f"âœ… **SAM2** ({model_name}) - {self.sam2_device}"
        elif not SAM2_AVAILABLE:
            return f"âŒ **SAM2**: íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜ (`pip install sam2`)"
        else:
            checkpoint = self.SAM2_CHECKPOINT_PATH
            if self.config:
                config_checkpoint = Path(self.config.sam2_checkpoint)
                if config_checkpoint.exists():
                    checkpoint = config_checkpoint

            if not checkpoint.exists():
                return f"âš ï¸ **SAM2** ({model_name}, {model_size}) - ë‹¤ìš´ë¡œë“œ í•„ìš”"
            else:
                return f"âš ï¸ **SAM2** ({model_name}) - ë²„íŠ¼ í´ë¦­í•˜ì—¬ ë¡œë“œ"

    def ensure_sam2_ready(self, progress_callback=None) -> Tuple[bool, str]:
        """
        SAM2 ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ í›„ ë¡œë“œ

        Args:
            progress_callback: ì§„í–‰ë¥  ì½œë°±

        Returns:
            (success, message)
        """
        # ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ OK
        if self.sam2_predictor is not None and self.sam2_video_predictor is not None:
            return True, "SAM2 ëª¨ë¸ ì‚¬ìš© ì¤€ë¹„ë¨"

        if not SAM2_AVAILABLE:
            return False, "âŒ SAM2 íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n`pip install sam2` ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”."

        # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
        checkpoint = self.SAM2_CHECKPOINT_PATH
        if self.config:
            config_checkpoint = Path(self.config.sam2_checkpoint)
            if config_checkpoint.exists():
                checkpoint = config_checkpoint

        # ì²´í¬í¬ì¸íŠ¸ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
        if not checkpoint.exists():
            print("ğŸ“¥ SAM2 ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ìë™ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            success, msg = self.download_sam2_checkpoint(progress_callback)
            if not success:
                return False, f"âŒ SAM2 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {msg}"

        # ëª¨ë¸ ë¡œë“œ
        success, msg = self.load_sam2_models()
        return success, msg

    def unload_sam2_models(self):
        """
        Unload SAM2 models to free GPU memory before SAM 3D inference

        This is critical for RTX 3060 12GB where:
        - SAM2 Large uses ~2-3GB
        - SAM3D uses ~8-10GB
        - Total 11-13GB > 12GB available

        By unloading SAM2 before SAM3D, we free ~3GB for SAM3D inference.
        """
        import gc

        if self.sam2_predictor is not None or self.sam2_video_predictor is not None:
            print("\nğŸ§¹ SAM2 ëª¨ë¸ ì–¸ë¡œë“œ ì‹œì‘ (ë©”ëª¨ë¦¬ í™•ë³´)...")

            # Print memory before cleanup
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                print(f"   í˜„ì¬ GPU ë©”ëª¨ë¦¬: {allocated:.2f} GB")

            # Delete SAM2 image predictor
            if self.sam2_predictor is not None:
                del self.sam2_predictor
                self.sam2_predictor = None
                print("   âœ“ SAM2 Image Predictor í•´ì œ")

            # Delete SAM2 video predictor
            if self.sam2_video_predictor is not None:
                del self.sam2_video_predictor
                self.sam2_video_predictor = None
                print("   âœ“ SAM2 Video Predictor í•´ì œ")

            # Force garbage collection
            gc.collect()

            # Clear CUDA cache
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                print("   âœ“ CUDA ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

                # Print memory after cleanup
                allocated_after = torch.cuda.memory_allocated(0) / 1024**3
                freed = allocated - allocated_after
                print(f"   âœ“ GPU ë©”ëª¨ë¦¬ í•´ì œ: {freed:.2f} GB")
                print(f"   í˜„ì¬ GPU ë©”ëª¨ë¦¬: {allocated_after:.2f} GB")

            print("âœ… SAM2 ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ\n")
        else:
            print("â„¹ï¸  SAM2 ëª¨ë¸ì´ ì´ë¯¸ ì–¸ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    def reload_sam2_models(self):
        """
        Reload SAM2 models after SAM 3D inference completes

        This allows users to continue using interactive segmentation after 3D reconstruction.
        """
        if not SAM2_AVAILABLE or not self.config:
            print("âš ï¸  SAM2ë¥¼ ë‹¤ì‹œ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (SAM2 unavailable or no config)")
            return

        if self.sam2_predictor is not None and self.sam2_video_predictor is not None:
            print("â„¹ï¸  SAM2 ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        print("\nğŸ”„ SAM2 ëª¨ë¸ ì¬ë¡œë“œ ì¤‘...")

        try:
            checkpoint = Path(self.config.sam2_checkpoint)
            model_cfg = self.config.sam2_config
            device = self.sam2_device

            if checkpoint.exists():
                from sam2.build_sam import build_sam2, build_sam2_video_predictor

                # Rebuild models
                sam2_model = build_sam2(model_cfg, str(checkpoint), device=device)
                self.sam2_predictor = SAM2ImagePredictor(sam2_model)
                self.sam2_video_predictor = build_sam2_video_predictor(model_cfg, str(checkpoint), device=device)

                print(f"âœ… SAM2 ëª¨ë¸ ì¬ë¡œë“œ ì™„ë£Œ (device: {device})\n")
            else:
                print(f"âŒ SAM2 checkpointë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint}")
        except Exception as e:
            print(f"âŒ SAM2 ì¬ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

    def quick_process(self, data_dir: str, video_file: str,
                     start_time: float, duration: float,
                     motion_threshold: float, segmentation_method: str,
                     progress=gr.Progress()) -> Tuple[np.ndarray, str]:
        """
        Quick Mode: ìë™ ì²˜ë¦¬ (ê¸°ì¡´ web_app.py ê¸°ëŠ¥ í†µí•©)
        """
        if not video_file:
            return None, "ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”"

        video_path = Path(data_dir) / video_file
        if not video_path.exists():
            return None, f"ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}"

        self.video_path = str(video_path)

        progress(0, desc="ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘...")

        try:
            # ë¹„ë””ì˜¤ ì²˜ë¦¬
            result, reconstruction = self.processor.process_video_segment(
                video_path=self.video_path,
                start_time=start_time,
                duration=duration,
                output_dir="outputs/",
                motion_threshold=motion_threshold,
                segmentation_method=segmentation_method
            )

            self.tracking_result = result

            progress(0.5, desc="ê²°ê³¼ ìƒì„± ì¤‘...")

            # ê²°ê³¼ ì‹œê°í™”
            if result.segments:
                first_frame = result.segments[0]

                # í”„ë ˆì„ ë‹¤ì‹œ ì½ê¸°
                cap = cv2.VideoCapture(self.video_path)
                cap.set(cv2.CAP_PROP_POS_FRAMES, first_frame.frame_idx)
                ret, frame = cap.read()
                cap.release()

                if ret:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    mask_colored = np.zeros_like(frame_rgb)
                    mask_colored[first_frame.mask > 0] = [0, 255, 0]
                    overlay = cv2.addWeighted(frame_rgb, 0.7, mask_colored, 0.3, 0)

                    if first_frame.bbox:
                        x1, y1, x2, y2 = first_frame.bbox
                        cv2.rectangle(overlay, (x1, y1), (x2, y2), (255, 0, 0), 2)

                    visualization = overlay
                else:
                    visualization = None
            else:
                visualization = None

            progress(0.8, desc="í†µê³„ ê³„ì‚° ì¤‘...")

            # ê²°ê³¼ í…ìŠ¤íŠ¸
            result_text = f"""
### ğŸ¯ Quick Process ì™„ë£Œ

**ê¸°ë³¸ ì •ë³´**
- ë¶„ì„ëœ í”„ë ˆì„: {len(result.segments)}
- ì²˜ë¦¬ ì‹œê°„: {start_time}s - {start_time + duration}s

**ëª¨ì…˜ ê°ì§€**
- ê°ì§€ ì—¬ë¶€: {'âœ… ì˜ˆ' if result.motion_detected else 'âŒ ì•„ë‹ˆì˜¤'}
- ì„ê³„ê°’: {motion_threshold} í”½ì…€
"""

            if result.motion_detected and len(result.segments) > 1:
                displacements = []
                for i in range(1, len(result.segments)):
                    prev = result.segments[i-1].center
                    curr = result.segments[i].center
                    dx = curr[0] - prev[0]
                    dy = curr[1] - prev[1]
                    disp = (dx**2 + dy**2)**0.5
                    displacements.append(disp)

                if displacements:
                    result_text += f"""
**ë³€ìœ„ í†µê³„**
- ìµœëŒ€ ë³€ìœ„: {max(displacements):.1f} í”½ì…€
- í‰ê·  ë³€ìœ„: {sum(displacements)/len(displacements):.1f} í”½ì…€
"""

            if result.segments:
                first = result.segments[0]
                result_text += f"""
**ê°ì²´ ì •ë³´**
- ë°”ìš´ë”© ë°•ìŠ¤: {first.bbox}
- ì¤‘ì‹¬ì : {first.center}
- ë©´ì : {first.area:.0f} í”½ì…€Â²

**ì¶œë ¥**
- ì €ì¥ ìœ„ì¹˜: `outputs/`
"""

            progress(1.0, desc="ì™„ë£Œ!")

            return visualization, result_text

        except Exception as e:
            import traceback
            error_msg = f"ì˜¤ë¥˜:\n```\n{str(e)}\n{traceback.format_exc()}\n```"
            return None, error_msg

    def scan_videos(self, data_dir: str) -> List[str]:
        """ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ ìŠ¤ìº”"""
        data_path = Path(data_dir)
        if not data_path.exists():
            return []

        video_extensions = ['.mp4', '.avi', '.mov', '.mkv']
        videos = []

        for ext in video_extensions:
            videos.extend([str(p.relative_to(data_path))
                          for p in data_path.rglob(f'*{ext}')])

        return sorted(videos)

    def calculate_stride_from_target(self, target_frames: int) -> int:
        """
        ëª©í‘œ í”„ë ˆì„ ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ stride ìë™ ê³„ì‚°

        Args:
            target_frames: ì²˜ë¦¬í•  ëª©í‘œ í”„ë ˆì„ ìˆ˜

        Returns:
            ê³„ì‚°ëœ stride ê°’
        """
        if len(self.frames) == 0:
            return 1

        stride = max(1, len(self.frames) // target_frames)
        return stride

    def _extract_subject_id(self, video_path: str) -> Optional[str]:
        """
        ë¹„ë””ì˜¤ ê²½ë¡œì—ì„œ subject ID (ì˜ˆ: mouse_1, mouse_2) ì¶”ì¶œ

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

        Returns:
            subject ID ë¬¸ìì—´ ë˜ëŠ” None
        """
        import re
        # mouse_1, mouse_2 ë“±ì˜ íŒ¨í„´ ì°¾ê¸°
        match = re.search(r'mouse_(\d+)', video_path, re.IGNORECASE)
        if match:
            return f"mouse_{match.group(1)}"

        # subject_1, subject_2 íŒ¨í„´ë„ ì§€ì›
        match = re.search(r'subject_(\d+)', video_path, re.IGNORECASE)
        if match:
            return f"subject_{match.group(1)}"

        return None

    def _extract_camera_id(self, video_path: str) -> Optional[str]:
        """
        ë¹„ë””ì˜¤ ê²½ë¡œì—ì„œ camera ID (ì˜ˆ: Camera1, cam2) ì¶”ì¶œ

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

        Returns:
            camera ID ë¬¸ìì—´ ë˜ëŠ” None
        """
        import re
        # Camera1, Camera2 íŒ¨í„´
        match = re.search(r'Camera(\d+)', video_path, re.IGNORECASE)
        if match:
            return f"cam{match.group(1)}"

        # cam1, cam2 íŒ¨í„´
        match = re.search(r'cam(\d+)', video_path, re.IGNORECASE)
        if match:
            return f"cam{match.group(1)}"

        # view1, view2 íŒ¨í„´
        match = re.search(r'view(\d+)', video_path, re.IGNORECASE)
        if match:
            return f"view{match.group(1)}"

        return None

    def _generate_unique_video_id(self, video_path: str) -> str:
        """
        ë¹„ë””ì˜¤ ê²½ë¡œì—ì„œ ê³ ìœ í•œ ID ìƒì„± (mouse + camera + íŒŒì¼ëª…)

        ì˜ˆ: /media/.../mouse_1/Camera1/0.mp4 -> "m1_cam1_0"

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

        Returns:
            ê³ ìœ í•œ ë¹„ë””ì˜¤ ID ë¬¸ìì—´
        """
        import re
        parts = []

        # Subject ID ì¶”ì¶œ
        subject_match = re.search(r'mouse_(\d+)', video_path, re.IGNORECASE)
        if subject_match:
            parts.append(f"m{subject_match.group(1)}")
        else:
            subject_match = re.search(r'subject_(\d+)', video_path, re.IGNORECASE)
            if subject_match:
                parts.append(f"s{subject_match.group(1)}")

        # Camera ID ì¶”ì¶œ
        camera_match = re.search(r'Camera(\d+)', video_path, re.IGNORECASE)
        if camera_match:
            parts.append(f"cam{camera_match.group(1)}")
        else:
            camera_match = re.search(r'cam(\d+)', video_path, re.IGNORECASE)
            if camera_match:
                parts.append(f"cam{camera_match.group(1)}")

        # íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)
        filename = Path(video_path).stem
        parts.append(filename)

        if parts:
            return "_".join(parts)
        else:
            # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì „ì²´ ê²½ë¡œ ê¸°ë°˜ í•´ì‹œ
            return Path(video_path).stem

    def _format_video_label_with_subject(self, video_path: str, video_name: str, base_path: Path = None) -> str:
        """
        ë¹„ë””ì˜¤ ë ˆì´ë¸” ìƒì„± (unique_id í˜•ì‹: m1_cam1_frame)

        Args:
            video_path: ì „ì²´ ë¹„ë””ì˜¤ ê²½ë¡œ
            video_name: ë¹„ë””ì˜¤ íŒŒì¼ëª…
            base_path: ê¸°ì¤€ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ ê³„ì‚°ìš©)

        Returns:
            í¬ë§·ëœ ë ˆì´ë¸” ë¬¸ìì—´ (ì˜ˆ: m1_cam1_0)
        """
        unique_id = self._generate_unique_video_id(video_path)

        # unique_idê°€ video_nameê³¼ ë‹¤ë¥´ë©´ unique_id ì‚¬ìš©, ê°™ìœ¼ë©´ ìƒëŒ€ê²½ë¡œ ì‚¬ìš©
        if unique_id != video_name:
            return unique_id

        if base_path:
            rel_path = str(Path(video_path).relative_to(base_path))
            return rel_path

        return video_name

    def scan_batch_videos(self, data_dir: str, pattern: str = "*.mp4") -> Tuple[List[str], str, gr.CheckboxGroup]:
        """
        í´ë” ë‚´ ëª¨ë“  ë¹„ë””ì˜¤ ìŠ¤ìº” ë° ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ (recursive)
        í´ë”ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‘œì‹œ

        Args:
            data_dir: ë¹„ë””ì˜¤ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬
            pattern: ë¹„ë””ì˜¤ íŒŒì¼ íŒ¨í„´ (ì˜ˆ: *.mp4, *.avi)

        Returns:
            (ë¹„ë””ì˜¤ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸, ìƒíƒœ ë©”ì‹œì§€, CheckboxGroup ì—…ë°ì´íŠ¸)
        """
        try:
            data_path = Path(data_dir)
            if not data_path.exists():
                empty_checkbox = gr.CheckboxGroup(choices=[], value=[])
                return [], f"âŒ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}", empty_checkbox

            # ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸° (recursive)
            video_files = sorted(data_path.rglob(pattern))

            if not video_files:
                empty_checkbox = gr.CheckboxGroup(choices=[], value=[])
                return [], f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pattern} (recursive íƒìƒ‰)", empty_checkbox

            # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            total_frames = 0
            total_duration = 0
            video_info = []

            for video_path in video_files:
                try:
                    info = self.processor.get_video_info(str(video_path))
                    total_frames += info['frame_count']
                    total_duration += info['duration']
                    video_info.append({
                        'path': str(video_path),
                        'name': video_path.name,
                        'frames': info['frame_count'],
                        'duration': info['duration'],
                        'fps': info['fps'],
                        'resolution': f"{info['width']}x{info['height']}"
                    })
                except Exception as e:
                    print(f"âš ï¸ {video_path.name} ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {e}")
                    continue

            # í‰ê·  FPS ê³„ì‚°
            avg_fps = total_frames / total_duration if total_duration > 0 else 0

            # ìƒíƒœ ë©”ì‹œì§€ ìƒì„±
            status = f"""
### ğŸ“‚ Batch ë¹„ë””ì˜¤ ìŠ¤ìº” ì™„ë£Œ âœ…

- **ë¹„ë””ì˜¤ íŒŒì¼ ìˆ˜**: {len(video_info)}
- **ì´ í”„ë ˆì„ ìˆ˜**: {total_frames:,}
- **ì´ ê¸¸ì´**: {total_duration:.1f}ì´ˆ ({total_duration/60:.1f}ë¶„)
- **í‰ê·  FPS**: {avg_fps:.1f}

<details>
<summary><b>ğŸ“‹ ë¹„ë””ì˜¤ ëª©ë¡ ({len(video_info)}ê°œ) - í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°/ì ‘ê¸°</b></summary>

"""
            for idx, info in enumerate(video_info, 1):
                # ì „ì²´ ìƒëŒ€ ê²½ë¡œ í‘œì‹œ
                rel_path = str(Path(info['path']).relative_to(data_path))
                status += f"\n{idx}. **{rel_path}**"
                status += f"\n   - í”„ë ˆì„: {info['frames']}, ê¸¸ì´: {info['duration']:.1f}ì´ˆ, FPS: {info['fps']:.1f}, í•´ìƒë„: {info['resolution']}\n"

            status += "\n</details>"

            # ë¹„ë””ì˜¤ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            video_paths = [str(v) for v in video_files]
            self.batch_videos = video_paths
            self.batch_video_info = video_info

            # CheckboxGroup ì—…ë°ì´íŠ¸ - subject ID í¬í•¨í•˜ì—¬ í‘œì‹œ
            video_labels = [
                self._format_video_label_with_subject(info['path'], info['name'], data_path)
                for info in video_info
            ]

            updated_checkbox = gr.CheckboxGroup(
                choices=video_labels,
                value=video_labels,  # ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë‘ ì„ íƒ
                label="ğŸ¬ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ì„ íƒ (Subject ID + ê²½ë¡œ)",
                info="ì„ íƒëœ ë¹„ë””ì˜¤ë§Œ ì²˜ë¦¬ë©ë‹ˆë‹¤"
            )

            # ë ˆì´ë¸”ê³¼ ì‹¤ì œ ê²½ë¡œ ë§¤í•‘ ì €ì¥
            self.batch_video_label_map = dict(zip(video_labels, video_paths))

            return video_paths, status, updated_checkbox

        except Exception as e:
            import traceback
            empty_checkbox = gr.CheckboxGroup(choices=[], value=[])
            return [], f"âŒ ìŠ¤ìº” ì‹¤íŒ¨:\n{str(e)}\n{traceback.format_exc()}", empty_checkbox

    def batch_load_reference_frame(self, selected_videos: List[str]) -> Tuple[np.ndarray, str]:
        """
        Batch ëª¨ë“œì—ì„œ ì„ íƒëœ ë¹„ë””ì˜¤ ì¤‘ ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ì˜ ì²« í”„ë ˆì„ì„ referenceë¡œ ë¡œë“œ

        Args:
            selected_videos: ì„ íƒëœ ë¹„ë””ì˜¤ ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸

        Returns:
            (reference_frame, status_message)
        """
        if not hasattr(self, 'batch_videos') or not self.batch_videos:
            return None, "âŒ ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ìŠ¤ìº”í•˜ì„¸ìš”"

        if not selected_videos or len(selected_videos) == 0:
            return None, "âŒ ìµœì†Œ 1ê°œì˜ ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”"

        try:
            # ì„ íƒëœ ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ì˜ ì‹¤ì œ ê²½ë¡œ ì°¾ê¸°
            first_selected_label = selected_videos[0]

            if hasattr(self, 'batch_video_label_map') and first_selected_label in self.batch_video_label_map:
                first_video_path = self.batch_video_label_map[first_selected_label]
            else:
                # Fallback: ì „ì²´ ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸
                first_video_path = self.batch_videos[0]

            # ì²« í”„ë ˆì„ ì¶”ì¶œ
            frames = self.processor.extract_frames(
                first_video_path,
                start_frame=0,
                num_frames=1,
                stride=1
            )

            if not frames:
                return None, "âŒ Reference í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨"

            # í”„ë ˆì„ ì €ì¥ (annotationìš©)
            self.frames = frames
            self.current_frame_idx = 0
            self.annotations = {'foreground': [], 'background': []}
            self.masks = [None] * len(frames)

            # RGB ë³€í™˜ (GradioëŠ” RGB ì‚¬ìš©)
            frame_rgb = cv2.cvtColor(frames[0], cv2.COLOR_BGR2RGB)

            status = f"""
### âœ… Reference í”„ë ˆì„ ë¡œë“œ ì™„ë£Œ

- **ì„ íƒëœ ë¹„ë””ì˜¤**: {len(selected_videos)}ê°œ ì¤‘ ì²« ë²ˆì§¸
- **Reference ë¹„ë””ì˜¤**: {first_selected_label}
- **íŒŒì¼ëª…**: {Path(first_video_path).name}
- **í•´ìƒë„**: {frame_rgb.shape[1]} x {frame_rgb.shape[0]}

ì´ì œ ì´ë¯¸ì§€ë¥¼ í´ë¦­í•˜ì—¬ annotationì„ ì¶”ê°€í•˜ì„¸ìš”.
"""

            return frame_rgb, status

        except Exception as e:
            import traceback
            return None, f"âŒ Reference í”„ë ˆì„ ë¡œë“œ ì‹¤íŒ¨:\n{str(e)}\n{traceback.format_exc()}"

    def batch_propagate_videos(
        self,
        target_frames: int = 100,
        selected_videos: List[str] = None,
        progress=gr.Progress()
    ) -> Tuple[str, str]:
        """
        ì—¬ëŸ¬ ë¹„ë””ì˜¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (Batch Processing)

        ê° ë¹„ë””ì˜¤ë§ˆë‹¤:
        1. ë¡œë“œ (ëª©í‘œ í”„ë ˆì„ ìˆ˜ì— ë§ì¶° stride ìë™ ê³„ì‚°)
        2. í˜„ì¬ annotationìœ¼ë¡œ propagate
        3. ê²°ê³¼ ì„ì‹œ ì €ì¥
        4. ë©”ëª¨ë¦¬ í•´ì œ

        Args:
            target_frames: ê° ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œí•  ëª©í‘œ í”„ë ˆì„ ìˆ˜
            selected_videos: ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
            progress: Gradio progress bar

        Returns:
            (ìƒíƒœ ë©”ì‹œì§€, ì™„ë£Œ ë©”ì‹œì§€)
        """
        if not hasattr(self, 'batch_videos') or not self.batch_videos:
            return "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ìŠ¤ìº”í•˜ì„¸ìš”", "âŒ ë¹„ë””ì˜¤ ì—†ìŒ"

        if len(self.annotations['foreground']) == 0:
            return "Annotationì´ í•„ìš”í•©ë‹ˆë‹¤ (ìµœì†Œ 1ê°œì˜ foreground point)", "âŒ Annotation ì—†ìŒ"

        try:
            import tempfile
            import shutil
            import torch

            # ì„ì‹œ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            batch_temp_dir = Path(tempfile.mkdtemp(prefix="sam3d_batch_"))

            # Reference annotation ì €ì¥
            reference_annotations = {
                'foreground': self.annotations['foreground'].copy(),
                'background': self.annotations['background'].copy()
            }

            # ì„ íƒëœ ë¹„ë””ì˜¤ í•„í„°ë§ (ë ˆì´ë¸” â†’ ê²½ë¡œ ë§¤í•‘ ì‚¬ìš©)
            if selected_videos and len(selected_videos) > 0:
                # ì„ íƒëœ ë ˆì´ë¸”ì„ ì‹¤ì œ ê²½ë¡œë¡œ ë³€í™˜
                videos_to_process = []
                if hasattr(self, 'batch_video_label_map'):
                    for label in selected_videos:
                        if label in self.batch_video_label_map:
                            videos_to_process.append(self.batch_video_label_map[label])
                else:
                    # ë ˆì´ë¸” ë§µì´ ì—†ìœ¼ë©´ ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­ (í•˜ìœ„ í˜¸í™˜ì„±)
                    for video_path in self.batch_videos:
                        video_name = Path(video_path).name
                        if video_name in selected_videos:
                            videos_to_process.append(video_path)
            else:
                # ì„ íƒì´ ì—†ìœ¼ë©´ ëª¨ë“  ë¹„ë””ì˜¤ ì²˜ë¦¬
                videos_to_process = self.batch_videos

            if not videos_to_process:
                return "ì²˜ë¦¬í•  ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”", "âŒ ì„ íƒëœ ë¹„ë””ì˜¤ ì—†ìŒ"

            total_videos = len(videos_to_process)
            total_processed_frames = 0
            video_results = []

            progress(0, desc=f"Batch ì²˜ë¦¬ ì‹œì‘: {total_videos}ê°œ ë¹„ë””ì˜¤...")

            for video_idx, video_path in enumerate(videos_to_process):
                video_name = Path(video_path).name
                progress(video_idx / total_videos, desc=f"ì²˜ë¦¬ ì¤‘: {video_name} ({video_idx+1}/{total_videos})")

                print(f"\n{'='*80}")
                print(f"ğŸ“¹ ë¹„ë””ì˜¤ {video_idx+1}/{total_videos}: {video_name}")
                print(f"{'='*80}")

                # 1. ë¹„ë””ì˜¤ ë¡œë“œ (stride ê°„ê²©)
                # strideë¥¼ ì°¾ì„ ë•ŒëŠ” batch_video_infoì—ì„œ ì „ì²´ í”„ë ˆì„ ì°¾ì•„ì•¼ í•¨
                # í˜„ì¬ video_idxëŠ” videos_to_processì˜ ì¸ë±ìŠ¤ì´ë¯€ë¡œ, ì›ë³¸ video_infoë¥¼ ì°¾ì•„ì•¼ í•¨
                matching_info = None
                for info in self.batch_video_info:
                    if info['path'] == video_path:
                        matching_info = info
                        break

                if matching_info is None:
                    print(f"âš ï¸ {video_name}: ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê±´ë„ˆëœ€")
                    continue

                num_frames = matching_info['frames']

                # stride ê³„ì‚°: target_framesì— ë§ì¶° ìë™ ì¡°ì •
                # ëª©í‘œ: target_frames í”„ë ˆì„ì„ ì¶”ì¶œí•˜ë„ë¡ stride ê³„ì‚°
                # stride = num_frames // target_frames (ìµœì†Œ 1)
                # ì‹¤ì œ ì¶”ì¶œë˜ëŠ” í”„ë ˆì„ ìˆ˜: ceil(num_frames / stride)
                calculated_stride = max(1, num_frames // target_frames)
                actual_num_frames_to_extract = (num_frames + calculated_stride - 1) // calculated_stride

                frame_indices = list(range(0, num_frames, calculated_stride))

                print(f"âœ“ í”„ë ˆì„ ì¶”ì¶œ ê³„íš:")
                print(f"  - ì´ ë¹„ë””ì˜¤ í”„ë ˆì„: {num_frames}")
                print(f"  - ëª©í‘œ í”„ë ˆì„ ìˆ˜: {target_frames}")
                print(f"  - ê³„ì‚°ëœ stride: {calculated_stride}")
                print(f"  - ì‹¤ì œ ì¶”ì¶œ í”„ë ˆì„ ìˆ˜: {actual_num_frames_to_extract}")
                print(f"  - ê³µì‹: ceil({num_frames} / {calculated_stride}) = {actual_num_frames_to_extract}")

                # Extract frames
                frames = self.processor.extract_frames(video_path, 0, num_frames, stride=calculated_stride)
                if not frames:
                    print(f"âš ï¸ {video_name}: í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
                    continue

                # 2. Propagate (SAM 2 Video Predictor)
                print(f"âœ“ Propagation ì‹œì‘...")

                # ì„ì‹œ ë””ë ‰í† ë¦¬ì— í”„ë ˆì„ ì €ì¥
                video_temp_dir = tempfile.mkdtemp(prefix=f"sam3d_video_{video_idx}_")

                try:
                    for idx, frame in enumerate(frames):
                        frame_path = Path(video_temp_dir) / f"{idx:05d}.jpg"
                        # framesëŠ” RGBì´ë¯€ë¡œ BGRë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                        cv2.imwrite(str(frame_path), cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))

                    # SAM 2 inference
                    if self.sam2_video_predictor is not None:
                        inference_state = self.sam2_video_predictor.init_state(video_path=video_temp_dir)

                        # Reference annotations ì ìš© (ì²« í”„ë ˆì„)
                        point_coords = []
                        point_labels = []

                        for px, py in reference_annotations['foreground']:
                            point_coords.append([px, py])
                            point_labels.append(1)

                        for px, py in reference_annotations['background']:
                            point_coords.append([px, py])
                            point_labels.append(0)

                        point_coords = np.array(point_coords, dtype=np.float32)
                        point_labels = np.array(point_labels, dtype=np.int32)

                        # Add points to first frame
                        self.sam2_video_predictor.add_new_points_or_box(
                            inference_state=inference_state,
                            frame_idx=0,
                            obj_id=1,
                            points=point_coords,
                            labels=point_labels,
                        )

                        # Propagate
                        video_segments = {}
                        for frame_idx, obj_ids, mask_logits in self.sam2_video_predictor.propagate_in_video(
                            inference_state,
                            start_frame_idx=0
                        ):
                            video_segments[frame_idx] = (mask_logits[0] > 0.0).cpu().numpy()

                        # 3. ê²°ê³¼ ì €ì¥ (ë¹„ë””ì˜¤ë³„ ë””ë ‰í† ë¦¬)
                        video_result_dir = batch_temp_dir / f"video_{video_idx:03d}"
                        video_result_dir.mkdir(exist_ok=True)

                        for frame_idx, mask in video_segments.items():
                            frame_dir = video_result_dir / f"frame_{frame_idx:04d}"
                            frame_dir.mkdir(exist_ok=True)

                            # Save frame and mask (RGBâ†’BGR ë³€í™˜)
                            cv2.imwrite(str(frame_dir / "original.png"), cv2.cvtColor(frames[frame_idx], cv2.COLOR_RGB2BGR))

                            mask_uint8 = mask.squeeze().astype(np.uint8) * 255
                            cv2.imwrite(str(frame_dir / "mask.png"), mask_uint8)

                        print(f"âœ“ {len(video_segments)} í”„ë ˆì„ ì €ì¥ ì™„ë£Œ")
                        total_processed_frames += len(video_segments)

                        video_results.append({
                            'video_idx': video_idx,
                            'video_name': video_name,
                            'video_path': video_path,
                            'frames': len(video_segments),
                            'result_dir': str(video_result_dir)
                        })

                finally:
                    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                    shutil.rmtree(video_temp_dir, ignore_errors=True)

                    # ì ê·¹ì ì¸ ë©”ëª¨ë¦¬ í•´ì œ
                    # SAM 2 inference_state ì •ë¦¬
                    if 'inference_state' in locals():
                        del inference_state
                    if 'video_segments' in locals():
                        del video_segments

                    # í”„ë ˆì„ ë©”ëª¨ë¦¬ í•´ì œ
                    del frames

                    # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()

                    # Python garbage collection ê°•ì œ ì‹¤í–‰
                    import gc
                    gc.collect()

                print(f"âœ“ {video_name} ì™„ë£Œ (ë©”ëª¨ë¦¬ í•´ì œë¨)")

            # ê²°ê³¼ ì €ì¥
            self.batch_results = {
                'temp_dir': str(batch_temp_dir),
                'videos': video_results,
                'total_frames': total_processed_frames,
                'target_frames': target_frames,
                'reference_annotations': reference_annotations
            }

            progress(1.0, desc="Batch ì²˜ë¦¬ ì™„ë£Œ!")

            status = f"""
### ğŸ‰ Batch Propagation ì™„ë£Œ âœ…

- **ì²˜ë¦¬ëœ ë¹„ë””ì˜¤**: {len(video_results)} / {total_videos}
- **ì´ í”„ë ˆì„ ìˆ˜**: {total_processed_frames}
- **ëª©í‘œ í”„ë ˆì„ ìˆ˜**: {target_frames} (ê° ë¹„ë””ì˜¤ë‹¹)
- **ì„ì‹œ ì €ì¥ ìœ„ì¹˜**: {batch_temp_dir}

### ë‹¤ìŒ ë‹¨ê³„:
- **Export to Fauna** í´ë¦­í•˜ì—¬ í†µí•© ë°ì´í„°ì…‹ ìƒì„±
"""

            return status, "âœ… ì™„ë£Œ"

        except Exception as e:
            import traceback
            error_msg = f"âŒ Batch ì²˜ë¦¬ ì‹¤íŒ¨:\n{str(e)}\n{traceback.format_exc()}"
            print(error_msg)
            return error_msg, "âŒ ì‹¤íŒ¨"

    def save_batch_session(self, session_name: str = "") -> Tuple[str, str]:
        """
        Batch ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì„¸ì…˜ìœ¼ë¡œ ì €ì¥ (Fauna í˜•ì‹ì´ ì•„ë‹Œ ê°œë³„ ë¹„ë””ì˜¤ë³„ ì €ì¥)

        Args:
            session_name: ì„¸ì…˜ ì´ë¦„

        Returns:
            (ì €ì¥ ê²½ë¡œ, ìƒíƒœ ë©”ì‹œì§€)
        """
        if not hasattr(self, 'batch_results') or not self.batch_results:
            return "", "âŒ ë¨¼ì € Batch Propagationì„ ì‹¤í–‰í•˜ì„¸ìš”"

        try:
            from datetime import datetime
            import shutil
            import json

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            if session_name and session_name.strip():
                session_id = f"{session_name.strip()}_{timestamp}"
            else:
                session_id = f"batch_{timestamp}"

            output_dir = Path(f"outputs/sessions/{session_id}")
            output_dir.mkdir(parents=True, exist_ok=True)

            print(f"\n{'='*80}")
            print(f"ğŸ’¾ Batch ì„¸ì…˜ ì €ì¥: {output_dir}")
            print(f"{'='*80}")

            batch_results = self.batch_results

            # ë©”íƒ€ë°ì´í„°
            metadata = {
                'session_id': session_id,
                'session_type': 'batch',
                'timestamp': timestamp,
                'total_videos': len(batch_results['videos']),
                'total_frames': batch_results['total_frames'],
                'target_frames': batch_results['target_frames'],
                'reference_annotations': batch_results['reference_annotations'],
                'videos': []
            }

            # per_video_annotations ì €ì¥ (ìˆìœ¼ë©´)
            if hasattr(self, 'per_video_annotations') and self.per_video_annotations:
                metadata['per_video_annotations'] = self.per_video_annotations
            elif 'per_video_annotations' in batch_results:
                metadata['per_video_annotations'] = batch_results['per_video_annotations']

            # ê° ë¹„ë””ì˜¤ ê²°ê³¼ë¥¼ ê°œë³„ í´ë”ì— ì €ì¥
            for video_result in batch_results['videos']:
                video_name = video_result['video_name']
                video_result_dir = Path(video_result['result_dir'])
                video_idx = video_result['video_idx']

                # ë¹„ë””ì˜¤ë³„ ì €ì¥ ë””ë ‰í† ë¦¬
                video_save_dir = output_dir / f"video_{video_idx:03d}_{Path(video_name).stem}"
                video_save_dir.mkdir(exist_ok=True)

                print(f"\nğŸ“¹ ì €ì¥ ì¤‘: {video_name}")

                # í”„ë ˆì„ ë³µì‚¬
                if video_result_dir.exists():
                    for frame_dir in video_result_dir.iterdir():
                        if frame_dir.is_dir() and frame_dir.name.startswith('frame_'):
                            dst = video_save_dir / frame_dir.name
                            shutil.copytree(frame_dir, dst, dirs_exist_ok=True)

                # result_dir ì—…ë°ì´íŠ¸ (Export Faunaì—ì„œ ì‚¬ìš©)
                video_result['result_dir'] = str(video_save_dir)

                # ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„°
                video_meta = {
                    'video_idx': video_idx,
                    'video_name': video_name,
                    'video_path': video_result['video_path'],
                    'num_frames': video_result['frames'],
                    'saved_dir': str(video_save_dir.relative_to(output_dir))
                }
                metadata['videos'].append(video_meta)

                print(f"  âœ“ {video_result['frames']} í”„ë ˆì„ ì €ì¥ ì™„ë£Œ")

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_path = output_dir / "session_metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            if 'temp_dir' in batch_results:
                temp_dir = Path(batch_results['temp_dir'])
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)

            print(f"\nâœ… Batch ì„¸ì…˜ ì €ì¥ ì™„ë£Œ!")
            print(f"   ê²½ë¡œ: {output_dir}")

            status = f"""
### ğŸ’¾ Batch ì„¸ì…˜ ì €ì¥ ì™„ë£Œ âœ…

- **ì„¸ì…˜ ID**: `{session_id}`
- **ì €ì¥ ê²½ë¡œ**: `{output_dir}`
- **ë¹„ë””ì˜¤ ìˆ˜**: {len(batch_results['videos'])}
- **ì´ í”„ë ˆì„ ìˆ˜**: {batch_results['total_frames']}
- **ëª©í‘œ í”„ë ˆì„ ìˆ˜**: {batch_results['target_frames']} (ê° ë¹„ë””ì˜¤ë‹¹)

### ì €ì¥ëœ ë¹„ë””ì˜¤:
"""
            for video_meta in metadata['videos']:
                status += f"\n- **{video_meta['video_name']}**: {video_meta['num_frames']} í”„ë ˆì„ (â†’ `{video_meta['saved_dir']}`)"

            status += f"""

### ì„¸ì…˜ êµ¬ì¡°:
```
{session_id}/
â”œâ”€â”€ video_000_{Path(metadata['videos'][0]['video_name']).stem}/
â”‚   â”œâ”€â”€ frame_0000/
â”‚   â”‚   â”œâ”€â”€ original.png
â”‚   â”‚   â””â”€â”€ mask.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ video_001_.../
â””â”€â”€ session_metadata.json
```

### ë‹¤ìŒ ë‹¨ê³„:
- ì €ì¥ëœ ì„¸ì…˜ì€ ë‚˜ì¤‘ì— ë¡œë“œ ê°€ëŠ¥
- ë˜ëŠ” **Export to Fauna**ë¡œ í†µí•© ë°ì´í„°ì…‹ ìƒì„±
"""

            return str(output_dir), status

        except Exception as e:
            import traceback
            error_msg = f"âŒ Batch ì„¸ì…˜ ì €ì¥ ì‹¤íŒ¨:\n{str(e)}\n{traceback.format_exc()}"
            print(error_msg)
            return "", error_msg

    def generate_batch_visualization(
        self,
        session_path: str = None,
        output_format: str = "images",
        progress=None
    ) -> Tuple[str, str]:
        """
        Batch ê²°ê³¼ì˜ ë§ˆìŠ¤í¬ ì‹œê°í™” ìƒì„±

        Args:
            session_path: ì„¸ì…˜ ê²½ë¡œ (Noneì´ë©´ í˜„ì¬ batch_results ì‚¬ìš©)
            output_format: "images" (ê°œë³„ ì´ë¯¸ì§€) ë˜ëŠ” "video" (ë¹„ë””ì˜¤)
            progress: Gradio progress

        Returns:
            (ì¶œë ¥ ê²½ë¡œ, ìƒíƒœ ë©”ì‹œì§€)
        """
        try:
            import tempfile

            # ë°ì´í„° ì†ŒìŠ¤ ê²°ì •
            if session_path:
                session_dir = Path(session_path)
                if not session_dir.exists():
                    return "", "âŒ ì„¸ì…˜ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                metadata_path = session_dir / "session_metadata.json"
                if not metadata_path.exists():
                    return "", "âŒ session_metadata.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)

                video_dirs = [session_dir / v['saved_dir'] for v in metadata.get('videos', [])]
            elif hasattr(self, 'batch_results') and self.batch_results:
                # ì„ì‹œ ê²°ê³¼ ì‚¬ìš©
                temp_dir = Path(self.batch_results['temp_dir'])
                video_dirs = [Path(v['result_dir']) for v in self.batch_results['videos']]
            else:
                return "", "âŒ ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Batch ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•˜ê±°ë‚˜ ì„¸ì…˜ì„ ë¡œë“œí•˜ì„¸ìš”."

            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            vis_output_dir = Path(self.default_output_dir) / "visualizations" / f"vis_{Path(tempfile.mktemp()).name[-8:]}"
            vis_output_dir.mkdir(parents=True, exist_ok=True)

            total_frames = 0
            processed_frames = 0

            # ì „ì²´ í”„ë ˆì„ ìˆ˜ ê³„ì‚°
            for video_dir in video_dirs:
                if video_dir.exists():
                    frame_dirs = [d for d in video_dir.iterdir() if d.is_dir() and d.name.startswith('frame_')]
                    total_frames += len(frame_dirs)

            if progress:
                progress(0, desc="ğŸ¨ ì‹œê°í™” ìƒì„± ì¤‘...")

            # ê° ë¹„ë””ì˜¤ ì²˜ë¦¬
            for video_idx, video_dir in enumerate(video_dirs):
                if not video_dir.exists():
                    continue

                video_name = video_dir.name
                video_vis_dir = vis_output_dir / video_name
                video_vis_dir.mkdir(exist_ok=True)

                frame_dirs = sorted([d for d in video_dir.iterdir() if d.is_dir() and d.name.startswith('frame_')])

                for frame_dir in frame_dirs:
                    original_path = frame_dir / "original.png"
                    mask_path = frame_dir / "mask.png"

                    if not original_path.exists() or not mask_path.exists():
                        continue

                    # ì´ë¯¸ì§€ ë¡œë“œ
                    original = cv2.imread(str(original_path))
                    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)

                    if original is None or mask is None:
                        continue

                    # ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ ìƒì„± (ë…¹ìƒ‰, 40% íˆ¬ëª…ë„)
                    overlay = original.copy()
                    mask_bool = mask > 127
                    overlay[mask_bool] = overlay[mask_bool] * 0.6 + np.array([0, 255, 0]) * 0.4

                    # ë§ˆìŠ¤í¬ ìœ¤ê³½ì„  ì¶”ê°€ (ë¹¨ê°„ìƒ‰)
                    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                    cv2.drawContours(overlay, contours, -1, (0, 0, 255), 2)

                    # ì €ì¥
                    vis_path = video_vis_dir / f"{frame_dir.name}_vis.png"
                    cv2.imwrite(str(vis_path), overlay.astype(np.uint8))

                    processed_frames += 1
                    if progress and total_frames > 0:
                        progress(processed_frames / total_frames, desc=f"ğŸ¨ {video_name}: {frame_dir.name}")

                # ë¹„ë””ì˜¤ ìƒì„± (ì„ íƒì )
                if output_format == "video":
                    vis_images = sorted(video_vis_dir.glob("*_vis.png"))
                    if vis_images:
                        first_img = cv2.imread(str(vis_images[0]))
                        h, w = first_img.shape[:2]

                        video_path = vis_output_dir / f"{video_name}_visualization.mp4"
                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                        out = cv2.VideoWriter(str(video_path), fourcc, 10, (w, h))

                        for img_path in vis_images:
                            img = cv2.imread(str(img_path))
                            out.write(img)

                        out.release()

            if progress:
                progress(1.0, desc="âœ… ì‹œê°í™” ì™„ë£Œ!")

            status = f"""
### ğŸ¨ ì‹œê°í™” ìƒì„± ì™„ë£Œ âœ…

- **ì¶œë ¥ ê²½ë¡œ**: `{vis_output_dir}`
- **ì²˜ë¦¬ëœ í”„ë ˆì„**: {processed_frames}ê°œ
- **ë¹„ë””ì˜¤ ìˆ˜**: {len(video_dirs)}ê°œ
- **í˜•ì‹**: {output_format}

ê° ë¹„ë””ì˜¤ í´ë”ì—ì„œ `*_vis.png` íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.
ë…¹ìƒ‰ ì˜ì—­ì´ ë§ˆìŠ¤í¬, ë¹¨ê°„ ìœ¤ê³½ì„ ì´ ê²½ê³„ì…ë‹ˆë‹¤.
"""

            return str(vis_output_dir), status

        except Exception as e:
            import traceback
            return "", f"âŒ ì‹œê°í™” ì‹¤íŒ¨: {str(e)}\n{traceback.format_exc()}"

    def get_batch_frame_list(self) -> List[Dict]:
        """
        Batch ê²°ê³¼ì˜ ì „ì²´ í”„ë ˆì„ ëª©ë¡ ë°˜í™˜ (ìŠ¬ë¼ì´ë”ìš©)

        Returns:
            í”„ë ˆì„ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{video_idx, video_name, frame_idx, frame_dir}, ...]
        """
        frame_list = []

        if not hasattr(self, 'batch_results') or not self.batch_results:
            return frame_list

        for video_result in self.batch_results['videos']:
            video_dir = Path(video_result['result_dir'])
            video_name = video_result['video_name']
            video_idx = video_result['video_idx']

            if not video_dir.exists():
                continue

            frame_dirs = sorted([d for d in video_dir.iterdir() if d.is_dir() and d.name.startswith('frame_')])

            for frame_dir in frame_dirs:
                frame_idx = int(frame_dir.name.split('_')[1])
                frame_list.append({
                    'video_idx': video_idx,
                    'video_name': video_name,
                    'frame_idx': frame_idx,
                    'frame_dir': str(frame_dir)
                })

        return frame_list

    def get_visualization_frame(self, global_idx: int) -> Tuple[np.ndarray, str]:
        """
        íŠ¹ì • ì¸ë±ìŠ¤ì˜ ì‹œê°í™” í”„ë ˆì„ ë°˜í™˜ (ìŠ¬ë¼ì´ë”ìš©)

        Args:
            global_idx: ì „ì²´ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ì—ì„œì˜ ì¸ë±ìŠ¤

        Returns:
            (ì‹œê°í™” ì´ë¯¸ì§€, ìƒíƒœ í…ìŠ¤íŠ¸)
        """
        frame_list = self.get_batch_frame_list()

        if not frame_list:
            return None, "ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Batch Propagateë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."

        if global_idx < 0 or global_idx >= len(frame_list):
            return None, f"ìœ íš¨í•˜ì§€ ì•Šì€ ì¸ë±ìŠ¤: {global_idx}"

        frame_info = frame_list[global_idx]
        frame_dir = Path(frame_info['frame_dir'])

        original_path = frame_dir / "original.png"
        mask_path = frame_dir / "mask.png"

        if not original_path.exists() or not mask_path.exists():
            return None, f"í”„ë ˆì„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {frame_dir}"

        # ì´ë¯¸ì§€ ë¡œë“œ
        original = cv2.imread(str(original_path))
        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)

        if original is None or mask is None:
            return None, "ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨"

        # BGR â†’ RGB ë³€í™˜
        original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)

        # ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ ìƒì„± (ë…¹ìƒ‰, 40% íˆ¬ëª…ë„)
        overlay = original.copy().astype(np.float32)
        mask_bool = mask > 127
        overlay[mask_bool] = overlay[mask_bool] * 0.6 + np.array([0, 255, 0]) * 0.4

        # ë§ˆìŠ¤í¬ ìœ¤ê³½ì„  ì¶”ê°€ (ë¹¨ê°„ìƒ‰)
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(overlay, contours, -1, (255, 0, 0), 2)

        status = f"ğŸ“¹ **{frame_info['video_name']}** | ğŸ–¼ï¸ Frame {frame_info['frame_idx']} | ({global_idx + 1}/{len(frame_list)})"

        return overlay.astype(np.uint8), status

    # ========== Per-Video Annotation Support ==========

    def _draw_points_on_frame(self, frame: np.ndarray) -> np.ndarray:
        """
        í”„ë ˆì„ì— í˜„ì¬ annotation points í‘œì‹œ

        Args:
            frame: RGB í”„ë ˆì„ ì´ë¯¸ì§€

        Returns:
            pointsê°€ í‘œì‹œëœ ì´ë¯¸ì§€
        """
        frame_with_points = frame.copy()

        # Foreground points (ë…¹ìƒ‰)
        for px, py in self.annotations['foreground']:
            cv2.circle(frame_with_points, (px, py), 5, (0, 255, 0), -1)
            cv2.circle(frame_with_points, (px, py), 7, (255, 255, 255), 2)

        # Background points (ë¹¨ê°„ìƒ‰)
        for px, py in self.annotations['background']:
            cv2.circle(frame_with_points, (px, py), 5, (255, 0, 0), -1)
            cv2.circle(frame_with_points, (px, py), 7, (255, 255, 255), 2)

        return frame_with_points

    def init_per_video_annotations(self):
        """ë¹„ë””ì˜¤ë³„ annotation ì €ì¥ì†Œ ì´ˆê¸°í™”"""
        if not hasattr(self, 'per_video_annotations'):
            self.per_video_annotations = {}

    def save_current_annotation_for_video(self, video_label: str) -> str:
        """
        í˜„ì¬ annotationì„ íŠ¹ì • ë¹„ë””ì˜¤ìš©ìœ¼ë¡œ ì €ì¥

        Args:
            video_label: ë¹„ë””ì˜¤ ë ˆì´ë¸” (UIì—ì„œ ì„ íƒí•œ ê²ƒ)

        Returns:
            ìƒíƒœ ë©”ì‹œì§€
        """
        self.init_per_video_annotations()

        if len(self.annotations['foreground']) == 0:
            return f"âŒ Annotationì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € foreground pointë¥¼ ì¶”ê°€í•˜ì„¸ìš”."

        # ë ˆì´ë¸” â†’ ê²½ë¡œ ë³€í™˜
        if hasattr(self, 'batch_video_label_map') and video_label in self.batch_video_label_map:
            video_path = self.batch_video_label_map[video_label]
        else:
            video_path = video_label

        self.per_video_annotations[video_path] = {
            'foreground': self.annotations['foreground'].copy(),
            'background': self.annotations['background'].copy(),
            'video_label': video_label
        }

        fg_count = len(self.annotations['foreground'])
        bg_count = len(self.annotations['background'])

        return f"âœ… **{video_label}** annotation ì €ì¥ë¨ (FG: {fg_count}, BG: {bg_count})"

    def load_video_for_annotation(self, video_label: str) -> Tuple[np.ndarray, str]:
        """
        íŠ¹ì • ë¹„ë””ì˜¤ì˜ ì²« í”„ë ˆì„ì„ ë¡œë“œí•˜ê³  ê¸°ì¡´ annotation ë³µì›

        Args:
            video_label: ë¹„ë””ì˜¤ ë ˆì´ë¸”

        Returns:
            (í”„ë ˆì„ ì´ë¯¸ì§€, ìƒíƒœ ë©”ì‹œì§€)
        """
        self.init_per_video_annotations()

        if not hasattr(self, 'batch_video_label_map'):
            return None, "âŒ ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ìŠ¤ìº”í•˜ì„¸ìš”."

        if video_label not in self.batch_video_label_map:
            return None, f"âŒ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_label}"

        video_path = self.batch_video_label_map[video_label]

        # ì²« í”„ë ˆì„ ì¶”ì¶œ
        frames = self.processor.extract_frames(video_path, 0, 1, stride=1)
        if not frames:
            return None, f"âŒ í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: {video_label}"

        # í˜„ì¬ í”„ë ˆì„ ì„¤ì •
        self.frames = frames
        self.current_frame_idx = 0

        # ê¸°ì¡´ annotation ë³µì› (ìˆìœ¼ë©´)
        if video_path in self.per_video_annotations:
            saved = self.per_video_annotations[video_path]
            self.annotations = {
                'foreground': saved['foreground'].copy(),
                'background': saved['background'].copy()
            }
            status = f"ğŸ“¹ **{video_label}** ë¡œë“œ ì™„ë£Œ (ê¸°ì¡´ annotation ë³µì›ë¨)"
        else:
            # ìƒˆ ë¹„ë””ì˜¤ë©´ annotation ì´ˆê¸°í™”
            self.annotations = {'foreground': [], 'background': []}
            status = f"ğŸ“¹ **{video_label}** ë¡œë“œ ì™„ë£Œ (ìƒˆ annotation)"

        # í˜„ì¬ annotation í‘œì‹œ
        frame_with_points = self._draw_points_on_frame(frames[0])

        return frame_with_points, status

    def get_per_video_annotation_status(self) -> str:
        """ë¹„ë””ì˜¤ë³„ annotation ìƒíƒœ ë°˜í™˜"""
        self.init_per_video_annotations()

        if not self.per_video_annotations:
            return "### ğŸ“‹ ë¹„ë””ì˜¤ë³„ Annotation: ì—†ìŒ"

        lines = ["### ğŸ“‹ ë¹„ë””ì˜¤ë³„ Annotation í˜„í™©\n"]
        for video_path, anno in self.per_video_annotations.items():
            label = anno.get('video_label', Path(video_path).name)
            fg = len(anno['foreground'])
            bg = len(anno['background'])
            lines.append(f"- **{label}**: FG {fg}ê°œ, BG {bg}ê°œ")

        return "\n".join(lines)

    def save_per_video_annotations_to_file(self, filename: str = "") -> Tuple[str, str]:
        """
        ë¹„ë””ì˜¤ë³„ annotationì„ JSON íŒŒì¼ë¡œ ì €ì¥ (propagation ì „ì—ë„ ì‚¬ìš© ê°€ëŠ¥)

        Args:
            filename: íŒŒì¼ ì´ë¦„ (ë¹„ì–´ìˆìœ¼ë©´ ìë™ ìƒì„±)

        Returns:
            (ì €ì¥ ê²½ë¡œ, ìƒíƒœ ë©”ì‹œì§€)
        """
        self.init_per_video_annotations()

        if not self.per_video_annotations:
            return "", "âŒ ì €ì¥í•  ë¹„ë””ì˜¤ë³„ annotationì´ ì—†ìŠµë‹ˆë‹¤."

        try:
            from datetime import datetime
            import json

            # ì €ì¥ ê²½ë¡œ ì„¤ì •
            annotations_dir = Path(self.default_output_dir) / "annotations"
            annotations_dir.mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if filename and filename.strip():
                save_filename = f"{filename.strip()}_{timestamp}.json"
            else:
                save_filename = f"per_video_annotations_{timestamp}.json"

            save_path = annotations_dir / save_filename

            # ì €ì¥ ë°ì´í„° êµ¬ì„±
            save_data = {
                'timestamp': timestamp,
                'num_videos': len(self.per_video_annotations),
                'per_video_annotations': self.per_video_annotations
            }

            with open(save_path, 'w') as f:
                json.dump(save_data, f, indent=2)

            status = f"""
### ğŸ’¾ ë¹„ë””ì˜¤ë³„ Annotation ì €ì¥ ì™„ë£Œ âœ…

- **íŒŒì¼**: `{save_path}`
- **ë¹„ë””ì˜¤ ìˆ˜**: {len(self.per_video_annotations)}ê°œ

ë‚˜ì¤‘ì— **Annotation ë¡œë“œ** ë²„íŠ¼ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
            return str(save_path), status

        except Exception as e:
            import traceback
            return "", f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}\n{traceback.format_exc()}"

    def load_per_video_annotations_from_file(self, filepath: str) -> Tuple[str, str]:
        """
        ì €ì¥ëœ ë¹„ë””ì˜¤ë³„ annotation JSON íŒŒì¼ ë¡œë“œ

        Args:
            filepath: JSON íŒŒì¼ ê²½ë¡œ

        Returns:
            (ìƒíƒœ í…ìŠ¤íŠ¸, annotation ìƒíƒœ)
        """
        try:
            import json

            filepath = Path(filepath)
            if not filepath.exists():
                return "âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", self.get_per_video_annotation_status()

            with open(filepath, 'r') as f:
                data = json.load(f)

            if 'per_video_annotations' not in data:
                return "âŒ ìœ íš¨í•˜ì§€ ì•Šì€ annotation íŒŒì¼ì…ë‹ˆë‹¤.", self.get_per_video_annotation_status()

            self.per_video_annotations = data['per_video_annotations']

            status = f"""
### ğŸ“‚ ë¹„ë””ì˜¤ë³„ Annotation ë¡œë“œ ì™„ë£Œ âœ…

- **íŒŒì¼**: `{filepath}`
- **ë¹„ë””ì˜¤ ìˆ˜**: {len(self.per_video_annotations)}ê°œ ë³µì›ë¨

ì´ì œ **ë¹„ë””ì˜¤ë³„ Batch Propagate**ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
            return status, self.get_per_video_annotation_status()

        except Exception as e:
            import traceback
            return f"âŒ ë¡œë“œ ì‹¤íŒ¨: {str(e)}", self.get_per_video_annotation_status()

    def scan_annotation_files(self) -> List[str]:
        """ì €ì¥ëœ annotation íŒŒì¼ ëª©ë¡ ìŠ¤ìº”"""
        annotations_dir = Path(self.default_output_dir) / "annotations"
        if not annotations_dir.exists():
            return []

        files = sorted(annotations_dir.glob("*.json"), key=lambda x: x.stat().st_mtime, reverse=True)
        return [str(f) for f in files]

    # ========== Preview Video Generation ==========

    def get_batch_video_list(self) -> List[Dict]:
        """
        Batch ê²°ê³¼ì˜ ë¹„ë””ì˜¤ ëª©ë¡ ë°˜í™˜

        Returns:
            ë¹„ë””ì˜¤ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{video_idx, video_name, video_path, result_dir, num_frames, subject_id}, ...]
        """
        if not hasattr(self, 'batch_results') or not self.batch_results:
            return []

        video_list = []
        for video_result in self.batch_results['videos']:
            video_dir = Path(video_result['result_dir'])
            if video_dir.exists():
                num_frames = len(list(video_dir.glob("frame_*")))
                video_path = video_result.get('video_path', '')
                subject_id = self._extract_subject_id(video_path)
                unique_id = self._generate_unique_video_id(video_path)
                video_list.append({
                    'video_idx': video_result['video_idx'],
                    'video_name': video_result['video_name'],
                    'video_path': video_path,
                    'result_dir': str(video_dir),
                    'num_frames': num_frames,
                    'subject_id': subject_id,
                    'unique_id': unique_id
                })
        return video_list

    def get_video_frame_for_preview(
        self,
        video_idx: int,
        frame_idx: int,
        display_mode: str = "overlay"
    ) -> Tuple[np.ndarray, str]:
        """
        íŠ¹ì • ë¹„ë””ì˜¤ì˜ íŠ¹ì • í”„ë ˆì„ ë°˜í™˜ (í”„ë¦¬ë·°ìš©)

        Args:
            video_idx: ë¹„ë””ì˜¤ ì¸ë±ìŠ¤
            frame_idx: í”„ë ˆì„ ì¸ë±ìŠ¤
            display_mode: "mask", "overlay", "side_by_side"

        Returns:
            (ì´ë¯¸ì§€, ìƒíƒœ í…ìŠ¤íŠ¸)
        """
        video_list = self.get_batch_video_list()

        if not video_list:
            return None, "ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        # video_idxë¡œ ë¹„ë””ì˜¤ ì°¾ê¸°
        video_info = None
        for v in video_list:
            if v['video_idx'] == video_idx:
                video_info = v
                break

        if video_info is None:
            return None, f"ë¹„ë””ì˜¤ {video_idx}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        video_dir = Path(video_info['result_dir'])
        frame_dirs = sorted([d for d in video_dir.iterdir() if d.is_dir() and d.name.startswith('frame_')])

        if frame_idx < 0 or frame_idx >= len(frame_dirs):
            return None, f"ìœ íš¨í•˜ì§€ ì•Šì€ í”„ë ˆì„ ì¸ë±ìŠ¤: {frame_idx}"

        frame_dir = frame_dirs[frame_idx]
        original_path = frame_dir / "original.png"
        mask_path = frame_dir / "mask.png"

        if not original_path.exists() or not mask_path.exists():
            return None, f"í”„ë ˆì„ íŒŒì¼ ì—†ìŒ: {frame_dir}"

        # ì´ë¯¸ì§€ ë¡œë“œ
        original = cv2.imread(str(original_path))
        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)

        if original is None or mask is None:
            return None, "ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨"

        # BGR â†’ RGB
        original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)

        # ë””ìŠ¤í”Œë ˆì´ ëª¨ë“œì— ë”°ë¼ ì¶œë ¥
        if display_mode == "mask":
            # Binary mask (3ì±„ë„ë¡œ ë³€í™˜)
            result = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)

        elif display_mode == "overlay":
            # ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ (ë…¹ìƒ‰, 40% íˆ¬ëª…ë„)
            result = original.copy().astype(np.float32)
            mask_bool = mask > 127
            result[mask_bool] = result[mask_bool] * 0.6 + np.array([0, 255, 0]) * 0.4
            # ìœ¤ê³½ì„  ì¶”ê°€ (ë¹¨ê°„ìƒ‰)
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(result, contours, -1, (255, 0, 0), 2)
            result = result.astype(np.uint8)

        elif display_mode == "side_by_side":
            # ì›ë³¸ | ë§ˆìŠ¤í¬ | ì˜¤ë²„ë ˆì´ (3ê°œ ë‚˜ë€íˆ, ì €í•´ìƒë„)
            h, w = original.shape[:2]
            scale = min(1.0, 400 / w)  # ìµœëŒ€ ë„ˆë¹„ 400px
            new_w, new_h = int(w * scale), int(h * scale)

            orig_small = cv2.resize(original, (new_w, new_h))
            mask_rgb = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)
            mask_small = cv2.resize(mask_rgb, (new_w, new_h))

            overlay = original.copy().astype(np.float32)
            mask_bool = mask > 127
            overlay[mask_bool] = overlay[mask_bool] * 0.6 + np.array([0, 255, 0]) * 0.4
            overlay_small = cv2.resize(overlay.astype(np.uint8), (new_w, new_h))

            result = np.hstack([orig_small, mask_small, overlay_small])

        else:
            result = original

        status = f"ğŸ“¹ **{video_info['video_name']}** | Frame {frame_idx + 1}/{len(frame_dirs)}"
        return result, status

    def generate_preview_video(
        self,
        video_idx: int,
        display_mode: str = "overlay",
        fps: int = 15,
        scale: float = 0.5,
        progress=None
    ) -> Tuple[str, str]:
        """
        íŠ¹ì • ë¹„ë””ì˜¤ì˜ í”„ë¦¬ë·° ì˜ìƒ ìƒì„± (ì €í•´ìƒë„, ë¹ ë¥¸ í™•ì¸ìš©)

        Args:
            video_idx: ë¹„ë””ì˜¤ ì¸ë±ìŠ¤
            display_mode: "mask", "overlay", "side_by_side"
            fps: í”„ë ˆì„ ë ˆì´íŠ¸
            scale: í•´ìƒë„ ìŠ¤ì¼€ì¼ (0.25 ~ 1.0)

        Returns:
            (ë¹„ë””ì˜¤ ê²½ë¡œ, ìƒíƒœ ë©”ì‹œì§€)
        """
        video_list = self.get_batch_video_list()

        if not video_list:
            return "", "ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Batch Propagateë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."

        # video_idxë¡œ ë¹„ë””ì˜¤ ì°¾ê¸°
        video_info = None
        for v in video_list:
            if v['video_idx'] == video_idx:
                video_info = v
                break

        if video_info is None:
            return "", f"ë¹„ë””ì˜¤ {video_idx}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        try:
            video_dir = Path(video_info['result_dir'])
            frame_dirs = sorted([d for d in video_dir.iterdir() if d.is_dir() and d.name.startswith('frame_')])

            if not frame_dirs:
                return "", "í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤."

            # ì¶œë ¥ ê²½ë¡œ
            preview_dir = Path(self.default_output_dir) / "previews"
            preview_dir.mkdir(parents=True, exist_ok=True)

            # unique_id ì‚¬ìš© (m1_cam1_0 í˜•ì‹)ìœ¼ë¡œ 72ê°œ ë¹„ë””ì˜¤ ëª¨ë‘ êµ¬ë¶„ ê°€ëŠ¥
            unique_id = video_info.get('unique_id')
            if not unique_id:
                # fallback: video_pathì—ì„œ unique_id ìƒì„±
                video_path = video_info.get('video_path', '')
                unique_id = self._generate_unique_video_id(video_path) if video_path else Path(video_info['video_name']).stem
            output_path = preview_dir / f"{unique_id}_{display_mode}_preview.mp4"

            # ì²« í”„ë ˆì„ìœ¼ë¡œ í¬ê¸° ê²°ì •
            first_frame, _ = self.get_video_frame_for_preview(video_idx, 0, display_mode)
            if first_frame is None:
                return "", "ì²« í”„ë ˆì„ ë¡œë“œ ì‹¤íŒ¨"

            h, w = first_frame.shape[:2]
            new_w, new_h = int(w * scale), int(h * scale)
            # ì§ìˆ˜ë¡œ ë§ì¶”ê¸° (ì½”ë± ìš”êµ¬ì‚¬í•­)
            new_w = new_w if new_w % 2 == 0 else new_w + 1
            new_h = new_h if new_h % 2 == 0 else new_h + 1

            # VideoWriter ì„¤ì •
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(str(output_path), fourcc, fps, (new_w, new_h))

            if progress:
                progress(0, desc=f"ğŸ¬ í”„ë¦¬ë·° ìƒì„± ì¤‘: {unique_id}")

            for i, frame_dir in enumerate(frame_dirs):
                frame, _ = self.get_video_frame_for_preview(video_idx, i, display_mode)
                if frame is not None:
                    # ë¦¬ì‚¬ì´ì¦ˆ ë° BGR ë³€í™˜
                    frame_resized = cv2.resize(frame, (new_w, new_h))
                    frame_bgr = cv2.cvtColor(frame_resized, cv2.COLOR_RGB2BGR)
                    out.write(frame_bgr)

                if progress:
                    progress((i + 1) / len(frame_dirs), desc=f"ğŸ¬ {unique_id}: {i+1}/{len(frame_dirs)}")

            out.release()

            if progress:
                progress(1.0, desc="âœ… í”„ë¦¬ë·° ìƒì„± ì™„ë£Œ")

            status = f"""
### ğŸ¬ í”„ë¦¬ë·° ì˜ìƒ ìƒì„± ì™„ë£Œ âœ…

- **ë¹„ë””ì˜¤**: {unique_id} ({video_info['video_name']})
- **ëª¨ë“œ**: {display_mode}
- **í”„ë ˆì„ ìˆ˜**: {len(frame_dirs)}
- **FPS**: {fps}
- **í•´ìƒë„**: {new_w}x{new_h} (ì›ë³¸ì˜ {int(scale*100)}%)
- **íŒŒì¼**: `{output_path}`
"""
            return str(output_path), status

        except Exception as e:
            import traceback
            return "", f"âŒ í”„ë¦¬ë·° ìƒì„± ì‹¤íŒ¨: {str(e)}\n{traceback.format_exc()}"

    def batch_propagate_with_per_video_annotations(
        self,
        target_frames: int = 100,
        selected_videos: List[str] = None,
        progress=gr.Progress()
    ) -> Tuple[str, str]:
        """
        ë¹„ë””ì˜¤ë³„ ê°œë³„ annotationì„ ì‚¬ìš©í•œ Batch Propagation

        ê° ë¹„ë””ì˜¤ë§ˆë‹¤ í•´ë‹¹ ë¹„ë””ì˜¤ì— ì €ì¥ëœ annotation ì‚¬ìš©.
        annotationì´ ì—†ëŠ” ë¹„ë””ì˜¤ëŠ” ê¸°ë³¸ reference annotation ì‚¬ìš©.
        """
        self.init_per_video_annotations()

        if not hasattr(self, 'batch_videos') or not self.batch_videos:
            return "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ìŠ¤ìº”í•˜ì„¸ìš”", "âŒ ë¹„ë””ì˜¤ ì—†ìŒ"

        # ê¸°ë³¸ reference annotation (í˜„ì¬ UIì— ìˆëŠ” ê²ƒ)
        default_annotations = {
            'foreground': self.annotations['foreground'].copy(),
            'background': self.annotations['background'].copy()
        }

        # per-video annotation ì‚¬ìš© ê°€ëŠ¥í•œ ë¹„ë””ì˜¤ ìˆ˜ í™•ì¸
        if not self.per_video_annotations and len(default_annotations['foreground']) == 0:
            return "Annotationì´ í•„ìš”í•©ë‹ˆë‹¤. ìµœì†Œ 1ê°œì˜ foreground pointê°€ í•„ìš”í•©ë‹ˆë‹¤.", "âŒ Annotation ì—†ìŒ"

        try:
            import tempfile
            import shutil
            import torch

            batch_temp_dir = Path(tempfile.mkdtemp(prefix="sam3d_batch_"))

            # ì„ íƒëœ ë¹„ë””ì˜¤ í•„í„°ë§
            if selected_videos and len(selected_videos) > 0:
                videos_to_process = []
                if hasattr(self, 'batch_video_label_map'):
                    for label in selected_videos:
                        if label in self.batch_video_label_map:
                            videos_to_process.append(self.batch_video_label_map[label])
            else:
                videos_to_process = self.batch_videos

            if not videos_to_process:
                return "ì²˜ë¦¬í•  ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”", "âŒ ì„ íƒëœ ë¹„ë””ì˜¤ ì—†ìŒ"

            total_videos = len(videos_to_process)
            total_processed_frames = 0
            video_results = []

            progress(0, desc=f"Batch ì²˜ë¦¬ ì‹œì‘: {total_videos}ê°œ ë¹„ë””ì˜¤...")

            for video_idx, video_path in enumerate(videos_to_process):
                video_name = Path(video_path).name
                progress(video_idx / total_videos, desc=f"ì²˜ë¦¬ ì¤‘: {video_name} ({video_idx+1}/{total_videos})")

                # í•´ë‹¹ ë¹„ë””ì˜¤ì˜ annotation ì„ íƒ
                if video_path in self.per_video_annotations:
                    video_annotations = self.per_video_annotations[video_path]
                    print(f"ğŸ“¹ {video_name}: ê°œë³„ annotation ì‚¬ìš©")
                else:
                    video_annotations = default_annotations
                    print(f"ğŸ“¹ {video_name}: ê¸°ë³¸ annotation ì‚¬ìš©")

                if len(video_annotations['foreground']) == 0:
                    print(f"âš ï¸ {video_name}: annotation ì—†ìŒ, ê±´ë„ˆëœ€")
                    continue

                # ë¹„ë””ì˜¤ ì •ë³´ ì°¾ê¸°
                matching_info = None
                for info in self.batch_video_info:
                    if info['path'] == video_path:
                        matching_info = info
                        break

                if matching_info is None:
                    continue

                num_frames = matching_info['frames']
                calculated_stride = max(1, num_frames // target_frames)

                # í”„ë ˆì„ ì¶”ì¶œ
                frames = self.processor.extract_frames(video_path, 0, num_frames, stride=calculated_stride)
                if not frames:
                    continue

                # ì„ì‹œ ë””ë ‰í† ë¦¬ì— í”„ë ˆì„ ì €ì¥
                video_temp_dir = tempfile.mkdtemp(prefix=f"sam3d_video_{video_idx}_")

                try:
                    for idx, frame in enumerate(frames):
                        frame_path = Path(video_temp_dir) / f"{idx:05d}.jpg"
                        cv2.imwrite(str(frame_path), cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))

                    # SAM 2 inference
                    if self.sam2_video_predictor is not None:
                        inference_state = self.sam2_video_predictor.init_state(video_path=video_temp_dir)

                        # í•´ë‹¹ ë¹„ë””ì˜¤ì˜ annotations ì ìš©
                        point_coords = []
                        point_labels = []

                        for px, py in video_annotations['foreground']:
                            point_coords.append([px, py])
                            point_labels.append(1)

                        for px, py in video_annotations['background']:
                            point_coords.append([px, py])
                            point_labels.append(0)

                        point_coords = np.array(point_coords, dtype=np.float32)
                        point_labels = np.array(point_labels, dtype=np.int32)

                        self.sam2_video_predictor.add_new_points_or_box(
                            inference_state=inference_state,
                            frame_idx=0,
                            obj_id=1,
                            points=point_coords,
                            labels=point_labels,
                        )

                        # Propagate
                        video_segments = {}
                        for frame_idx, obj_ids, mask_logits in self.sam2_video_predictor.propagate_in_video(
                            inference_state,
                            start_frame_idx=0
                        ):
                            video_segments[frame_idx] = (mask_logits[0] > 0.0).cpu().numpy()

                        # ê²°ê³¼ ì €ì¥
                        video_result_dir = batch_temp_dir / f"video_{video_idx:03d}"
                        video_result_dir.mkdir(exist_ok=True)

                        for frame_idx, mask in video_segments.items():
                            frame_dir = video_result_dir / f"frame_{frame_idx:04d}"
                            frame_dir.mkdir(exist_ok=True)
                            cv2.imwrite(str(frame_dir / "original.png"), cv2.cvtColor(frames[frame_idx], cv2.COLOR_RGB2BGR))
                            mask_uint8 = mask.squeeze().astype(np.uint8) * 255
                            cv2.imwrite(str(frame_dir / "mask.png"), mask_uint8)

                        total_processed_frames += len(video_segments)

                        video_results.append({
                            'video_idx': video_idx,
                            'video_name': video_name,
                            'video_path': video_path,
                            'frames': len(video_segments),
                            'result_dir': str(video_result_dir),
                            'annotation_type': 'per_video' if video_path in self.per_video_annotations else 'default'
                        })

                finally:
                    shutil.rmtree(video_temp_dir, ignore_errors=True)
                    if 'inference_state' in locals():
                        del inference_state
                    if 'video_segments' in locals():
                        del video_segments
                    del frames
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    import gc
                    gc.collect()

            # ê²°ê³¼ ì €ì¥
            self.batch_results = {
                'temp_dir': str(batch_temp_dir),
                'videos': video_results,
                'total_frames': total_processed_frames,
                'target_frames': target_frames,
                'reference_annotations': default_annotations,
                'per_video_annotations': {k: v for k, v in self.per_video_annotations.items()}
            }

            progress(1.0, desc="Batch ì²˜ë¦¬ ì™„ë£Œ!")

            # ê°œë³„ annotation ì‚¬ìš© ë¹„ë””ì˜¤ ìˆ˜ ì¹´ìš´íŠ¸
            per_video_count = sum(1 for v in video_results if v.get('annotation_type') == 'per_video')
            default_count = len(video_results) - per_video_count

            status = f"""
### ğŸ‰ Batch Propagation ì™„ë£Œ (ë¹„ë””ì˜¤ë³„ Annotation) âœ…

- **ì²˜ë¦¬ëœ ë¹„ë””ì˜¤**: {len(video_results)} / {total_videos}
  - ê°œë³„ annotation: {per_video_count}ê°œ
  - ê¸°ë³¸ annotation: {default_count}ê°œ
- **ì´ í”„ë ˆì„ ìˆ˜**: {total_processed_frames}
- **ì„ì‹œ ì €ì¥ ìœ„ì¹˜**: {batch_temp_dir}

### ë‹¤ìŒ ë‹¨ê³„:
- **ê²°ê³¼ í™•ì¸**: ìŠ¬ë¼ì´ë”ë¡œ í”„ë ˆì„ë³„ ë§ˆìŠ¤í¬ í™•ì¸
- **Export to Fauna**: í†µí•© ë°ì´í„°ì…‹ ìƒì„±
"""

            return status, "âœ… ì™„ë£Œ"

        except Exception as e:
            import traceback
            return f"âŒ Batch ì²˜ë¦¬ ì‹¤íŒ¨:\n{str(e)}\n{traceback.format_exc()}", "âŒ ì‹¤íŒ¨"

    def load_batch_session(self, session_path: str) -> Tuple[str, str]:
        """
        ì €ì¥ëœ Batch ì„¸ì…˜ ë¡œë“œ

        Args:
            session_path: ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë˜ëŠ” session_metadata.json ê²½ë¡œ

        Returns:
            (ìƒíƒœ ë©”ì‹œì§€, ì„±ê³µ ì—¬ë¶€)
        """
        try:
            import json

            session_path = Path(session_path)

            # session_metadata.json ê²½ë¡œ ì°¾ê¸°
            if session_path.is_file() and session_path.name == "session_metadata.json":
                metadata_path = session_path
                session_dir = session_path.parent
            elif session_path.is_dir():
                metadata_path = session_path / "session_metadata.json"
                session_dir = session_path
            else:
                return "âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜ ê²½ë¡œì…ë‹ˆë‹¤", ""

            if not metadata_path.exists():
                return f"âŒ ì„¸ì…˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_path}", ""

            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)

            # ì„¸ì…˜ íƒ€ì… í™•ì¸
            if metadata.get('session_type') != 'batch':
                return f"âŒ Batch ì„¸ì…˜ì´ ì•„ë‹™ë‹ˆë‹¤. (íƒ€ì…: {metadata.get('session_type')})", ""

            print(f"\n{'='*80}")
            print(f"ğŸ“‚ Batch ì„¸ì…˜ ë¡œë“œ: {session_dir}")
            print(f"{'='*80}")

            # batch_results ë³µì›
            video_results = []
            for video_meta in metadata['videos']:
                video_result_dir = session_dir / video_meta['saved_dir']

                if not video_result_dir.exists():
                    print(f"  âš ï¸  ê²½ê³ : {video_result_dir} ì—†ìŒ")
                    continue

                # í”„ë ˆì„ ê°œìˆ˜ í™•ì¸
                num_frames = len(list(video_result_dir.glob("frame_*")))

                video_results.append({
                    'video_idx': video_meta['video_idx'],
                    'video_name': video_meta['video_name'],
                    'video_path': video_meta['video_path'],
                    'frames': num_frames,
                    'result_dir': str(video_result_dir)
                })

                print(f"  âœ“ {video_meta['video_name']}: {num_frames} í”„ë ˆì„")

            # batch_results ì„¤ì •
            self.batch_results = {
                'temp_dir': '',  # ë¡œë“œëœ ì„¸ì…˜ì€ ì„ì‹œ ë””ë ‰í† ë¦¬ ì—†ìŒ
                'videos': video_results,
                'total_frames': metadata['total_frames'],
                'target_frames': metadata['target_frames'],
                'reference_annotations': metadata['reference_annotations']
            }

            # per_video_annotations ë³µì› (ìˆìœ¼ë©´)
            if 'per_video_annotations' in metadata:
                self.per_video_annotations = metadata['per_video_annotations']
                print(f"  âœ“ ë¹„ë””ì˜¤ë³„ annotation {len(self.per_video_annotations)}ê°œ ë³µì›ë¨")
            else:
                self.init_per_video_annotations()

            print(f"\nâœ… Batch ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ!")

            # per_video_annotations ìˆ˜ í™•ì¸
            per_video_count = len(self.per_video_annotations) if hasattr(self, 'per_video_annotations') else 0

            # ë¹„ë””ì˜¤ ëª©ë¡ì„ ì ‘ì„ ìˆ˜ ìˆê²Œ êµ¬ì„±
            video_list_items = []
            for video_result in video_results:
                video_path = video_result.get('video_path', '')
                unique_id = self._generate_unique_video_id(video_path) if video_path else video_result['video_name']
                video_list_items.append(f"- **{unique_id}**: {video_result['frames']} í”„ë ˆì„")

            video_list_str = "\n".join(video_list_items)

            status = f"""
### ğŸ“‚ Batch ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ âœ…

- **ì„¸ì…˜ ID**: `{metadata['session_id']}`
- **ë¡œë“œ ê²½ë¡œ**: `{session_dir}`
- **ë¹„ë””ì˜¤ ìˆ˜**: {len(video_results)}
- **ì´ í”„ë ˆì„ ìˆ˜**: {metadata['total_frames']}
- **ëª©í‘œ í”„ë ˆì„ ìˆ˜**: {metadata['target_frames']} (ê° ë¹„ë””ì˜¤ë‹¹)
- **ë¹„ë””ì˜¤ë³„ Annotation**: {per_video_count}ê°œ ë³µì›ë¨

<details>
<summary><b>ğŸ“‹ ë¡œë“œëœ ë¹„ë””ì˜¤ ëª©ë¡ ({len(video_results)}ê°œ) - í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°/ì ‘ê¸°</b></summary>

{video_list_str}

</details>

### ë‹¤ìŒ ë‹¨ê³„:
- **Export to Fauna** í´ë¦­í•˜ì—¬ í†µí•© ë°ì´í„°ì…‹ ìƒì„±
- ë˜ëŠ” ì¶”ê°€ í¸ì§‘ ìˆ˜í–‰
"""

            return status, "âœ… ë¡œë“œ ì™„ë£Œ"

        except Exception as e:
            import traceback
            error_msg = f"âŒ Batch ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨:\n{str(e)}\n{traceback.format_exc()}"
            print(error_msg)
            return error_msg, ""

    def delete_batch_session(self, session_path: str) -> Tuple[str, List[str]]:
        """
        Batch ì„¸ì…˜ ì‚­ì œ

        Args:
            session_path: ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ

        Returns:
            (ìƒíƒœ ë©”ì‹œì§€, ì—…ë°ì´íŠ¸ëœ ì„¸ì…˜ ëª©ë¡)
        """
        import shutil

        if not session_path:
            return "âŒ ì‚­ì œí•  ì„¸ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", []

        session_dir = Path(session_path)
        if not session_dir.exists():
            return f"âŒ ì„¸ì…˜ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {session_path}", []

        # ë©”íƒ€ë°ì´í„° í™•ì¸
        metadata_path = session_dir / "session_metadata.json"
        if not metadata_path.exists():
            return f"âŒ ìœ íš¨í•œ ì„¸ì…˜ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {session_path}", []

        try:
            session_name = session_dir.name
            shutil.rmtree(session_dir)
            print(f"ğŸ—‘ï¸ ì„¸ì…˜ ì‚­ì œë¨: {session_dir}")

            # ì„¸ì…˜ ëª©ë¡ ìƒˆë¡œê³ ì¹¨
            sessions_dir = Path(self.default_output_dir) / "sessions"
            sessions = []
            if sessions_dir.exists():
                for s_dir in sessions_dir.iterdir():
                    if s_dir.is_dir() and (s_dir / "session_metadata.json").exists():
                        sessions.append(str(s_dir))

            return f"âœ… ì„¸ì…˜ '{session_name}' ì‚­ì œë¨", sorted(sessions, reverse=True)

        except Exception as e:
            return f"âŒ ì„¸ì…˜ ì‚­ì œ ì‹¤íŒ¨: {str(e)}", []

    def rename_batch_session(self, session_path: str, new_name: str) -> Tuple[str, List[str]]:
        """
        Batch ì„¸ì…˜ ì´ë¦„ ë³€ê²½

        Args:
            session_path: ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            new_name: ìƒˆ ì„¸ì…˜ ì´ë¦„

        Returns:
            (ìƒíƒœ ë©”ì‹œì§€, ì—…ë°ì´íŠ¸ëœ ì„¸ì…˜ ëª©ë¡)
        """
        import json

        if not session_path:
            return "âŒ ì´ë¦„ì„ ë³€ê²½í•  ì„¸ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", []

        if not new_name or not new_name.strip():
            return "âŒ ìƒˆ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”", []

        new_name = new_name.strip()

        # ìœ íš¨í•œ íŒŒì¼ëª… ë¬¸ìë§Œ í—ˆìš©
        invalid_chars = '<>:"/\\|?*'
        if any(c in new_name for c in invalid_chars):
            return f"âŒ ì„¸ì…˜ ì´ë¦„ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ìê°€ ìˆìŠµë‹ˆë‹¤: {invalid_chars}", []

        session_dir = Path(session_path)
        if not session_dir.exists():
            return f"âŒ ì„¸ì…˜ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {session_path}", []

        # ë©”íƒ€ë°ì´í„° í™•ì¸
        metadata_path = session_dir / "session_metadata.json"
        if not metadata_path.exists():
            return f"âŒ ìœ íš¨í•œ ì„¸ì…˜ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {session_path}", []

        try:
            old_name = session_dir.name
            new_session_dir = session_dir.parent / new_name

            if new_session_dir.exists():
                return f"âŒ ì´ë¯¸ ê°™ì€ ì´ë¦„ì˜ ì„¸ì…˜ì´ ì¡´ì¬í•©ë‹ˆë‹¤: {new_name}", []

            # ë””ë ‰í† ë¦¬ ì´ë¦„ ë³€ê²½
            session_dir.rename(new_session_dir)

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            new_metadata_path = new_session_dir / "session_metadata.json"
            with open(new_metadata_path, 'r') as f:
                metadata = json.load(f)
            metadata['session_id'] = new_name
            with open(new_metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            print(f"âœï¸ ì„¸ì…˜ ì´ë¦„ ë³€ê²½: {old_name} â†’ {new_name}")

            # ì„¸ì…˜ ëª©ë¡ ìƒˆë¡œê³ ì¹¨
            sessions_dir = Path(self.default_output_dir) / "sessions"
            sessions = []
            if sessions_dir.exists():
                for s_dir in sessions_dir.iterdir():
                    if s_dir.is_dir() and (s_dir / "session_metadata.json").exists():
                        sessions.append(str(s_dir))

            return f"âœ… ì„¸ì…˜ ì´ë¦„ ë³€ê²½ë¨: '{old_name}' â†’ '{new_name}'", sorted(sessions, reverse=True)

        except Exception as e:
            return f"âŒ ì„¸ì…˜ ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: {str(e)}", []

    def export_batch_to_fauna(self, output_name: str = "fauna_dataset", file_structure: str = "video_folders") -> Tuple[str, str]:
        """
        Batch ì²˜ë¦¬ ê²°ê³¼ë¥¼ Fauna ë°ì´í„°ì…‹ í˜•ì‹ìœ¼ë¡œ export

        Args:
            output_name: ì¶œë ¥ ë°ì´í„°ì…‹ ì´ë¦„
            file_structure: íŒŒì¼ êµ¬ì¡° ("video_folders" ë˜ëŠ” "flat")

        Returns:
            (Fauna ë°ì´í„°ì…‹ ê²½ë¡œ, ìƒíƒœ ë©”ì‹œì§€)
        """
        if not hasattr(self, 'batch_results') or not self.batch_results:
            return "", "âŒ ë¨¼ì € Batch Propagationì„ ì‹¤í–‰í•˜ì„¸ìš”"

        try:
            from datetime import datetime
            import shutil
            import json

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = Path(f"outputs/fauna_datasets/{output_name}_{timestamp}")
            output_dir.mkdir(parents=True, exist_ok=True)

            print(f"\n{'='*80}")
            print(f"ğŸ“¦ Fauna ë°ì´í„°ì…‹ ìƒì„±: {output_dir}")
            print(f"ğŸ“ íŒŒì¼ êµ¬ì¡°: {file_structure}")
            print(f"{'='*80}")

            batch_results = self.batch_results
            total_frames_exported = 0
            video_segments_info = []

            # ê° ë¹„ë””ì˜¤ ê²°ê³¼ë¥¼ export
            for video_idx, video_result in enumerate(batch_results['videos']):
                video_name = video_result['video_name']
                video_result_dir = Path(video_result['result_dir'])
                num_frames = video_result['frames']

                print(f"\nğŸ“¹ {video_name}: {num_frames} í”„ë ˆì„ export ì¤‘...")

                # ë¹„ë””ì˜¤ ì´ë¦„ì—ì„œ ì•ˆì „í•œ prefix ìƒì„± (ê²½ë¡œ êµ¬ë¶„ì ì œê±°)
                video_prefix = f"video{video_idx:03d}"

                video_segment = {
                    'video_name': video_name,
                    'video_path': video_result['video_path'],
                    'video_idx': video_result['video_idx'],
                    'video_prefix': video_prefix,
                    'num_frames': num_frames
                }
                video_segments_info.append(video_segment)

                # ë¹„ë””ì˜¤ë³„ í´ë” ìƒì„± (video_folders ëª¨ë“œì¸ ê²½ìš°)
                if file_structure == "video_folders":
                    video_output_dir = output_dir / video_prefix
                    video_output_dir.mkdir(exist_ok=True)

                # í”„ë ˆì„ ë³µì‚¬
                for local_frame_idx in range(num_frames):
                    src_frame_dir = video_result_dir / f"frame_{local_frame_idx:04d}"

                    if not src_frame_dir.exists():
                        print(f"  âš ï¸  ê²½ê³ : {src_frame_dir} ì—†ìŒ, ê±´ë„ˆëœ€")
                        continue

                    # ì›ë³¸ íŒŒì¼ ì½ê¸°
                    src_rgb = src_frame_dir / "original.png"
                    src_mask = src_frame_dir / "mask.png"

                    if not src_rgb.exists() or not src_mask.exists():
                        print(f"  âš ï¸  ê²½ê³ : frame_{local_frame_idx:04d} íŒŒì¼ ëˆ„ë½")
                        continue

                    # ëª©ì ì§€ ê²½ë¡œ ê²°ì •
                    if file_structure == "video_folders":
                        # video001/frame_0000_rgb.png
                        dst_rgb = video_output_dir / f"frame_{local_frame_idx:04d}_rgb.png"
                        dst_mask = video_output_dir / f"frame_{local_frame_idx:04d}_mask.png"
                    else:  # flat
                        # video001_frame_0000_rgb.png
                        dst_rgb = output_dir / f"{video_prefix}_frame_{local_frame_idx:04d}_rgb.png"
                        dst_mask = output_dir / f"{video_prefix}_frame_{local_frame_idx:04d}_mask.png"

                    # íŒŒì¼ ë³µì‚¬
                    shutil.copy2(src_rgb, dst_rgb)
                    shutil.copy2(src_mask, dst_mask)

                total_frames_exported += num_frames
                print(f"  âœ“ {num_frames} í”„ë ˆì„ ë³µì‚¬ ì™„ë£Œ")

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                'dataset_name': output_name,
                'timestamp': timestamp,
                'file_structure': file_structure,
                'total_frames': total_frames_exported,
                'num_videos': len(batch_results['videos']),
                'target_frames': batch_results['target_frames'],
                'reference_annotations': batch_results['reference_annotations'],
                'video_segments': video_segments_info
            }

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_path = output_dir / "dataset_metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            print(f"\nâœ… Fauna ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
            print(f"   ê²½ë¡œ: {output_dir}")
            print(f"   ì´ í”„ë ˆì„: {total_frames_exported}")

            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            if 'temp_dir' in batch_results:
                temp_dir = Path(batch_results['temp_dir'])
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    print(f"âœ“ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ")

            # ìƒíƒœ ë©”ì‹œì§€ ìƒì„±
            structure_example = ""
            if file_structure == "video_folders":
                structure_example = f"""
```
{output_dir.name}/
â”œâ”€â”€ video000/
â”‚   â”œâ”€â”€ frame_0000_rgb.png
â”‚   â”œâ”€â”€ frame_0000_mask.png
â”‚   â”œâ”€â”€ frame_0001_rgb.png
â”‚   â””â”€â”€ frame_0001_mask.png
â”œâ”€â”€ video001/
â”‚   â””â”€â”€ ...
â””â”€â”€ dataset_metadata.json
```
"""
            else:  # flat
                structure_example = f"""
```
{output_dir.name}/
â”œâ”€â”€ video000_frame_0000_rgb.png
â”œâ”€â”€ video000_frame_0000_mask.png
â”œâ”€â”€ video000_frame_0001_rgb.png
â”œâ”€â”€ video000_frame_0001_mask.png
â”œâ”€â”€ video001_frame_0000_rgb.png
â””â”€â”€ dataset_metadata.json
```
"""

            status = f"""
### ğŸ‰ Fauna ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ âœ…

- **ì¶œë ¥ ê²½ë¡œ**: `{output_dir}`
- **íŒŒì¼ êµ¬ì¡°**: {file_structure}
- **ì´ í”„ë ˆì„ ìˆ˜**: {total_frames_exported}
- **ë¹„ë””ì˜¤ ìˆ˜**: {len(batch_results['videos'])}
- **ëª©í‘œ í”„ë ˆì„ ìˆ˜**: {batch_results['target_frames']} (ê° ë¹„ë””ì˜¤ë‹¹)

### ë¹„ë””ì˜¤ ì •ë³´:
"""
            for seg in video_segments_info:
                status += f"\n- **{seg['video_name']}**: {seg['num_frames']} í”„ë ˆì„ (prefix: {seg['video_prefix']})"

            status += f"\n\n### ë°ì´í„°ì…‹ êµ¬ì¡°:{structure_example}"

            return str(output_dir), status

        except Exception as e:
            import traceback
            error_msg = f"âŒ Fauna Export ì‹¤íŒ¨: {str(e)}\n\n{traceback.format_exc()}"
            print(error_msg)
            return "", error_msg

    def get_video_duration(self, data_dir: str, video_file: str) -> float:
        """
        ë¹„ë””ì˜¤ íŒŒì¼ì˜ ì „ì²´ ê¸¸ì´(ì´ˆ) ë°˜í™˜

        Args:
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
            video_file: ë¹„ë””ì˜¤ íŒŒì¼ëª…

        Returns:
            ë¹„ë””ì˜¤ ê¸¸ì´(ì´ˆ), ì‹¤íŒ¨ ì‹œ 3.0 (ê¸°ë³¸ê°’)
        """
        if not video_file:
            return 3.0

        video_path = Path(data_dir) / video_file
        if not video_path.exists():
            return 3.0

        try:
            info = self.processor.get_video_info(str(video_path))
            duration = info['frame_count'] / info['fps']
            print(f"âœ“ ë¹„ë””ì˜¤ ê¸¸ì´: {duration:.2f}ì´ˆ ({info['frame_count']} í”„ë ˆì„, {info['fps']:.2f} fps)")
            return round(duration, 2)
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {e}")
            return 3.0

    def load_video(self, data_dir: str, video_file: str,
                   start_time: float, duration: float) -> Tuple[np.ndarray, str, gr.Slider]:
        """ë¹„ë””ì˜¤ ë¡œë“œ ë° í”„ë ˆì„ ì¶”ì¶œ"""
        default_slider = gr.Slider(label="í”„ë ˆì„ ìœ„ì¹˜", minimum=0, maximum=100, value=0, step=1)

        if not video_file:
            return None, "ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”", default_slider

        video_path = Path(data_dir) / video_file

        if not video_path.exists():
            return None, f"ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}", default_slider

        self.video_path = str(video_path)

        try:
            # ë¹„ë””ì˜¤ ì •ë³´
            info = self.processor.get_video_info(self.video_path)
            fps = info['fps']
            total_duration = info['frame_count'] / fps

            # durationì´ ë¹„ë””ì˜¤ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ì „ì²´ ê¸¸ì´ ì‚¬ìš©
            if duration <= 0 or duration > total_duration:
                duration = total_duration
                print(f"âœ“ Durationì„ ë¹„ë””ì˜¤ ì „ì²´ ê¸¸ì´ë¡œ ì„¤ì •: {duration:.2f}ì´ˆ")

            # í”„ë ˆì„ ì¶”ì¶œ
            start_frame = int(start_time * fps)
            num_frames = int(duration * fps)

            self.frames = self.processor.extract_frames(
                self.video_path,
                start_frame,
                num_frames,
                stride=1
            )

            if not self.frames:
                return None, "âŒ í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤", default_slider

            # ì´ˆê¸°í™”
            self.current_frame_idx = 0
            self.annotations = {'foreground': [], 'background': []}
            self.masks = [None] * len(self.frames)
            self.current_mask = None

            info_text = f"""
### ë¹„ë””ì˜¤ ë¡œë“œ ì™„ë£Œ âœ…

- **í”„ë ˆì„ ìˆ˜**: {len(self.frames)}
- **í•´ìƒë„**: {info['width']} x {info['height']}
- **FPS**: {info['fps']:.2f}
- **êµ¬ê°„**: {start_time}s - {start_time + duration}s

### ë‹¤ìŒ ë‹¨ê³„:
1. **Foreground Point** í´ë¦­í•˜ì—¬ ê°ì²´ ìœ„ì¹˜ ì§€ì •
2. **Background Point** í´ë¦­í•˜ì—¬ ë°°ê²½ ìœ„ì¹˜ ì§€ì • (ì„ íƒì‚¬í•­)
3. **Segment Current Frame** í´ë¦­í•˜ì—¬ í˜„ì¬ í”„ë ˆì„ ì„¸ê·¸ë©˜í…Œì´ì…˜
4. **Propagate to All Frames** í´ë¦­í•˜ì—¬ ì „ì²´ ë¹„ë””ì˜¤ ì¶”ì 
5. **Generate 3D Mesh** í´ë¦­í•˜ì—¬ 3D ìƒì„±
            """

            # ì²« í”„ë ˆì„ ë°˜í™˜ + ìŠ¬ë¼ì´ë” ì—…ë°ì´íŠ¸ (self.framesëŠ” ì´ë¯¸ RGB)
            frame_rgb = self.frames[0].copy()

            # ìŠ¬ë¼ì´ë” ë²”ìœ„ ì—…ë°ì´íŠ¸
            slider_update = gr.Slider(
                label="í”„ë ˆì„ ìœ„ì¹˜",
                minimum=0,
                maximum=len(self.frames) - 1,
                value=0,
                step=1,
                interactive=True,
                info=f"ìŠ¬ë¼ì´ë”ë¥¼ ë“œë˜ê·¸í•˜ì—¬ í”„ë ˆì„ ì´ë™ (ì´ {len(self.frames)}ê°œ)"
            )

            return frame_rgb, info_text, slider_update

        except Exception as e:
            import traceback
            error_msg = f"""
### âŒ ì˜¤ë¥˜ ë°œìƒ

**ì—ëŸ¬ ë©”ì‹œì§€:**
```
{str(e)}
```

**ìƒì„¸ ì •ë³´:**
```
{traceback.format_exc()}
```

**í™•ì¸ì‚¬í•­:**
- ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œê°€ ì •í™•í•œê°€ìš”?
- ë¹„ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ë‚˜ìš”?
- íŒŒì¼ í˜•ì‹ì´ ì§€ì›ë˜ë‚˜ìš”? (MP4, AVI, MOV, MKV)
"""
            print(f"[ERROR] {error_msg}")
            return None, error_msg, default_slider

    def add_point(self, image: np.ndarray, point_type: str, evt: gr.SelectData) -> Tuple[np.ndarray, str]:
        """
        ì´ë¯¸ì§€ í´ë¦­ ì‹œ point ì¶”ê°€

        Args:
            image: í˜„ì¬ ì´ë¯¸ì§€
            point_type: 'foreground' or 'background'
            evt: Gradio í´ë¦­ ì´ë²¤íŠ¸
        """
        if image is None or len(self.frames) == 0:
            return image, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

        # í´ë¦­ ì¢Œí‘œ
        x, y = evt.index[0], evt.index[1]

        # Point ì¶”ê°€
        self.annotations[point_type].append((x, y))

        # í˜„ì¬ í”„ë ˆì„ì— point í‘œì‹œ (self.framesëŠ” ì´ë¯¸ RGB)
        frame_rgb = self.frames[self.current_frame_idx].copy()

        # Foreground points (ë…¹ìƒ‰)
        for px, py in self.annotations['foreground']:
            cv2.circle(frame_rgb, (px, py), 5, (0, 255, 0), -1)
            cv2.circle(frame_rgb, (px, py), 7, (255, 255, 255), 2)

        # Background points (ë¹¨ê°„ìƒ‰)
        for px, py in self.annotations['background']:
            cv2.circle(frame_rgb, (px, py), 5, (255, 0, 0), -1)
            cv2.circle(frame_rgb, (px, py), 7, (255, 255, 255), 2)

        status = f"""
**Annotations:**
- Foreground: {len(self.annotations['foreground'])} points
- Background: {len(self.annotations['background'])} points

í´ë¦­í•œ ìœ„ì¹˜: ({x}, {y}) - {point_type}
"""

        return frame_rgb, status

    def segment_current_frame(self) -> Tuple[np.ndarray, str]:
        """
        í˜„ì¬ í”„ë ˆì„ì„ SAM2ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜
        SAM2 ëª¨ë¸ì´ í•„ìˆ˜ì´ë©°, ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì•ˆë‚´ í‘œì‹œ
        """
        if len(self.frames) == 0:
            return None, "âŒ ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

        if len(self.annotations['foreground']) == 0:
            return None, "âŒ ìµœì†Œ 1ê°œì˜ foreground pointê°€ í•„ìš”í•©ë‹ˆë‹¤"

        # SAM2 ëª¨ë¸ í™•ì¸ - ì—†ìœ¼ë©´ ì—ëŸ¬
        if self.sam2_predictor is None:
            checkpoint = self.SAM2_CHECKPOINT_PATH
            if not checkpoint.exists():
                return None, f"""âŒ **SAM2 ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤**

SAM2 ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.
ìƒë‹¨ì˜ **ğŸ”„ SAM2 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ** ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.

ë˜ëŠ” í„°ë¯¸ë„ì—ì„œ:
```
./download_checkpoints.sh
```

ì˜ˆìƒ ê²½ë¡œ: `{checkpoint}`
"""
            else:
                return None, """âŒ **SAM2 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤**

ìƒë‹¨ì˜ **ğŸ”„ SAM2 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ** ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”.
"""

        try:
            # self.framesëŠ” ì´ë¯¸ RGB
            frame_rgb = self.frames[self.current_frame_idx]

            # SAM2 inference
            self.sam2_predictor.set_image(frame_rgb)

            # Pointsì™€ labels ì¤€ë¹„
            point_coords = []
            point_labels = []

            for px, py in self.annotations['foreground']:
                point_coords.append([px, py])
                point_labels.append(1)  # foreground

            for px, py in self.annotations['background']:
                point_coords.append([px, py])
                point_labels.append(0)  # background

            point_coords = np.array(point_coords, dtype=np.float32)
            point_labels = np.array(point_labels, dtype=np.int32)

            # SAM2 predict
            masks, scores, _ = self.sam2_predictor.predict(
                point_coords=point_coords,
                point_labels=point_labels,
                multimask_output=True
            )

            # Best mask ì„ íƒ
            best_idx = np.argmax(scores)
            mask = masks[best_idx]
            confidence = scores[best_idx]

            status_method = f"SAM2 (confidence: {confidence:.3f})"

            # ë§ˆìŠ¤í¬ ì €ì¥
            self.masks[self.current_frame_idx] = mask
            self.current_mask = mask

            # ì‹œê°í™”
            overlay = frame_rgb.copy()
            overlay[mask > 0] = [0, 255, 0]  # ë…¹ìƒ‰ ë§ˆìŠ¤í¬
            result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)

            # Points í‘œì‹œ
            for px, py in self.annotations['foreground']:
                cv2.circle(result, (px, py), 5, (0, 255, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)
            for px, py in self.annotations['background']:
                cv2.circle(result, (px, py), 5, (255, 0, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)

            mask_area = np.sum(mask > 0)
            mask_pct = mask_area / mask.size * 100

            status = f"""
### Segmentation ì™„ë£Œ âœ…

- **Method**: {status_method}
- **í”„ë ˆì„**: {self.current_frame_idx + 1} / {len(self.frames)}
- **ë§ˆìŠ¤í¬ ì˜ì—­**: {mask_area} í”½ì…€ ({mask_pct:.1f}%)
- **Foreground points**: {len(self.annotations['foreground'])}
- **Background points**: {len(self.annotations['background'])}

### ë‹¤ìŒ:
- ë‹¤ë¥¸ í”„ë ˆì„ì—ë„ annotationí•˜ë ¤ë©´ í”„ë ˆì„ ì´ë™
- ë˜ëŠ” **Propagate to All Frames** í´ë¦­
"""

            return result, status

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return None, f"ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {str(e)}\n\n```\n{error_detail}\n```"

    def propagate_to_all_frames(self, stride: int = 1, progress=gr.Progress()) -> Tuple[np.ndarray, str]:
        """
        í˜„ì¬ í”„ë ˆì„ì˜ annotationì„ ì „ì²´ ë¹„ë””ì˜¤ì— propagation (tracking)
        SAM 2 Video Predictorë¥¼ ì‚¬ìš©í•œ ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì 

        Args:
            stride: í”„ë ˆì„ ì²˜ë¦¬ ê°„ê²© (1=ëª¨ë“  í”„ë ˆì„, 10=10í”„ë ˆì„ë§ˆë‹¤ ì²˜ë¦¬)

        ì¤‘ìš”: ê³ ì • pointsë¥¼ ëª¨ë“  í”„ë ˆì„ì— ì¬ì ìš©í•˜ì§€ ì•ŠìŒ!
        ëŒ€ì‹  SAM 2ì˜ memory mechanismì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ê°ì²´ ì¶”ì 
        """
        if len(self.frames) == 0:
            return None, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

        if len(self.annotations['foreground']) == 0:
            return None, "Annotation pointsê°€ í•„ìš”í•©ë‹ˆë‹¤ (ìµœì†Œ 1ê°œì˜ foreground point)"

        try:
            progress(0, desc="ë¹„ë””ì˜¤ tracking ì´ˆê¸°í™” (SAM 2 Video Predictor)...")

            # SAM 2 Video Predictor ì‚¬ìš© (ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì )
            if self.sam2_video_predictor is not None:
                # 1. ì„ì‹œ ë””ë ‰í† ë¦¬ì— í”„ë ˆì„ ì €ì¥ (SAM 2 Video PredictorëŠ” ë””ë ‰í† ë¦¬ ì…ë ¥ í•„ìš”)
                import tempfile
                import os
                temp_dir = tempfile.mkdtemp(prefix="sam3d_video_")

                try:
                    # ë©”ëª¨ë¦¬ ë³´í˜¸: ìµœëŒ€ 500 í”„ë ˆì„ìœ¼ë¡œ ì œí•œ (ì•½ 6GB ë©”ëª¨ë¦¬)
                    MAX_FRAMES = 500
                    effective_stride = stride
                    frame_indices = list(range(0, len(self.frames), stride))

                    if len(frame_indices) > MAX_FRAMES:
                        # stride ìë™ ì¡°ì •
                        effective_stride = max(stride, len(self.frames) // MAX_FRAMES)
                        frame_indices = list(range(0, len(self.frames), effective_stride))
                        print(f"âš ï¸ ë©”ëª¨ë¦¬ ë³´í˜¸: stride {stride} â†’ {effective_stride} ìë™ ì¡°ì • ({len(frame_indices)} í”„ë ˆì„)")

                    progress(0.05, desc=f"í”„ë ˆì„ ì €ì¥ ì¤‘ (stride={effective_stride}, ì´ {len(frame_indices)} í”„ë ˆì„)...")

                    # stride ê°„ê²©ìœ¼ë¡œë§Œ í”„ë ˆì„ ì €ì¥ (self.framesëŠ” RGBì´ë¯€ë¡œ BGRë¡œ ë³€í™˜)
                    for idx, i in enumerate(frame_indices):
                        frame_path = os.path.join(temp_dir, f"{idx:05d}.jpg")
                        cv2.imwrite(frame_path, cv2.cvtColor(self.frames[i], cv2.COLOR_RGB2BGR))

                    # ì›ë³¸ ì¸ë±ìŠ¤ ë§¤í•‘ ì €ì¥ (ë‚˜ì¤‘ì— ê²°ê³¼ë¥¼ ì›ë³¸ ì¸ë±ìŠ¤ë¡œ ë³µì›)
                    self.stride_frame_mapping = {idx: i for idx, i in enumerate(frame_indices)}
                    self.effective_stride = effective_stride  # status ë©”ì‹œì§€ë¥¼ ìœ„í•´ ì €ì¥

                    print(f"âœ“ {len(frame_indices)} í”„ë ˆì„ ì €ì¥ ì™„ë£Œ (ì›ë³¸ {len(self.frames)}ê°œ ì¤‘)")

                    progress(0.1, desc="SAM 2 Video Predictor ì´ˆê¸°í™” ì¤‘...")

                    # 2. Inference state ì´ˆê¸°í™”
                    inference_state = self.sam2_video_predictor.init_state(video_path=temp_dir)

                    # 3. í˜„ì¬ í”„ë ˆì„ì—ë§Œ annotation points ì¶”ê°€ (conditioning frame)
                    point_coords = []
                    point_labels = []

                    for px, py in self.annotations['foreground']:
                        point_coords.append([px, py])
                        point_labels.append(1)

                    for px, py in self.annotations['background']:
                        point_coords.append([px, py])
                        point_labels.append(0)

                    point_coords = np.array(point_coords, dtype=np.float32)
                    point_labels = np.array(point_labels, dtype=np.int32)

                    # í˜„ì¬ í”„ë ˆì„ ì¸ë±ìŠ¤ë¥¼ stride ê¸°ë°˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                    # ì˜ˆ: ì›ë³¸ í”„ë ˆì„ 20, stride=10 -> stride ì¸ë±ìŠ¤ 2
                    stride_frame_idx = self.current_frame_idx // stride
                    if self.current_frame_idx not in frame_indices:
                        # í˜„ì¬ í”„ë ˆì„ì´ strideì— í¬í•¨ë˜ì§€ ì•Šìœ¼ë©´ ê°€ì¥ ê°€ê¹Œìš´ í”„ë ˆì„ ì‚¬ìš©
                        stride_frame_idx = min(range(len(frame_indices)),
                                              key=lambda i: abs(frame_indices[i] - self.current_frame_idx))

                    progress(0.15, desc=f"ì´ˆê¸° í”„ë ˆì„ ({self.current_frame_idx} -> stride idx {stride_frame_idx}) annotation ì¤‘...")

                    # í˜„ì¬ í”„ë ˆì„ì„ conditioning frameìœ¼ë¡œ ì„¤ì •
                    _, out_obj_ids, out_mask_logits = self.sam2_video_predictor.add_new_points_or_box(
                        inference_state=inference_state,
                        frame_idx=stride_frame_idx,
                        obj_id=1,  # Single object tracking
                        points=point_coords,
                        labels=point_labels,
                    )

                    progress(0.2, desc="ë©”ëª¨ë¦¬ ê¸°ë°˜ ì „íŒŒ ì‹œì‘...")

                    # 4. Propagate using memory-based tracking (NO points on other frames!)
                    video_segments = {}
                    for stride_idx, obj_ids, mask_logits in self.sam2_video_predictor.propagate_in_video(
                        inference_state,
                        start_frame_idx=stride_frame_idx
                    ):
                        # Memory-based tracking - ê° í”„ë ˆì„ì€ ì´ì „ í”„ë ˆì„ì˜ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©
                        # PointsëŠ” ì¬ì ìš©ë˜ì§€ ì•ŠìŒ!
                        video_segments[stride_idx] = (mask_logits[0] > 0.0).cpu().numpy()

                        progress_pct = 0.2 + 0.6 * (stride_idx + 1) / len(frame_indices)
                        progress(progress_pct, desc=f"Tracking... {stride_idx+1}/{len(frame_indices)} (stride {stride})")

                    # 5. ê²°ê³¼ë¥¼ self.masksì— ì €ì¥ (stride ê°„ê²©ì˜ í”„ë ˆì„ë§Œ)
                    self.masks = [None] * len(self.frames)
                    for stride_idx, mask in video_segments.items():
                        original_idx = self.stride_frame_mapping.get(stride_idx)
                        if original_idx is not None and original_idx < len(self.masks):
                            self.masks[original_idx] = mask.squeeze()

                    progress(0.9, desc="Tracking ì™„ë£Œ, ê²°ê³¼ ì²˜ë¦¬ ì¤‘...")

                finally:
                    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                    import shutil
                    shutil.rmtree(temp_dir, ignore_errors=True)

            else:
                # Fallback: Image predictor ì‚¬ìš© (êµ¬ë²„ì „ ë°©ì‹ - ì •í™•ë„ ë‚®ìŒ)
                progress(0, desc="Fallback: í”„ë ˆì„ë³„ ì„¸ê·¸ë©˜í…Œì´ì…˜...")

                for i, frame in enumerate(self.frames):
                    if self.masks[i] is None:
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                        if self.sam2_predictor is not None:
                            self.sam2_predictor.set_image(frame_rgb)

                            point_coords = []
                            point_labels = []

                            for px, py in self.annotations['foreground']:
                                point_coords.append([px, py])
                                point_labels.append(1)

                            for px, py in self.annotations['background']:
                                point_coords.append([px, py])
                                point_labels.append(0)

                            point_coords = np.array(point_coords, dtype=np.float32)
                            point_labels = np.array(point_labels, dtype=np.int32)

                            masks, scores, _ = self.sam2_predictor.predict(
                                point_coords=point_coords,
                                point_labels=point_labels,
                                multimask_output=True
                            )

                            best_idx = np.argmax(scores)
                            mask = masks[best_idx]
                        else:
                            mask = self.processor.segment_object_interactive(frame, method='contour')

                        self.masks[i] = mask

                    progress((i + 1) / len(self.frames), desc=f"Processing... {i+1}/{len(self.frames)}")

            progress(1.0, desc="ì‹œê°í™” ì¤€ë¹„ ì¤‘...")

            # í˜„ì¬ í”„ë ˆì„ ì‹œê°í™” (self.framesëŠ” ì´ë¯¸ RGB)
            self.current_frame_idx = min(self.current_frame_idx, len(self.frames) - 1)
            current_frame = self.frames[self.current_frame_idx]
            current_mask = self.masks[self.current_frame_idx]

            frame_rgb = current_frame.copy()  # ì´ë¯¸ RGBì´ë¯€ë¡œ ë³€í™˜ ë¶ˆí•„ìš”
            overlay = frame_rgb.copy()
            if current_mask is not None:
                overlay[current_mask > 0] = [0, 255, 0]
            result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)

            # í†µê³„
            tracked_frames = sum(1 for m in self.masks if m is not None)

            method_used = "SAM 2 Video Predictor (Memory-based)" if self.sam2_video_predictor else "SAM 2 Image (Fallback)"

            # ì‹¤ì œ ì‚¬ìš©ëœ stride ê³„ì‚°
            used_stride = getattr(self, 'effective_stride', stride)

            status = f"""
### Propagation ì™„ë£Œ âœ…

- **Method**: {method_used}
- **Stride**: {used_stride} (ì²˜ë¦¬ëœ í”„ë ˆì„: {tracked_frames}, ì „ì²´: {len(self.frames)})
- **íš¨ìœ¨ì„±**: {100 * tracked_frames / len(self.frames):.1f}% í”„ë ˆì„ë§Œ ì²˜ë¦¬
- **í˜„ì¬ í”„ë ˆì„**: {self.current_frame_idx + 1} / {len(self.frames)}
- **Conditioning Frame**: {self.current_frame_idx} (Pointsë§Œ ì—¬ê¸° ì ìš©)

### ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì  (Stride ì ìš©):
- í˜„ì¬ í”„ë ˆì„ì˜ pointsë§Œ ì‚¬ìš©
- {used_stride} ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì²˜ë¦¬ (ì˜ˆ: 3000 í”„ë ˆì„ â†’ {tracked_frames} í”„ë ˆì„)
- ê°ì²´ ì´ë™ì—ë„ ì •í™•í•œ ë§ˆìŠ¤í¬ ìƒì„±
- ë©”ëª¨ë¦¬ ë³´í˜¸: ìµœëŒ€ 500 í”„ë ˆì„ìœ¼ë¡œ ìë™ ì œí•œ

### ë‹¤ìŒ:
- **í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜**ìœ¼ë¡œ ê²°ê³¼ í™•ì¸ (stride ê°„ê²©ë§Œ ë§ˆìŠ¤í¬ ì¡´ì¬)
- **Generate 3D Mesh** í´ë¦­í•˜ì—¬ 3D ìƒì„±
- ë˜ëŠ” **Save Masks** í´ë¦­í•˜ì—¬ ë§ˆìŠ¤í¬ ì €ì¥
"""

            return result, status

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return None, f"Propagation ì‹¤íŒ¨: {str(e)}\n\n```\n{error_detail}\n```"

    def download_sam3d_checkpoint(self, progress=gr.Progress()) -> bool:
        """
        SAM 3D ì²´í¬í¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
        """
        import subprocess
        import os
        from dotenv import load_dotenv

        progress(0, desc="SAM 3D ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘...")

        # .env íŒŒì¼ ë¡œë“œ
        env_path = Path(__file__).parent.parent / ".env"
        if env_path.exists():
            load_dotenv(env_path)
            print(f"âœ“ .env íŒŒì¼ ë¡œë“œë¨: {env_path}")
        else:
            print(f"âš ï¸ .env íŒŒì¼ ì—†ìŒ: {env_path}")

        # HuggingFace í† í° í™•ì¸
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            print("âš ï¸ HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ê°€ëŠ¥.")

        # Configì—ì„œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        if self.config:
            checkpoint_dir = Path(self.config.sam3d_checkpoint_dir).expanduser()
        else:
            # Fallback: relative to project root (í†µí•© êµ¬ì¡°)
            project_root = Path(__file__).parent.parent
            checkpoint_dir = project_root / "checkpoints" / "sam3d"

        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        progress(0.1, desc="Git LFS í™•ì¸ ì¤‘...")

        # Git LFS í™•ì¸ ë° ì„¤ì¹˜
        try:
            subprocess.run(["git", "lfs", "version"], check=True, capture_output=True)
        except:
            progress(0.2, desc="Git LFS ì„¤ì¹˜ ì¤‘...")
            try:
                subprocess.run(["sudo", "apt-get", "update"], check=True)
                subprocess.run(["sudo", "apt-get", "install", "-y", "git-lfs"], check=True)
                subprocess.run(["git", "lfs", "install"], check=True)
            except Exception as e:
                print(f"Git LFS ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
                return False

        progress(0.3, desc="SAM 3D ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (5-10GB, ì‹œê°„ ì†Œìš”)")

        # HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ (í† í° ì¸ì¦ ì‚¬ìš©)
        try:
            # í† í°ì´ ìˆìœ¼ë©´ ì¸ì¦ URL ì‚¬ìš©
            if hf_token:
                clone_url = f"https://oauth2:{hf_token}@huggingface.co/facebook/sam-3d-objects"
                pull_url = f"https://oauth2:{hf_token}@huggingface.co/facebook/sam-3d-objects"
            else:
                clone_url = "https://huggingface.co/facebook/sam-3d-objects"
                pull_url = "origin"

            if not (checkpoint_dir / "pipeline.yaml").exists():
                # ì²˜ìŒ ë‹¤ìš´ë¡œë“œ
                subprocess.run([
                    "git", "clone",
                    clone_url,
                    str(checkpoint_dir)
                ], check=True, cwd=checkpoint_dir.parent)
                progress(0.9, desc="ë‹¤ìš´ë¡œë“œ ì™„ë£Œ, ê²€ì¦ ì¤‘...")
            else:
                # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì—…ë°ì´íŠ¸
                if hf_token:
                    subprocess.run(["git", "pull", pull_url], check=True, cwd=checkpoint_dir)
                else:
                    subprocess.run(["git", "pull"], check=True, cwd=checkpoint_dir)
                progress(0.9, desc="ì—…ë°ì´íŠ¸ ì™„ë£Œ, ê²€ì¦ ì¤‘...")

            # ë‹¤ìš´ë¡œë“œ í™•ì¸
            if (checkpoint_dir / "pipeline.yaml").exists():
                progress(1.0, desc="SAM 3D ì²´í¬í¬ì¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ!")
                return True
            else:
                return False

        except Exception as e:
            print(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def generate_3d_mesh(
        self,
        seed: int = 42,
        stage1_steps: int = 25,
        stage2_steps: int = 25,
        with_postprocess: bool = False,
        simplify_ratio: float = 0.95,
        with_texture_baking: bool = False,
        texture_size: int = 1024,
        use_vertex_color: bool = True,
        progress=gr.Progress()
    ) -> Tuple[str, str]:
        """
        ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ë¡œ 3D mesh ìƒì„±

        Args:
            seed: ëœë¤ ì‹œë“œ (ì¬í˜„ì„±)
            stage1_steps: Stage 1 diffusion steps
            stage2_steps: Stage 2 diffusion steps
            with_postprocess: í›„ì²˜ë¦¬ í™œì„±í™”
            simplify_ratio: Face ìœ ì§€ ë¹„ìœ¨
            with_texture_baking: í…ìŠ¤ì²˜ ë² ì´í‚¹
            texture_size: í…ìŠ¤ì²˜ í•´ìƒë„
            use_vertex_color: ë²„í…ìŠ¤ ì»¬ëŸ¬ ì‚¬ìš©
        """
        # ë©”ì‹œ íŒŒë¼ë¯¸í„° ì„¤ì • ì €ì¥
        mesh_settings = {
            "seed": int(seed),
            "stage1_inference_steps": int(stage1_steps),
            "stage2_inference_steps": int(stage2_steps),
            "with_mesh_postprocess": with_postprocess,
            "simplify_ratio": float(simplify_ratio),
            "with_texture_baking": with_texture_baking,
            "texture_size": int(texture_size),
            "use_vertex_color": use_vertex_color
        }
        logger.info("=" * 60)
        logger.info("ğŸ”¹ generate_3d_mesh() ì‹œì‘")
        logger.info("=" * 60)

        # GPU ë©”ëª¨ë¦¬ ìƒíƒœ ë¡œê¹…
        if torch.cuda.is_available():
            mem_allocated = torch.cuda.memory_allocated() / 1024**3
            mem_reserved = torch.cuda.memory_reserved() / 1024**3
            mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.debug(f"GPU ë©”ëª¨ë¦¬: {mem_allocated:.2f}GB / {mem_total:.2f}GB (reserved: {mem_reserved:.2f}GB)")

        if len(self.frames) == 0 or all(m is None for m in self.masks):
            logger.error("âŒ í”„ë ˆì„ ë˜ëŠ” ë§ˆìŠ¤í¬ ì—†ìŒ")
            return None, "ë¨¼ì € ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ì™„ë£Œí•˜ì„¸ìš”"

        logger.debug(f"í”„ë ˆì„ ìˆ˜: {len(self.frames)}, ë§ˆìŠ¤í¬ ìˆ˜: {sum(1 for m in self.masks if m is not None)}")

        try:
            progress(0, desc="3D mesh ìƒì„± ì¤€ë¹„ ì¤‘...")

            # SAM 3D ì²´í¬í¬ì¸íŠ¸ í™•ì¸
            if self.config:
                checkpoint_dir = Path(self.config.sam3d_checkpoint_dir).expanduser()
                logger.info(f"âœ“ Configì—ì„œ checkpoint ê²½ë¡œ ë¡œë“œ: {checkpoint_dir}")
            else:
                # Fallback: relative to project root (í†µí•© êµ¬ì¡°)
                project_root = Path(__file__).parent.parent
                checkpoint_dir = project_root / "checkpoints" / "sam3d"
                logger.info(f"âœ“ ê¸°ë³¸ checkpoint ê²½ë¡œ ì‚¬ìš©: {checkpoint_dir}")

            logger.info(f"âœ“ Checkpoint ì¡´ì¬ í™•ì¸ ì¤‘: {checkpoint_dir}")
            logger.debug(f"   checkpoint_dir.exists(): {checkpoint_dir.exists()}")
            logger.debug(f"   pipeline.yaml ì¡´ì¬: {(checkpoint_dir / 'pipeline.yaml').exists()}")

            # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ëª©ë¡ ë¡œê¹…
            if checkpoint_dir.exists():
                ckpt_files = list(checkpoint_dir.glob("*.ckpt"))
                logger.debug(f"   .ckpt íŒŒì¼ ìˆ˜: {len(ckpt_files)}")
                for f in ckpt_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ
                    logger.debug(f"     - {f.name}: {f.stat().st_size / 1024**2:.1f} MB")

            if not (checkpoint_dir / "pipeline.yaml").exists():
                logger.error("âŒ pipeline.yaml íŒŒì¼ì´ ì—†ìŒ")
                progress(0.1, desc="SAM 3D ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ë‹¤ìš´ë¡œë“œ ì‹œì‘...")

                download_success = self.download_sam3d_checkpoint(progress)

                if not download_success:
                    return None, """
### âŒ SAM 3D ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

**ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°©ë²•:**
```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰
./download_checkpoints.sh
```

**í•„ìš”í•œ ì„¤ì •:**
1. `.env` íŒŒì¼ì— HuggingFace í† í° ì„¤ì •: `HF_TOKEN=your_token`
2. Git LFS ì„¤ì¹˜: `sudo apt install git-lfs`
"""

            # í˜„ì¬ ì„ íƒëœ í”„ë ˆì„ ì‚¬ìš©
            frame_idx = self.current_frame_idx
            frame = self.frames[frame_idx]
            mask = self.masks[frame_idx] if frame_idx < len(self.masks) else None

            logger.info(f"âœ“ í˜„ì¬ í”„ë ˆì„ ì„ íƒ: {frame_idx + 1}/{len(self.frames)}")
            logger.debug(f"   Frame shape: {frame.shape}, dtype: {frame.dtype}")
            logger.debug(f"   Mask shape: {mask.shape if mask is not None else 'None'}")
            logger.debug(f"   Mask type: {type(mask)}, unique values: {np.unique(mask) if mask is not None else 'N/A'}")

            if mask is None:
                logger.error(f"âŒ í”„ë ˆì„ {frame_idx + 1}ì— ë§ˆìŠ¤í¬ ì—†ìŒ")
                return None, f"í”„ë ˆì„ {frame_idx + 1}ì— ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìˆ˜í–‰í•˜ì„¸ìš”."

            # 3D ì¬êµ¬ì„± ì‹œë„
            logger.info("âœ“ 3D ì¬êµ¬ì„± ì‹œì‘...")
            logger.info(f"   Mesh ì„¤ì •: seed={mesh_settings['seed']}, steps={mesh_settings['stage1_inference_steps']}/{mesh_settings['stage2_inference_steps']}")
            logger.debug(f"   SAM3DProcessor checkpoint: {self.processor.sam3d_checkpoint}")
            progress(0.5, desc="SAM 3D ì¬êµ¬ì„± ì¤‘...")

            # Unload SAM2 models to free GPU memory for SAM 3D
            # Critical for RTX 3060 12GB: SAM2 (3GB) + SAM3D (10GB) = 13GB > 12GB
            self.unload_sam2_models()

            try:
                logger.info("SAM3D inference ì‹œì‘...")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    mem_before = torch.cuda.memory_allocated() / 1024**3
                    logger.debug(f"   GPU ë©”ëª¨ë¦¬ (inference ì „): {mem_before:.2f} GB")

                # íŒŒë¼ë¯¸í„°ë¥¼ ì „ë‹¬í•˜ì—¬ ì¬êµ¬ì„±
                reconstruction = self.processor.reconstruct_3d(
                    frame, mask,
                    seed=mesh_settings['seed'],
                    mesh_settings=mesh_settings
                )

                if torch.cuda.is_available():
                    mem_after = torch.cuda.memory_allocated() / 1024**3
                    logger.debug(f"   GPU ë©”ëª¨ë¦¬ (inference í›„): {mem_after:.2f} GB")

                logger.info(f"âœ“ Reconstruction ì™„ë£Œ: {type(reconstruction)}")

                if reconstruction:
                    # PLY ì €ì¥ - í”„ë ˆì„ ë²ˆí˜¸ì™€ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ê³ ìœ  íŒŒì¼ëª… ìƒì„±
                    from datetime import datetime
                    import json
                    project_root = Path(__file__).parent.parent
                    output_dir = project_root / "outputs" / "3d_meshes"
                    output_dir.mkdir(parents=True, exist_ok=True)

                    timestamp = datetime.now().strftime("%H%M%S")
                    filename = f"mesh_frame{frame_idx:04d}_{timestamp}.ply"
                    output_path = output_dir / filename

                    logger.info(f"âœ“ Mesh ì €ì¥ ì¤‘: {output_path}")
                    self.processor.export_mesh(reconstruction, str(output_path), format='ply')
                    logger.info(f"âœ“ Mesh ì €ì¥ ì™„ë£Œ")
                    logger.debug(f"   Output keys: {reconstruction.keys() if isinstance(reconstruction, dict) else 'N/A'}")

                    # ì„¤ì • íŒŒì¼ ì €ì¥
                    settings_filename = f"mesh_frame{frame_idx:04d}_{timestamp}_settings.json"
                    settings_path = output_dir / settings_filename
                    settings_data = {
                        "timestamp": datetime.now().isoformat(),
                        "source": {
                            "video_path": getattr(self, 'video_path', None),
                            "frame_idx": frame_idx,
                            "total_frames": len(self.frames)
                        },
                        "parameters": mesh_settings,
                        "output": {
                            "filename": filename,
                            "format": "ply"
                        }
                    }
                    with open(settings_path, 'w', encoding='utf-8') as f:
                        json.dump(settings_data, f, indent=2, ensure_ascii=False)
                    logger.info(f"âœ“ ì„¤ì • ì €ì¥: {settings_path}")

                    progress(1.0, desc="ì™„ë£Œ!")

                    status = f"""
### 3D Mesh ìƒì„± ì™„ë£Œ âœ…

- **í”„ë ˆì„**: {frame_idx + 1} / {len(self.frames)}
- **ì €ì¥ ìœ„ì¹˜**: `{output_path}`
- **ì„¤ì • íŒŒì¼**: `{settings_path}`

**íŒŒë¼ë¯¸í„°:**
- Seed: {mesh_settings['seed']}
- Steps: {mesh_settings['stage1_inference_steps']}/{mesh_settings['stage2_inference_steps']}
- í›„ì²˜ë¦¬: {'âœ“' if mesh_settings['with_mesh_postprocess'] else 'âœ—'}

### 3D ë·°ì–´ë¡œ í™•ì¸:
```bash
meshlab {output_path}
```

ë˜ëŠ” ì˜¨ë¼ì¸: https://3dviewer.net/
"""
                    print("âœ… generate_3d_mesh() ì™„ë£Œ")

                    # Reload SAM2 models for continued use
                    self.reload_sam2_models()

                    return str(output_path), status
                else:
                    print("âŒ Reconstructionì´ None")

                    # Reload SAM2 models even on failure
                    self.reload_sam2_models()

                    return None, "3D ì¬êµ¬ì„± ì‹¤íŒ¨ (SAM 3D ì²´í¬í¬ì¸íŠ¸ í•„ìš”)"

            except Exception as e:
                # SAM 3D ì—†ìœ¼ë©´ ê°„ë‹¨í•œ point cloudë§Œ ìƒì„±
                print(f"âŒ 3D ì¬êµ¬ì„± ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()

                # Reload SAM2 models even on failure
                self.reload_sam2_models()

                return None, f"3D ì¬êµ¬ì„± ì‹¤íŒ¨: {str(e)}\n\nSAM 3D ì²´í¬í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤."

        except Exception as e:
            import traceback
            return None, f"ì˜¤ë¥˜:\n{str(e)}\n{traceback.format_exc()}"

    def batch_generate_3d_mesh_current(
        self,
        video_idx: int,
        frame_idx: int,
        seed: int = 42,
        stage1_steps: int = 25,
        stage2_steps: int = 25,
        with_postprocess: bool = False,
        simplify_ratio: float = 0.95,
        with_texture_baking: bool = False,
        texture_size: int = 1024,
        use_vertex_color: bool = True,
        progress=gr.Progress()
    ) -> Tuple[str, str]:
        """
        Batch mode: í˜„ì¬ ì„ íƒëœ ë¹„ë””ì˜¤/í”„ë ˆì„ì˜ 3D Mesh ìƒì„±
        """
        from datetime import datetime
        import json

        # ë©”ì‹œ íŒŒë¼ë¯¸í„° ì„¤ì •
        mesh_settings = {
            "seed": int(seed),
            "stage1_inference_steps": int(stage1_steps),
            "stage2_inference_steps": int(stage2_steps),
            "with_mesh_postprocess": with_postprocess,
            "simplify_ratio": float(simplify_ratio),
            "with_texture_baking": with_texture_baking,
            "texture_size": int(texture_size),
            "use_vertex_color": use_vertex_color
        }

        if not hasattr(self, 'batch_results') or not self.batch_results:
            return None, "ë¨¼ì € Batch Propagateë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."

        # batch_resultsëŠ” {'videos': [...], 'temp_dir': ..., 'total_frames': ...} êµ¬ì¡°
        videos = self.batch_results.get('videos', [])
        if not videos:
            return None, "ë¹„ë””ì˜¤ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        # video_idxë¡œ í•´ë‹¹ ë¹„ë””ì˜¤ ì°¾ê¸°
        video_result = None
        for v in videos:
            if v.get('video_idx') == video_idx:
                video_result = v
                break

        if video_result is None:
            return None, f"ë¹„ë””ì˜¤ ì¸ë±ìŠ¤ {video_idx}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì´ {len(videos)}ê°œ ë¹„ë””ì˜¤)"

        video_name = video_result.get('video_name', f'video_{video_idx:03d}')
        video_path = video_result.get('video_path', '')
        unique_id = self._generate_unique_video_id(video_path) if video_path else video_name
        result_dir = video_result.get('result_dir', '')

        # í”„ë ˆì„ ë””ë ‰í† ë¦¬ì—ì„œ ë§ˆìŠ¤í¬ì™€ ì´ë¯¸ì§€ ë¡œë“œ
        if not result_dir or not Path(result_dir).exists():
            return None, f"ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {result_dir}"

        frame_dirs = sorted([d for d in Path(result_dir).iterdir() if d.is_dir() and d.name.startswith('frame_')])
        if not frame_dirs:
            return None, f"ë¹„ë””ì˜¤ {video_name}ì— í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤."

        if frame_idx < 0 or frame_idx >= len(frame_dirs):
            return None, f"ì˜ëª»ëœ í”„ë ˆì„ ì¸ë±ìŠ¤: {frame_idx} (ì´ {len(frame_dirs)}ê°œ í”„ë ˆì„)"

        # í”„ë ˆì„ ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ ë¡œë“œ
        frame_dir = frame_dirs[frame_idx]
        original_path = frame_dir / "original.png"
        mask_path = frame_dir / "mask.png"

        if not original_path.exists():
            return None, f"ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {original_path}"
        if not mask_path.exists():
            return None, f"ë§ˆìŠ¤í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {mask_path}"

        import cv2
        frame = cv2.imread(str(original_path))
        if frame is None:
            return None, f"ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {original_path}"
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
        if mask is None:
            return None, f"ë§ˆìŠ¤í¬ ë¡œë“œ ì‹¤íŒ¨: {mask_path}"
        mask = (mask > 127).astype(np.uint8) * 255  # ì´ì§„ ë§ˆìŠ¤í¬ë¡œ ë³€í™˜

        # ë§ˆìŠ¤í¬ ìœ íš¨ì„± ê²€ì‚¬
        mask_pixels = np.sum(mask > 0)
        total_pixels = mask.shape[0] * mask.shape[1]
        mask_ratio = mask_pixels / total_pixels

        logger.info(f"ë§ˆìŠ¤í¬ ì •ë³´: {mask_pixels} í”½ì…€ ({mask_ratio*100:.2f}%), shape={mask.shape}")

        if mask_pixels == 0:
            return None, f"ë§ˆìŠ¤í¬ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìœ íš¨í•œ ì„¸ê·¸ë©˜í…Œì´ì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤."

        if mask_pixels < 100:
            return None, f"ë§ˆìŠ¤í¬ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤ ({mask_pixels} í”½ì…€). ìµœì†Œ 100í”½ì…€ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤."

        if mask_ratio < 0.001:
            return None, f"ë§ˆìŠ¤í¬ ì˜ì—­ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤ ({mask_ratio*100:.3f}%). ê°ì²´ê°€ ì´ë¯¸ì§€ì—ì„œ ì¶©ë¶„íˆ ë³´ì´ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."

        logger.info(f"Batch 3D Mesh ìƒì„±: {unique_id} ({video_name}), frame {frame_idx}")
        logger.info(f"   Mesh ì„¤ì •: seed={mesh_settings['seed']}, steps={mesh_settings['stage1_inference_steps']}/{mesh_settings['stage2_inference_steps']}")
        progress(0.3, desc="SAM 3D ì´ˆê¸°í™” ì¤‘...")

        # Unload SAM2 for memory
        self.unload_sam2_models()

        try:
            progress(0.5, desc="3D ì¬êµ¬ì„± ì¤‘...")
            reconstruction = self.processor.reconstruct_3d(
                frame, mask,
                seed=mesh_settings['seed'],
                mesh_settings=mesh_settings
            )

            if reconstruction:
                # ì„¸ì…˜ í´ë” ë‚´ë¶€ì— ì €ì¥ (ìˆìœ¼ë©´), ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ
                output_dir = self._get_session_mesh_dir()

                timestamp = datetime.now().strftime("%H%M%S")
                filename = f"{unique_id}_frame{frame_idx:04d}_{timestamp}.ply"
                output_path = output_dir / filename

                self.processor.export_mesh(reconstruction, str(output_path), format='ply')
                logger.info(f"Mesh ì €ì¥ ì™„ë£Œ: {output_path}")

                # ì„¤ì • íŒŒì¼ ì €ì¥
                settings_filename = f"{unique_id}_frame{frame_idx:04d}_{timestamp}_settings.json"
                settings_path = output_dir / settings_filename
                settings_data = {
                    "timestamp": datetime.now().isoformat(),
                    "source": {
                        "session_path": self.current_session_path,
                        "video_name": video_name,
                        "unique_id": unique_id,
                        "video_idx": video_idx,
                        "frame_idx": frame_idx,
                        "total_frames": len(frame_dirs)
                    },
                    "parameters": mesh_settings,
                    "output": {
                        "filename": filename,
                        "format": "ply"
                    }
                }
                with open(settings_path, 'w', encoding='utf-8') as f:
                    json.dump(settings_data, f, indent=2, ensure_ascii=False)

                # ì„¸ì…˜ ë©”íƒ€ë°ì´í„°ì— mesh ì •ë³´ ì¶”ê°€
                mesh_info = {
                    "unique_id": unique_id,
                    "video_name": video_name,
                    "video_idx": video_idx,
                    "frame_idx": frame_idx,
                    "filename": filename,
                    "settings_file": settings_filename,
                    "timestamp": datetime.now().isoformat(),
                    "parameters": mesh_settings
                }
                self._update_session_mesh_metadata(mesh_info)

                progress(1.0, desc="ì™„ë£Œ!")
                self.reload_sam2_models()

                status = f"""### 3D Mesh ìƒì„± ì™„ë£Œ âœ…

- **ë¹„ë””ì˜¤**: {video_name}
- **í”„ë ˆì„**: {frame_idx + 1}
- **ì €ì¥ ìœ„ì¹˜**: `{output_path}`
- **ì„¤ì • íŒŒì¼**: `{settings_path}`
- **ì„¸ì…˜ ë©”íƒ€ë°ì´í„°**: ìë™ ì—…ë°ì´íŠ¸ë¨

**íŒŒë¼ë¯¸í„°:**
- Seed: {mesh_settings['seed']}
- Steps: {mesh_settings['stage1_inference_steps']}/{mesh_settings['stage2_inference_steps']}
- í›„ì²˜ë¦¬: {'âœ“' if mesh_settings['with_mesh_postprocess'] else 'âœ—'}
"""
                return str(output_path), status
            else:
                self.reload_sam2_models()
                return None, "3D ì¬êµ¬ì„± ì‹¤íŒ¨"

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"Batch 3D Mesh ìƒì„± ì‹¤íŒ¨: {e}\n{error_details}")
            self.reload_sam2_models()

            # ë” ì¹œì ˆí•œ ì˜¤ë¥˜ ë©”ì‹œì§€
            error_msg = str(e)
            if "numel() == 0" in error_msg or "reduction dim" in error_msg:
                return None, f"""### 3D Mesh ìƒì„± ì‹¤íŒ¨ âŒ

**ì›ì¸**: ë§ˆìŠ¤í¬ì—ì„œ ìœ íš¨í•œ 3D êµ¬ì¡°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ê°€ëŠ¥í•œ í•´ê²° ë°©ë²•**:
1. ë‹¤ë¥¸ í”„ë ˆì„ì„ ì„ íƒí•´ ë³´ì„¸ìš” (ê°ì²´ê°€ ë” ëª…í™•í•œ í”„ë ˆì„)
2. ë§ˆìŠ¤í¬ í’ˆì§ˆ í™•ì¸ - ê°ì²´ê°€ ì™„ì „íˆ ì„¸ê·¸ë©˜í…Œì´ì…˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
3. Seed ê°’ì„ ë³€ê²½í•´ ë³´ì„¸ìš”

**ë””ë²„ê·¸ ì •ë³´**: ë§ˆìŠ¤í¬ {mask_pixels} í”½ì…€ ({mask_ratio*100:.2f}%)
"""
            else:
                return None, f"3D Mesh ìƒì„± ì‹¤íŒ¨: {error_msg}\n\nìì„¸í•œ ë¡œê·¸ëŠ” í„°ë¯¸ë„ì„ í™•ì¸í•˜ì„¸ìš”."

    def batch_generate_3d_mesh_selected(
        self,
        selected_frames: List[dict],
        seed: int = 42,
        stage1_steps: int = 25,
        stage2_steps: int = 25,
        with_postprocess: bool = False,
        simplify_ratio: float = 0.95,
        with_texture_baking: bool = False,
        texture_size: int = 1024,
        use_vertex_color: bool = True,
        progress=gr.Progress()
    ) -> Tuple[str, str]:
        """
        ì„ íƒëœ í”„ë ˆì„ë“¤ì˜ 3D Mesh ì¼ê´„ ìƒì„±

        Args:
            selected_frames: ì„ íƒëœ í”„ë ˆì„ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{video_idx, video_name, frame_idx}, ...]
            ê¸°íƒ€: mesh ìƒì„± íŒŒë¼ë¯¸í„°

        Returns:
            (ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ, ìƒíƒœ ë©”ì‹œì§€)
        """
        from datetime import datetime
        import json

        if not selected_frames:
            return None, "ì„ íƒëœ í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í”„ë ˆì„ì„ ì¶”ê°€í•˜ì„¸ìš”."

        if not hasattr(self, 'batch_results') or not self.batch_results:
            return None, "Batch ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Batch Propagateë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."

        # ë©”ì‹œ íŒŒë¼ë¯¸í„° ì„¤ì •
        mesh_settings = {
            "seed": int(seed),
            "stage1_inference_steps": int(stage1_steps),
            "stage2_inference_steps": int(stage2_steps),
            "with_mesh_postprocess": with_postprocess,
            "simplify_ratio": float(simplify_ratio),
            "with_texture_baking": with_texture_baking,
            "texture_size": int(texture_size),
            "use_vertex_color": use_vertex_color
        }

        # ì„¸ì…˜ í´ë” ë‚´ë¶€ì— ì €ì¥
        output_dir = self._get_session_mesh_dir()

        generated_meshes = []
        failed_meshes = []
        total = len(selected_frames)

        logger.info(f"ì„ íƒëœ í”„ë ˆì„ 3D Mesh ìƒì„± ì‹œì‘: {total}ê°œ í”„ë ˆì„")
        logger.info(f"   Mesh ì„¤ì •: seed={mesh_settings['seed']}, steps={mesh_settings['stage1_inference_steps']}/{mesh_settings['stage2_inference_steps']}")

        # ì²« ë²ˆì§¸ í”„ë ˆì„ ì „ì— SAM2 ì–¸ë¡œë“œ
        self.unload_sam2_models()

        for i, frame_info in enumerate(selected_frames):
            video_idx = frame_info['video_idx']
            frame_idx = frame_info['frame_idx']
            video_name = frame_info.get('video_name', f'video_{video_idx:03d}')

            # batch_resultsëŠ” {'videos': [...], 'temp_dir': ..., 'total_frames': ...} êµ¬ì¡°
            videos = self.batch_results.get('videos', [])

            # video_idxë¡œ í•´ë‹¹ ë¹„ë””ì˜¤ ì°¾ê¸°
            video_result = None
            for v in videos:
                if v.get('video_idx') == video_idx:
                    video_result = v
                    break

            if video_result is None:
                failed_meshes.append(f"{video_name} frame {frame_idx}: ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                continue

            video_path = video_result.get('video_path', '')
            unique_id = self._generate_unique_video_id(video_path) if video_path else video_name
            result_dir = video_result.get('result_dir', '')

            progress((i + 0.2) / total, desc=f"3D Mesh ìƒì„± ì¤‘: {unique_id} frame {frame_idx}")

            # í”„ë ˆì„ ë””ë ‰í† ë¦¬ì—ì„œ ë§ˆìŠ¤í¬ì™€ ì´ë¯¸ì§€ ë¡œë“œ
            if not result_dir or not Path(result_dir).exists():
                failed_meshes.append(f"{unique_id} frame {frame_idx}: ê²°ê³¼ ë””ë ‰í† ë¦¬ ì—†ìŒ")
                continue

            frame_dirs = sorted([d for d in Path(result_dir).iterdir() if d.is_dir() and d.name.startswith('frame_')])
            if not frame_dirs:
                failed_meshes.append(f"{unique_id} frame {frame_idx}: í”„ë ˆì„ ë””ë ‰í† ë¦¬ ì—†ìŒ")
                continue

            if frame_idx < 0 or frame_idx >= len(frame_dirs):
                failed_meshes.append(f"{unique_id} frame {frame_idx}: ì˜ëª»ëœ í”„ë ˆì„ ì¸ë±ìŠ¤ (ì´ {len(frame_dirs)}ê°œ)")
                continue

            # í”„ë ˆì„ ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ ë¡œë“œ
            frame_dir = frame_dirs[frame_idx]
            original_path = frame_dir / "original.png"
            mask_path = frame_dir / "mask.png"

            if not original_path.exists():
                failed_meshes.append(f"{unique_id} frame {frame_idx}: ì›ë³¸ ì´ë¯¸ì§€ ì—†ìŒ")
                continue
            if not mask_path.exists():
                failed_meshes.append(f"{unique_id} frame {frame_idx}: ë§ˆìŠ¤í¬ íŒŒì¼ ì—†ìŒ")
                continue

            import cv2
            frame = cv2.imread(str(original_path))
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
            mask = (mask > 127).astype(np.uint8) * 255  # ì´ì§„ ë§ˆìŠ¤í¬ë¡œ ë³€í™˜

            try:
                progress((i + 0.5) / total, desc=f"3D ì¬êµ¬ì„± ì¤‘: {unique_id} frame {frame_idx}")

                reconstruction = self.processor.reconstruct_3d(
                    frame, mask,
                    seed=mesh_settings['seed'],
                    mesh_settings=mesh_settings
                )

                if reconstruction:
                    timestamp = datetime.now().strftime("%H%M%S")
                    filename = f"{unique_id}_frame{frame_idx:04d}_{timestamp}.ply"
                    output_path = output_dir / filename

                    self.processor.export_mesh(reconstruction, str(output_path), format='ply')

                    # ì„¤ì • íŒŒì¼ ì €ì¥
                    settings_filename = f"{unique_id}_frame{frame_idx:04d}_{timestamp}_settings.json"
                    settings_path = output_dir / settings_filename
                    settings_data = {
                        "timestamp": datetime.now().isoformat(),
                        "source": {
                            "session_path": self.current_session_path,
                            "unique_id": unique_id,
                            "video_name": video_name,
                            "video_idx": video_idx,
                            "frame_idx": frame_idx,
                            "total_frames": len(frame_dirs)
                        },
                        "parameters": mesh_settings,
                        "output": {
                            "filename": filename,
                            "format": "ply"
                        }
                    }
                    with open(settings_path, 'w', encoding='utf-8') as f:
                        json.dump(settings_data, f, indent=2, ensure_ascii=False)

                    # ì„¸ì…˜ ë©”íƒ€ë°ì´í„°ì— mesh ì •ë³´ ì¶”ê°€
                    mesh_info = {
                        "unique_id": unique_id,
                        "video_name": video_name,
                        "video_idx": video_idx,
                        "frame_idx": frame_idx,
                        "filename": filename,
                        "settings_file": settings_filename,
                        "timestamp": datetime.now().isoformat(),
                        "parameters": mesh_settings
                    }
                    self._update_session_mesh_metadata(mesh_info)

                    generated_meshes.append({
                        'unique_id': unique_id,
                        'video': video_name,
                        'video_idx': video_idx,
                        'frame': frame_idx,
                        'path': str(output_path)
                    })
                    logger.info(f"Generated: {filename}")
                else:
                    failed_meshes.append(f"{video_name} frame {frame_idx}: ì¬êµ¬ì„± ì‹¤íŒ¨")

            except Exception as e:
                logger.error(f"Failed {video_name} frame {frame_idx}: {e}")
                failed_meshes.append(f"{video_name} frame {frame_idx}: {str(e)}")
                continue

        # SAM2 ë‹¤ì‹œ ë¡œë“œ
        self.reload_sam2_models()

        progress(1.0, desc="ì™„ë£Œ!")

        if generated_meshes:
            mesh_list = "\n".join([f"- {m['video']} (frame {m['frame']})" for m in generated_meshes])
            failed_list = "\n".join([f"- {f}" for f in failed_meshes]) if failed_meshes else ""

            status = f"""### ì„ íƒ í”„ë ˆì„ 3D Mesh ìƒì„± ì™„ë£Œ âœ…

**ìƒì„± ì„±ê³µ**: {len(generated_meshes)}/{total}ê°œ
**ì €ì¥ ìœ„ì¹˜**: `{output_dir}`

**íŒŒë¼ë¯¸í„°:**
- Seed: {mesh_settings['seed']}
- Steps: {mesh_settings['stage1_inference_steps']}/{mesh_settings['stage2_inference_steps']}
- í›„ì²˜ë¦¬: {'âœ“' if mesh_settings['with_mesh_postprocess'] else 'âœ—'}

**ìƒì„±ëœ ë©”ì‹œ:**
{mesh_list}
"""
            if failed_meshes:
                status += f"""
**ì‹¤íŒ¨í•œ í”„ë ˆì„:**
{failed_list}
"""
            return str(output_dir), status
        else:
            return None, f"3D Mesh ìƒì„± ì‹¤íŒ¨ (ëª¨ë“  í”„ë ˆì„)\n\nì‹¤íŒ¨ ëª©ë¡:\n" + "\n".join([f"- {f}" for f in failed_meshes])

    def batch_generate_3d_mesh_all(
        self,
        seed: int = 42,
        stage1_steps: int = 25,
        stage2_steps: int = 25,
        with_postprocess: bool = False,
        simplify_ratio: float = 0.95,
        with_texture_baking: bool = False,
        texture_size: int = 1024,
        use_vertex_color: bool = True,
        progress=gr.Progress()
    ) -> Tuple[str, str]:
        """
        Batch mode: ëª¨ë“  ë¹„ë””ì˜¤ì˜ ì¤‘ê°„ í”„ë ˆì„ì—ì„œ 3D Mesh ìƒì„±
        """
        from datetime import datetime
        import json

        # ë©”ì‹œ íŒŒë¼ë¯¸í„° ì„¤ì •
        mesh_settings = {
            "seed": int(seed),
            "stage1_inference_steps": int(stage1_steps),
            "stage2_inference_steps": int(stage2_steps),
            "with_mesh_postprocess": with_postprocess,
            "simplify_ratio": float(simplify_ratio),
            "with_texture_baking": with_texture_baking,
            "texture_size": int(texture_size),
            "use_vertex_color": use_vertex_color
        }

        if not hasattr(self, 'batch_results') or not self.batch_results:
            return None, "ë¨¼ì € Batch Propagateë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."

        # batch_resultsëŠ” {'videos': [...], 'temp_dir': ..., 'total_frames': ...} êµ¬ì¡°
        videos = self.batch_results.get('videos', [])
        if not videos:
            return None, "ë¹„ë””ì˜¤ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ì„¸ì…˜ í´ë” ë‚´ë¶€ì— ì €ì¥ (ìˆìœ¼ë©´), ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ
        output_dir = self._get_session_mesh_dir()

        generated_meshes = []
        total = len(videos)

        logger.info(f"ì „ì²´ 3D Mesh ìƒì„± ì‹œì‘: {total}ê°œ ë¹„ë””ì˜¤")
        logger.info(f"   Mesh ì„¤ì •: seed={mesh_settings['seed']}, steps={mesh_settings['stage1_inference_steps']}/{mesh_settings['stage2_inference_steps']}")

        for i, video_result in enumerate(videos):
            video_idx = video_result.get('video_idx', i)
            video_name = video_result.get('video_name', f'video_{i:03d}')
            video_path = video_result.get('video_path', '')
            unique_id = self._generate_unique_video_id(video_path) if video_path else video_name
            result_dir = video_result.get('result_dir', '')

            # í”„ë ˆì„ ë””ë ‰í† ë¦¬ í™•ì¸
            if not result_dir or not Path(result_dir).exists():
                logger.warning(f"Skip {unique_id}: result_dir not found")
                continue

            frame_dirs = sorted([d for d in Path(result_dir).iterdir() if d.is_dir() and d.name.startswith('frame_')])
            if not frame_dirs:
                logger.warning(f"Skip {unique_id}: no frame directories")
                continue

            # ì¤‘ê°„ í”„ë ˆì„ ì„ íƒ
            mid_idx = len(frame_dirs) // 2
            frame_dir = frame_dirs[mid_idx]
            original_path = frame_dir / "original.png"
            mask_path = frame_dir / "mask.png"

            if not original_path.exists() or not mask_path.exists():
                logger.warning(f"Skip {unique_id}: no original/mask at frame {mid_idx}")
                continue

            import cv2
            frame = cv2.imread(str(original_path))
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
            mask = (mask > 127).astype(np.uint8) * 255

            progress((i + 0.5) / total, desc=f"3D Mesh ìƒì„± ì¤‘: {unique_id} ({i+1}/{total})")

            # ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ ì „ì— SAM2 ì–¸ë¡œë“œ
            if i == 0:
                self.unload_sam2_models()

            try:
                reconstruction = self.processor.reconstruct_3d(
                    frame, mask,
                    seed=mesh_settings['seed'],
                    mesh_settings=mesh_settings
                )

                if reconstruction:
                    timestamp = datetime.now().strftime("%H%M%S")
                    filename = f"{unique_id}_frame{mid_idx:04d}_{timestamp}.ply"
                    output_path = output_dir / filename

                    self.processor.export_mesh(reconstruction, str(output_path), format='ply')

                    # ì„¤ì • íŒŒì¼ ì €ì¥
                    settings_filename = f"{unique_id}_frame{mid_idx:04d}_{timestamp}_settings.json"
                    settings_path = output_dir / settings_filename
                    settings_data = {
                        "timestamp": datetime.now().isoformat(),
                        "source": {
                            "session_path": self.current_session_path,
                            "unique_id": unique_id,
                            "video_name": video_name,
                            "video_idx": video_idx,
                            "frame_idx": mid_idx,
                            "total_frames": len(frame_dirs)
                        },
                        "parameters": mesh_settings,
                        "output": {
                            "filename": filename,
                            "format": "ply"
                        }
                    }
                    with open(settings_path, 'w', encoding='utf-8') as f:
                        json.dump(settings_data, f, indent=2, ensure_ascii=False)

                    # ì„¸ì…˜ ë©”íƒ€ë°ì´í„°ì— mesh ì •ë³´ ì¶”ê°€
                    mesh_info = {
                        "unique_id": unique_id,
                        "video_name": video_name,
                        "video_idx": video_idx,
                        "frame_idx": mid_idx,
                        "filename": filename,
                        "settings_file": settings_filename,
                        "timestamp": datetime.now().isoformat(),
                        "parameters": mesh_settings
                    }
                    self._update_session_mesh_metadata(mesh_info)

                    generated_meshes.append({
                        'unique_id': unique_id,
                        'video': video_name,
                        'video_idx': video_idx,
                        'frame': mid_idx,
                        'path': str(output_path)
                    })
                    logger.info(f"Generated: {filename}")

            except Exception as e:
                logger.error(f"Failed {video_name}: {e}")
                continue

        # SAM2 ë‹¤ì‹œ ë¡œë“œ
        self.reload_sam2_models()

        progress(1.0, desc="ì™„ë£Œ!")

        if generated_meshes:
            mesh_list = "\n".join([f"- {m['unique_id']} (frame {m['frame']}): `{m['path']}`" for m in generated_meshes])
            status = f"""### ì „ì²´ 3D Mesh ìƒì„± ì™„ë£Œ âœ…

**ìƒì„±ëœ ë©”ì‹œ**: {len(generated_meshes)}/{total}

**íŒŒë¼ë¯¸í„°:**
- Seed: {mesh_settings['seed']}
- Steps: {mesh_settings['stage1_inference_steps']}/{mesh_settings['stage2_inference_steps']}
- í›„ì²˜ë¦¬: {'âœ“' if mesh_settings['with_mesh_postprocess'] else 'âœ—'}

{mesh_list}
"""
            return str(output_dir), status
        else:
            return None, "3D Mesh ìƒì„± ì‹¤íŒ¨ (ëª¨ë“  ë¹„ë””ì˜¤)"

    def _update_session_mesh_metadata(self, mesh_info: dict) -> bool:
        """
        ì„¸ì…˜ ë©”íƒ€ë°ì´í„°ì— 3D mesh ì •ë³´ ì¶”ê°€/ì—…ë°ì´íŠ¸

        Args:
            mesh_info: mesh ì •ë³´ ë”•ì…”ë„ˆë¦¬ {video_name, frame_idx, filename, settings, ...}

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            if not self.current_session_path:
                logger.warning("í˜„ì¬ ì„¸ì…˜ ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - mesh ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ìŠ¤í‚µ")
                return False

            session_dir = Path(self.current_session_path)
            metadata_path = session_dir / "session_metadata.json"

            if not metadata_path.exists():
                logger.warning(f"ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì—†ìŒ: {metadata_path}")
                return False

            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)

            # 3d_meshes í•„ë“œ ì´ˆê¸°í™” (ì—†ìœ¼ë©´)
            if '3d_meshes' not in metadata:
                metadata['3d_meshes'] = []

            # ê¸°ì¡´ í•­ëª© ì¤‘ë³µ í™•ì¸ (ê°™ì€ ë¹„ë””ì˜¤/í”„ë ˆì„ì´ë©´ ì—…ë°ì´íŠ¸)
            updated = False
            for i, existing in enumerate(metadata['3d_meshes']):
                if (existing.get('video_name') == mesh_info.get('video_name') and
                    existing.get('frame_idx') == mesh_info.get('frame_idx')):
                    metadata['3d_meshes'][i] = mesh_info
                    updated = True
                    break

            if not updated:
                metadata['3d_meshes'].append(mesh_info)

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)

            logger.info(f"ì„¸ì…˜ ë©”íƒ€ë°ì´í„°ì— mesh ì •ë³´ {'ì—…ë°ì´íŠ¸' if updated else 'ì¶”ê°€'}: {mesh_info.get('filename')}")
            return True

        except Exception as e:
            logger.error(f"ì„¸ì…˜ mesh ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False

    def _get_session_mesh_dir(self) -> Optional[Path]:
        """
        í˜„ì¬ ì„¸ì…˜ì˜ 3D mesh ì €ì¥ ë””ë ‰í† ë¦¬ ë°˜í™˜
        ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ë°˜í™˜
        """
        if self.current_session_path:
            session_dir = Path(self.current_session_path)
            mesh_dir = session_dir / "3d_meshes"
            mesh_dir.mkdir(parents=True, exist_ok=True)
            return mesh_dir
        else:
            # ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
            project_root = Path(__file__).parent.parent
            mesh_dir = project_root / "outputs" / "3d_meshes"
            mesh_dir.mkdir(parents=True, exist_ok=True)
            return mesh_dir

    def save_annotation_session(self, session_name: str = "", save_as_new: bool = False) -> str:
        """
        Annotation ì„¸ì…˜ ì „ì²´ ì €ì¥ (annotation points + masks + metadata)

        Args:
            session_name: ì„¸ì…˜ ì´ë¦„ (ë¹„ì–´ìˆìœ¼ë©´ timestamp ì‚¬ìš©)
            save_as_new: Trueë©´ í•­ìƒ ìƒˆ ì„¸ì…˜ ìƒì„±, Falseë©´ ê¸°ì¡´ ì„¸ì…˜ ë®ì–´ì“°ê¸° ì‹œë„
        """
        print("\n" + "="*80)
        print("ğŸ”¹ save_annotation_session() ì‹œì‘")
        print("="*80)

        if len(self.frames) == 0:
            print("âŒ ì €ì¥ ì‹¤íŒ¨: í”„ë ˆì„ ì—†ìŒ")
            return "ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"

        print(f"âœ“ í”„ë ˆì„ ìˆ˜: {len(self.frames)}")
        print(f"âœ“ ë§ˆìŠ¤í¬ ìˆ˜: {len(self.masks)}")
        print(f"âœ“ Foreground points: {len(self.annotations['foreground'])}")
        print(f"âœ“ Background points: {len(self.annotations['background'])}")

        try:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            # ë®ì–´ì“°ê¸° vs ìƒˆë¡œ ì €ì¥ ê²°ì •
            if not save_as_new and self.current_session_path and Path(self.current_session_path).exists():
                # ê¸°ì¡´ ì„¸ì…˜ ë®ì–´ì“°ê¸°
                output_dir = Path(self.current_session_path)
                session_id = output_dir.name
                print(f"âœ“ ê¸°ì¡´ ì„¸ì…˜ ë®ì–´ì“°ê¸°: {session_id}")
            else:
                # ìƒˆ ì„¸ì…˜ ìƒì„±
                if session_name and session_name.strip():
                    session_id = f"{session_name.strip()}_{timestamp}"
                else:
                    session_id = timestamp
                output_dir = Path(f"outputs/sessions/{session_id}")
                print(f"âœ“ ìƒˆ ì„¸ì…˜ ID ìƒì„±: {session_id}")

            output_dir.mkdir(parents=True, exist_ok=True)
            print(f"âœ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")

            # 1. Annotation ë©”íƒ€ë°ì´í„° ì €ì¥ (JSON)
            print("\nğŸ”¹ Step 1: ë©”íƒ€ë°ì´í„° êµ¬ì„± ì¤‘...")

            # Stride ê³„ì‚° (stride_frame_mappingì´ ìˆìœ¼ë©´ ì¶”ì¶œ, ì—†ìœ¼ë©´ 1ë¡œ ê°€ì •)
            effective_stride = 1
            if hasattr(self, 'stride_frame_mapping') and self.stride_frame_mapping:
                frame_indices = sorted(self.stride_frame_mapping.values())
                if len(frame_indices) > 1:
                    effective_stride = frame_indices[1] - frame_indices[0]

            num_frames_saved = sum(1 for m in self.masks if m is not None)

            metadata = {
                "session_id": session_id,
                "session_type": "interactive",  # For scan_aug_sessions to find it
                "video_path": self.video_path,
                "num_frames": num_frames_saved,  # Load í•¨ìˆ˜ê°€ ì°¾ëŠ” í‚¤
                "num_frames_total": len(self.frames),
                "num_frames_saved": num_frames_saved,
                "stride": effective_stride,
                "current_frame_idx": self.current_frame_idx,
                "annotations": {
                    "foreground": self.annotations['foreground'],
                    "background": self.annotations['background']
                },
                "frame_info": []
            }
            print(f"âœ“ ë©”íƒ€ë°ì´í„° êµ¬ì„± ì™„ë£Œ (stride={effective_stride})")

            # 2. ê° í”„ë ˆì„ ì €ì¥ (stride ê°„ê²©ì˜ í”„ë ˆì„ë§Œ)
            print("\nğŸ”¹ Step 2: í”„ë ˆì„ë³„ ì €ì¥ ì‹œì‘ (ë§ˆìŠ¤í¬ê°€ ìˆëŠ” í”„ë ˆì„ë§Œ)...")
            saved_masks = 0
            saved_frame_idx = 0
            for i, (frame, mask) in enumerate(zip(self.frames, self.masks)):
                # ë§ˆìŠ¤í¬ê°€ ì—†ëŠ” í”„ë ˆì„ì€ ê±´ë„ˆë›°ê¸° (strideë¡œ ìƒëµëœ í”„ë ˆì„)
                if mask is None:
                    continue

                if saved_frame_idx % 10 == 0:  # 10í”„ë ˆì„ë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                    print(f"  ì§„í–‰: {saved_frame_idx} í”„ë ˆì„ ì €ì¥ë¨ (ì›ë³¸ ì¸ë±ìŠ¤: {i}/{len(self.frames)})...")

                frame_dir = output_dir / f"frame_{saved_frame_idx:04d}"
                frame_dir.mkdir(exist_ok=True)

                # ì›ë³¸ í”„ë ˆì„ ì €ì¥
                try:
                    frame_path = frame_dir / "original.png"
                    success = cv2.imwrite(str(frame_path), frame)
                    if not success:
                        print(f"  âš ï¸ í”„ë ˆì„ {i} ì €ì¥ ì‹¤íŒ¨: {frame_path}")
                except Exception as e:
                    print(f"  âŒ í”„ë ˆì„ {i} ì €ì¥ ì˜¤ë¥˜: {str(e)}")
                    raise

                # ë§ˆìŠ¤í¬ ì €ì¥ (ì´ë¯¸ mask is not None ì²´í¬ë¡œ ë“¤ì–´ì˜´)
                try:
                    mask_path = frame_dir / "mask.png"
                    mask_uint8 = mask.astype(np.uint8) * 255
                    success = cv2.imwrite(str(mask_path), mask_uint8)
                    if not success:
                        print(f"  âš ï¸ ë§ˆìŠ¤í¬ {i} ì €ì¥ ì‹¤íŒ¨: {mask_path}")

                    # ì‹œê°í™” (ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´) ì €ì¥
                    vis_path = frame_dir / "visualization.png"
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    overlay = frame_rgb.copy()
                    overlay[mask > 0] = [0, 255, 0]
                    result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)

                    # Annotation points í‘œì‹œ
                    for px, py in self.annotations['foreground']:
                        cv2.circle(result, (px, py), 5, (0, 255, 0), -1)
                        cv2.circle(result, (px, py), 7, (255, 255, 255), 2)
                    for px, py in self.annotations['background']:
                        cv2.circle(result, (px, py), 5, (255, 0, 0), -1)
                        cv2.circle(result, (px, py), 7, (255, 255, 255), 2)

                    result_bgr = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)
                    success = cv2.imwrite(str(vis_path), result_bgr)
                    if not success:
                        print(f"  âš ï¸ ì‹œê°í™” {i} ì €ì¥ ì‹¤íŒ¨: {vis_path}")

                    saved_masks += 1

                    # í”„ë ˆì„ ë©”íƒ€ë°ì´í„° (ì›ë³¸ ì¸ë±ìŠ¤ì™€ ì €ì¥ ì¸ë±ìŠ¤ ëª¨ë‘ ê¸°ë¡)
                    mask_area = np.sum(mask > 0)
                    metadata["frame_info"].append({
                        "saved_frame_idx": saved_frame_idx,  # Fauna í˜•ì‹ ì¸ë±ìŠ¤
                        "original_frame_idx": i,  # ì›ë³¸ ë¹„ë””ì˜¤ ì¸ë±ìŠ¤
                        "has_mask": True,
                        "mask_area": int(mask_area),
                        "mask_percentage": float(mask_area / mask.size * 100)
                    })

                    saved_frame_idx += 1

                except Exception as e:
                    print(f"  âŒ ë§ˆìŠ¤í¬ {i} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                    raise

            # ë§ˆìŠ¤í¬ê°€ ì—†ëŠ” í”„ë ˆì„ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°ëŠ” ë” ì´ìƒ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            # (strideë¡œ ìƒëµëœ í”„ë ˆì„)

            print(f"âœ“ í”„ë ˆì„ë³„ ì €ì¥ ì™„ë£Œ: ì›ë³¸ {len(self.frames)}ê°œ ì¤‘ ë§ˆìŠ¤í¬ê°€ ìˆëŠ” {saved_masks}ê°œë§Œ ì €ì¥")

            # 3. Metadata JSON ì €ì¥
            print("\nğŸ”¹ Step 3: ë©”íƒ€ë°ì´í„° JSON ì €ì¥ ì¤‘...")
            metadata_path = output_dir / "session_metadata.json"
            try:
                with open(metadata_path, 'w') as f:
                    json.dump(metadata, f, indent=2)
                print(f"âœ“ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_path}")
            except Exception as e:
                print(f"âŒ ë©”íƒ€ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {str(e)}")
                raise

            # í˜„ì¬ ì„¸ì…˜ ê²½ë¡œ ì—…ë°ì´íŠ¸ (ë‹¤ìŒ ì €ì¥ ì‹œ ë®ì–´ì“°ê¸°ìš©)
            self.current_session_path = str(output_dir)

            print("\n" + "="*80)
            print("âœ… save_annotation_session() ì™„ë£Œ!")
            print("="*80 + "\n")

            return f"""
### Annotation ì„¸ì…˜ ì €ì¥ ì™„ë£Œ âœ…

**ì„¸ì…˜ ID**: `{session_id}`

**ì €ì¥ ë‚´ìš©**:
- ğŸ“ ì›ë³¸ í”„ë ˆì„: {len(self.frames)}ê°œ
- ğŸ­ ë§ˆìŠ¤í¬: {saved_masks}ê°œ
- ğŸ“ Annotation points: {len(self.annotations['foreground'])} foreground, {len(self.annotations['background'])} background
- ğŸ“‹ ë©”íƒ€ë°ì´í„°: session_metadata.json

**ì €ì¥ ìœ„ì¹˜**: `{output_dir}/`

**ë””ë ‰í† ë¦¬ êµ¬ì¡°**:
```
{session_id}/
â”œâ”€â”€ session_metadata.json
â”œâ”€â”€ frame_0000/
â”‚   â”œâ”€â”€ original.png
â”‚   â”œâ”€â”€ mask.png
â”‚   â””â”€â”€ visualization.png
â”œâ”€â”€ frame_0001/
â”‚   â””â”€â”€ ...
```

**ì„¸ì…˜ ì¬ë¡œë“œ**: ì´ session_idë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚˜ì¤‘ì— ì¬ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print("\n" + "="*80)
            print("âŒ save_annotation_session() ì‹¤íŒ¨!")
            print("="*80)
            print(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            print(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
            print("\nì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            print(error_detail)
            print("="*80 + "\n")
            return f"ì €ì¥ ì˜¤ë¥˜: {str(e)}\n\n```\n{error_detail}\n```"

    def load_annotation_session(self, session_id: str) -> Tuple[np.ndarray, str]:
        """
        ì €ì¥ëœ annotation ì„¸ì…˜ ë¡œë“œ (ë‹¨ì¼ ë¹„ë””ì˜¤ ì„¸ì…˜ ë° batch ì„¸ì…˜ ëª¨ë‘ ì§€ì›)
        """
        try:
            session_dir = Path(f"outputs/sessions/{session_id}")
            if not session_dir.exists():
                return None, f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}"

            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = session_dir / "session_metadata.json"
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)

            # ì„¸ì…˜ íƒ€ì… í™•ì¸ (batch vs single)
            session_type = metadata.get("session_type", "single")

            if session_type == "batch":
                # Batch ì„¸ì…˜ ë¡œë“œ
                return self._load_batch_session(session_id, session_dir, metadata)
            else:
                # ë‹¨ì¼ ë¹„ë””ì˜¤ ì„¸ì…˜ ë¡œë“œ
                return self._load_single_session(session_id, session_dir, metadata)

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return None, f"ë¡œë“œ ì˜¤ë¥˜: {str(e)}\n\n```\n{error_detail}\n```"

    def _load_single_session(self, session_id: str, session_dir: Path, metadata: dict) -> Tuple[np.ndarray, str]:
        """ë‹¨ì¼ ë¹„ë””ì˜¤ ì„¸ì…˜ ë¡œë“œ"""
        # í”„ë ˆì„ ë° ë§ˆìŠ¤í¬ ë¡œë“œ
        num_frames = metadata["num_frames"]
        self.frames = []
        self.masks = []

        for i in range(num_frames):
            frame_dir = session_dir / f"frame_{i:04d}"

            # ì›ë³¸ í”„ë ˆì„ ë¡œë“œ (BGRâ†’RGB ë³€í™˜í•˜ì—¬ self.framesëŠ” í•­ìƒ RGBë¡œ ìœ ì§€)
            frame_path = frame_dir / "original.png"
            frame = cv2.imread(str(frame_path))
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            self.frames.append(frame_rgb)

            # ë§ˆìŠ¤í¬ ë¡œë“œ (ìˆìœ¼ë©´)
            mask_path = frame_dir / "mask.png"
            if mask_path.exists():
                mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
                self.masks.append((mask > 0).astype(bool))
            else:
                self.masks.append(None)

        # Annotation points ë³µì›
        self.annotations = {
            'foreground': metadata["annotations"]["foreground"],
            'background': metadata["annotations"]["background"]
        }

        # ë¹„ë””ì˜¤ ê²½ë¡œ ë° í˜„ì¬ í”„ë ˆì„ ì¸ë±ìŠ¤ ë³µì›
        self.video_path = metadata["video_path"]
        self.current_frame_idx = metadata["current_frame_idx"]

        # í˜„ì¬ í”„ë ˆì„ ì‹œê°í™” (self.framesëŠ” ì´ë¯¸ RGB)
        current_frame = self.frames[self.current_frame_idx]
        current_mask = self.masks[self.current_frame_idx]

        frame_rgb = current_frame.copy()  # ì´ë¯¸ RGBì´ë¯€ë¡œ ë³€í™˜ ë¶ˆí•„ìš”
        if current_mask is not None:
            overlay = frame_rgb.copy()
            overlay[current_mask > 0] = [0, 255, 0]
            result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)
        else:
            result = frame_rgb

        # Annotation points í‘œì‹œ
        for px, py in self.annotations['foreground']:
            cv2.circle(result, (px, py), 5, (0, 255, 0), -1)
            cv2.circle(result, (px, py), 7, (255, 255, 255), 2)
        for px, py in self.annotations['background']:
            cv2.circle(result, (px, py), 5, (255, 0, 0), -1)
            cv2.circle(result, (px, py), 7, (255, 255, 255), 2)

        masks_loaded = sum(1 for m in self.masks if m is not None)

        status = f"""
### ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ âœ…

**ì„¸ì…˜ ID**: `{session_id}`

**ë¡œë“œëœ ë°ì´í„°**:
- ğŸ“ í”„ë ˆì„: {len(self.frames)}ê°œ
- ğŸ­ ë§ˆìŠ¤í¬: {masks_loaded}ê°œ
- ğŸ“ Foreground points: {len(self.annotations['foreground'])}ê°œ
- ğŸ“ Background points: {len(self.annotations['background'])}ê°œ
- ğŸ“¹ ë¹„ë””ì˜¤: {self.video_path}

**í˜„ì¬ í”„ë ˆì„**: {self.current_frame_idx + 1} / {len(self.frames)}

ì´ì œ í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜, ì¶”ê°€ annotation, propagation ë“±ì„ ê³„ì†í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

        # í˜„ì¬ ì„¸ì…˜ ê²½ë¡œ ì—…ë°ì´íŠ¸ (ë‹¤ìŒ ì €ì¥ ì‹œ ë®ì–´ì“°ê¸°ìš©)
        self.current_session_path = str(session_dir)

        return result, status

    def _load_batch_session(self, session_id: str, session_dir: Path, metadata: dict) -> Tuple[np.ndarray, str]:
        """
        Batch ì„¸ì…˜ ë¡œë“œ - batch_results êµ¬ì¡°ë¡œ ë³µì›í•˜ì—¬ í€„ë¦¬í‹° ì²´í¬ ë“±ì—ì„œ ì‚¬ìš© ê°€ëŠ¥
        (Quick Modeì—ì„œ ì„¸ì…˜ ë¡œë“œ ì‹œ í˜¸ì¶œë¨)
        """
        # load_batch_sessionì˜ í•µì‹¬ ë¡œì§ ì¬ì‚¬ìš©
        status_msg, _ = self.load_batch_session(str(session_dir))

        # ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ì˜ ì²« ë²ˆì§¸ í”„ë ˆì„ ë¯¸ë¦¬ë³´ê¸° ìƒì„±
        preview_image = None
        video_list = self.get_batch_video_list()

        if video_list and len(video_list) > 0:
            first_video_dir = Path(video_list[0]['result_dir'])
            frame_dirs = sorted([d for d in first_video_dir.iterdir() if d.is_dir() and d.name.startswith('frame_')])

            if frame_dirs:
                first_frame_dir = frame_dirs[0]
                original_path = first_frame_dir / "original.png"
                mask_path = first_frame_dir / "mask.png"

                if original_path.exists():
                    original = cv2.imread(str(original_path))
                    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)

                    if mask_path.exists():
                        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
                        # ì˜¤ë²„ë ˆì´ ìƒì„±
                        overlay = original.copy().astype(np.float32)
                        mask_bool = mask > 127
                        overlay[mask_bool] = overlay[mask_bool] * 0.6 + np.array([0, 255, 0]) * 0.4
                        preview_image = overlay.astype(np.uint8)
                    else:
                        preview_image = original

        # reference_annotationsë¥¼ í˜„ì¬ annotationsì—ë„ ì„¤ì • (Quick Mode í˜¸í™˜)
        if hasattr(self, 'batch_results') and self.batch_results:
            ref_annot = self.batch_results.get('reference_annotations', {'foreground': [], 'background': []})
            self.annotations = {
                'foreground': ref_annot.get('foreground', []),
                'background': ref_annot.get('background', [])
            }

        # í˜„ì¬ ì„¸ì…˜ ê²½ë¡œ ì—…ë°ì´íŠ¸
        self.current_session_path = str(session_dir)

        # ìƒíƒœ ë©”ì‹œì§€ì— Quick Modeìš© ì•ˆë‚´ ì¶”ê°€
        total_videos = len(video_list) if video_list else 0
        total_frames = sum(v['num_frames'] for v in video_list) if video_list else 0

        video_list_str = ""
        for v in video_list[:10]:
            video_list_str += f"\n- **{v['video_name']}**: {v['num_frames']} í”„ë ˆì„"
        if len(video_list) > 10:
            video_list_str += f"\n- ... ì™¸ {len(video_list) - 10}ê°œ ë¹„ë””ì˜¤"

        per_video_count = len(self.per_video_annotations) if hasattr(self, 'per_video_annotations') and self.per_video_annotations else 0

        status = f"""
### ğŸ“‚ Batch ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ âœ…

**ì„¸ì…˜ ID**: `{session_id}`
**ì„¸ì…˜ íƒ€ì…**: Batch ({total_videos}ê°œ ë¹„ë””ì˜¤)

**ë¡œë“œëœ ë°ì´í„°**:
- ğŸ¬ ë¹„ë””ì˜¤ ìˆ˜: {total_videos}ê°œ
- ğŸ“ ì´ í”„ë ˆì„: {total_frames}ê°œ
- ğŸ“ Reference Annotations: FG {len(self.annotations.get('foreground', []))}ê°œ, BG {len(self.annotations.get('background', []))}ê°œ
- ğŸ¯ Per-video Annotations: {per_video_count}ê°œ ë¹„ë””ì˜¤

**ë¹„ë””ì˜¤ ëª©ë¡**:{video_list_str}

---

### ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:
1. **Batch Mode íƒ­**ì—ì„œ **ê²°ê³¼ ì‹œê°í™” & í€„ë¦¬í‹° ì²´í¬** - ë¹„ë””ì˜¤ë³„ ë§ˆìŠ¤í¬ í™•ì¸
2. **Export to Fauna** - ë°ì´í„°ì…‹ ë‚´ë³´ë‚´ê¸°
3. **í”„ë¦¬ë·° ë¹„ë””ì˜¤ ìƒì„±** - ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´ ì˜ìƒ

> ğŸ’¡ **Tip**: Batch Mode íƒ­ìœ¼ë¡œ ì´ë™í•˜ì—¬ ë¹„ë””ì˜¤ ëª©ë¡ì„ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.
"""

        return preview_image, status

    def get_session_ids(self) -> List[str]:
        """ì €ì¥ëœ ì„¸ì…˜ ID ëª©ë¡ ë°˜í™˜ (Dropdownìš©)"""
        try:
            sessions_dir = Path("outputs/sessions")
            if not sessions_dir.exists():
                return []

            sessions = sorted([d.name for d in sessions_dir.iterdir() if d.is_dir()], reverse=True)
            return sessions

        except Exception as e:
            print(f"ì„¸ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []

    def delete_session(self, session_id: str) -> Tuple[str, List[str]]:
        """
        ì„¸ì…˜ ì‚­ì œ

        Args:
            session_id: ì‚­ì œí•  ì„¸ì…˜ ID

        Returns:
            (ìƒíƒœ ë©”ì‹œì§€, ì—…ë°ì´íŠ¸ëœ ì„¸ì…˜ ëª©ë¡)
        """
        import shutil

        if not session_id:
            return "âš ï¸ ì‚­ì œí•  ì„¸ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", self.get_session_ids()

        try:
            session_dir = Path("outputs/sessions") / session_id

            if not session_dir.exists():
                return f"âŒ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}", self.get_session_ids()

            # í˜„ì¬ ë¡œë“œëœ ì„¸ì…˜ì¸ì§€ í™•ì¸
            if self.current_session_path and Path(self.current_session_path) == session_dir:
                self.current_session_path = None

            # ì„¸ì…˜ í´ë” ì‚­ì œ
            shutil.rmtree(session_dir)

            return f"âœ… ì„¸ì…˜ ì‚­ì œ ì™„ë£Œ: `{session_id}`", self.get_session_ids()

        except Exception as e:
            import traceback
            return f"âŒ ì‚­ì œ ì‹¤íŒ¨: {str(e)}\n{traceback.format_exc()}", self.get_session_ids()

    def rename_session(self, session_id: str, new_name: str) -> Tuple[str, List[str], str]:
        """
        ì„¸ì…˜ ì´ë¦„ ë³€ê²½

        Args:
            session_id: ë³€ê²½í•  ì„¸ì…˜ ID
            new_name: ìƒˆ ì´ë¦„

        Returns:
            (ìƒíƒœ ë©”ì‹œì§€, ì—…ë°ì´íŠ¸ëœ ì„¸ì…˜ ëª©ë¡, ìƒˆ ì„¸ì…˜ ID)
        """
        if not session_id:
            return "âš ï¸ ë³€ê²½í•  ì„¸ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", self.get_session_ids(), None

        if not new_name or not new_name.strip():
            return "âš ï¸ ìƒˆ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”", self.get_session_ids(), session_id

        new_name = new_name.strip()

        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (íŒŒì¼ì‹œìŠ¤í…œ ì•ˆì „)
        import re
        safe_name = re.sub(r'[<>:"/\\|?*]', '_', new_name)

        try:
            sessions_dir = Path("outputs/sessions")
            old_path = sessions_dir / session_id
            new_path = sessions_dir / safe_name

            if not old_path.exists():
                return f"âŒ ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}", self.get_session_ids(), None

            if new_path.exists():
                return f"âŒ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì´ë¦„ì…ë‹ˆë‹¤: {safe_name}", self.get_session_ids(), session_id

            # í´ë” ì´ë¦„ ë³€ê²½
            old_path.rename(new_path)

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata_path = new_path / "session_metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                metadata['session_id'] = safe_name
                with open(metadata_path, 'w') as f:
                    json.dump(metadata, f, indent=2)

            # í˜„ì¬ ë¡œë“œëœ ì„¸ì…˜ ê²½ë¡œ ì—…ë°ì´íŠ¸
            if self.current_session_path and Path(self.current_session_path) == old_path:
                self.current_session_path = str(new_path)

            return f"âœ… ì´ë¦„ ë³€ê²½ ì™„ë£Œ: `{session_id}` â†’ `{safe_name}`", self.get_session_ids(), safe_name

        except Exception as e:
            import traceback
            return f"âŒ ì´ë¦„ ë³€ê²½ ì‹¤íŒ¨: {str(e)}\n{traceback.format_exc()}", self.get_session_ids(), session_id

    def list_saved_sessions(self) -> str:
        """ì €ì¥ëœ ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ"""
        try:
            sessions_dir = Path("outputs/sessions")
            if not sessions_dir.exists():
                return "ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤"

            sessions = sorted([d.name for d in sessions_dir.iterdir() if d.is_dir()])

            if not sessions:
                return "ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤"

            result = "### ì €ì¥ëœ Annotation ì„¸ì…˜ ëª©ë¡\n\n"
            for session_id in sessions:
                metadata_path = sessions_dir / session_id / "session_metadata.json"
                if metadata_path.exists():
                    with open(metadata_path, 'r') as f:
                        metadata = json.load(f)
                    num_frames = metadata["num_frames"]
                    num_masks = sum(1 for info in metadata["frame_info"] if info.get("has_mask", False))
                    video_path = Path(metadata["video_path"]).name if metadata.get("video_path") else "Unknown"

                    result += f"""
**{session_id}**
- ë¹„ë””ì˜¤: `{video_path}`
- í”„ë ˆì„: {num_frames}ê°œ
- ë§ˆìŠ¤í¬: {num_masks}ê°œ
- ê²½ë¡œ: `outputs/sessions/{session_id}/`

---
"""

            return result

        except Exception as e:
            return f"ì˜¤ë¥˜: {str(e)}"

    def save_masks(self) -> str:
        """ë§ˆìŠ¤í¬ë§Œ ê°„ë‹¨íˆ ì €ì¥ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        if all(m is None for m in self.masks):
            return "ì €ì¥í•  ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤"

        try:
            output_dir = Path("outputs/masks")
            output_dir.mkdir(parents=True, exist_ok=True)

            saved_count = 0
            for i, mask in enumerate(self.masks):
                if mask is not None:
                    output_path = output_dir / f"mask_{i:04d}.png"
                    cv2.imwrite(str(output_path), mask.astype(np.uint8) * 255)
                    saved_count += 1

            return f"""
### ë§ˆìŠ¤í¬ ì €ì¥ ì™„ë£Œ âœ…

- **ì €ì¥ëœ ë§ˆìŠ¤í¬**: {saved_count} / {len(self.masks)}
- **ì €ì¥ ìœ„ì¹˜**: `{output_dir}/`

**ì°¸ê³ **: ì „ì²´ ì„¸ì…˜(annotation + masks + metadata)ì„ ì €ì¥í•˜ë ¤ë©´ **"ğŸ’¾ Save Session"** ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.
"""

        except Exception as e:
            return f"ì˜¤ë¥˜: {str(e)}"

    def navigate_frame(self, direction: str, step: int = 1) -> Tuple[np.ndarray, str]:
        """
        í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜

        Args:
            direction: "prev", "next", "first", "last", "goto"
            step: ì´ë™í•  í”„ë ˆì„ ìˆ˜
        """
        if len(self.frames) == 0:
            return None, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

        old_idx = self.current_frame_idx

        if direction == "prev":
            self.current_frame_idx = max(0, self.current_frame_idx - step)
        elif direction == "next":
            self.current_frame_idx = min(len(self.frames) - 1, self.current_frame_idx + step)
        elif direction == "first":
            self.current_frame_idx = 0
        elif direction == "last":
            self.current_frame_idx = len(self.frames) - 1
        elif direction == "goto":
            # stepì€ ì‹¤ì œ í”„ë ˆì„ ë²ˆí˜¸ (0-indexed)
            self.current_frame_idx = max(0, min(len(self.frames) - 1, step))

        # í˜„ì¬ í”„ë ˆì„ ì‹œê°í™”
        frame = self.frames[self.current_frame_idx]
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # ë§ˆìŠ¤í¬ê°€ ìˆìœ¼ë©´ í‘œì‹œ
        mask = self.masks[self.current_frame_idx]
        if mask is not None:
            overlay = frame_rgb.copy()
            overlay[mask > 0] = [0, 255, 0]
            result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)

            mask_area = np.sum(mask > 0)
            mask_pct = mask_area / mask.size * 100
            mask_info = f"ë§ˆìŠ¤í¬: {mask_area} í”½ì…€ ({mask_pct:.1f}%)"
        else:
            result = frame_rgb
            mask_info = "ë§ˆìŠ¤í¬ ì—†ìŒ"

        # Points í‘œì‹œ (annotationì´ ìˆìœ¼ë©´)
        if len(self.annotations['foreground']) > 0 or len(self.annotations['background']) > 0:
            for px, py in self.annotations['foreground']:
                cv2.circle(result, (px, py), 5, (0, 255, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)
            for px, py in self.annotations['background']:
                cv2.circle(result, (px, py), 5, (255, 0, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)

        status = f"""
### í”„ë ˆì„ {self.current_frame_idx + 1} / {len(self.frames)}

- **ì´ë™**: {old_idx + 1} â†’ {self.current_frame_idx + 1}
- **{mask_info}**
"""

        return result, status

    def clear_annotations(self) -> Tuple[np.ndarray, str]:
        """
        ëª¨ë“  annotation pointsì™€ masks ì´ˆê¸°í™”

        Returns:
            í˜„ì¬ í”„ë ˆì„ ì´ë¯¸ì§€, ìƒíƒœ ë©”ì‹œì§€
        """
        # Annotations ì´ˆê¸°í™”
        self.annotations['foreground'] = []
        self.annotations['background'] = []

        # Masks ì´ˆê¸°í™”
        self.masks = [None] * len(self.frames) if self.frames else []
        self.current_mask = None

        # í˜„ì¬ í”„ë ˆì„ ì´ë¯¸ì§€ ë°˜í™˜ (annotation ì—†ì´)
        if len(self.frames) > 0:
            current_frame = self.frames[self.current_frame_idx]
            frame_rgb = cv2.cvtColor(current_frame, cv2.COLOR_BGR2RGB)

            status = """
### Annotations ì´ˆê¸°í™” ì™„ë£Œ âœ…

- ëª¨ë“  foreground/background points ì œê±°
- ëª¨ë“  ë§ˆìŠ¤í¬ ì´ˆê¸°í™”
- ìƒˆë¡œ annotation ì‹œì‘ ê°€ëŠ¥

**ë‹¤ìŒ ë‹¨ê³„**: ì´ë¯¸ì§€ í´ë¦­í•˜ì—¬ ìƒˆë¡œìš´ annotation ì‹œì‘
"""
            return frame_rgb, status
        else:
            return None, "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

    def _find_next_sequence(self, fauna_root: Path, animal_name: str) -> str:
        """
        ë‹¤ìŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ ë²ˆí˜¸ ì°¾ê¸°

        Args:
            fauna_root: Fauna ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ
            animal_name: ë™ë¬¼ ì´ë¦„

        Returns:
            "seq_XXX" í˜•ì‹ì˜ ì‹œí€€ìŠ¤ ì´ë¦„
        """
        train_dir = fauna_root / animal_name / "train"

        # train ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ seq_000ë¶€í„° ì‹œì‘
        if not train_dir.exists():
            return "seq_000"

        # ê¸°ì¡´ seq_XXX ë””ë ‰í† ë¦¬ ì°¾ê¸°
        existing_sequences = [
            d.name for d in train_dir.iterdir()
            if d.is_dir() and d.name.startswith("seq_")
        ]

        # ê¸°ì¡´ ì‹œí€€ìŠ¤ê°€ ì—†ìœ¼ë©´ seq_000
        if not existing_sequences:
            return "seq_000"

        # ê°€ì¥ í° ì‹œí€€ìŠ¤ ë²ˆí˜¸ ì°¾ê¸°
        try:
            max_seq_num = max([int(s.split("_")[1]) for s in existing_sequences])
            next_seq_num = max_seq_num + 1
            return f"seq_{next_seq_num:03d}"
        except (IndexError, ValueError):
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ seq_000 ë°˜í™˜
            return "seq_000"

    def export_fauna_dataset(
        self,
        animal_name: str = "mouse",
        target_frames: int = 50,
        progress=gr.Progress()
    ) -> str:
        """
        Fauna ë°ì´í„°ì…‹ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§: ì „ì²´ ë¹„ë””ì˜¤ì—ì„œ target_frames ê°œë§Œ ê· ë“± ê°„ê²©ìœ¼ë¡œ ì„ íƒ
        ìë™ ì‹œí€€ìŠ¤ ë²ˆí˜¸ í• ë‹¹: ê¸°ì¡´ ì‹œí€€ìŠ¤ë¥¼ ë®ì–´ì“°ì§€ ì•ŠìŒ

        Args:
            animal_name: ë™ë¬¼ ì´ë¦„ (í´ë”ëª…)
            target_frames: ì €ì¥í•  í”„ë ˆì„ ìˆ˜ (ê¸°ë³¸ 50ê°œ)

        Returns:
            ìƒíƒœ ë©”ì‹œì§€
        """
        if len(self.frames) == 0:
            return "âŒ ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        if all(m is None for m in self.masks):
            return "âŒ ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Propagateë¥¼ ì‹¤í–‰í•˜ì„¸ìš”"

        try:
            from datetime import datetime

            progress(0, desc="Fauna ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")

            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì • - session_id ê¸°ë°˜ìœ¼ë¡œ ì €ì¥
            project_root = Path(__file__).parent.parent
            fauna_root = project_root / "outputs" / "fauna_datasets"

            # session_id ê²°ì •: current_session_pathê°€ ìˆìœ¼ë©´ í•´ë‹¹ ID ì‚¬ìš©, ì—†ìœ¼ë©´ timestamp ìƒì„±
            if self.current_session_path:
                # ê¸°ì¡´ ì„¸ì…˜ ID ì‚¬ìš© (í´ë” ì´ë¦„ì—ì„œ ì¶”ì¶œ)
                sequence_name = Path(self.current_session_path).name
                print(f"   Using existing session ID: {sequence_name}")
            else:
                # ì„¸ì…˜ì´ ì €ì¥ë˜ì§€ ì•Šì€ ê²½ìš° timestamp ê¸°ë°˜ ID ìƒì„±
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                sequence_name = f"unsaved_{timestamp}"
                print(f"   Generated new session ID: {sequence_name}")

            output_dir = fauna_root / animal_name / "train" / sequence_name
            output_dir.mkdir(parents=True, exist_ok=True)

            print(f"\nğŸ”¹ Fauna ë°ì´í„°ì…‹ ì €ì¥:")
            print(f"   Animal: {animal_name}")
            print(f"   Sequence: {sequence_name}")
            print(f"   Path: {output_dir}")

            # ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§: target_framesê°œë¥¼ ê· ë“± ê°„ê²©ìœ¼ë¡œ ì„ íƒ
            total_frames = len(self.frames)
            if total_frames <= target_frames:
                # í”„ë ˆì„ ìˆ˜ê°€ ì ìœ¼ë©´ ì „ë¶€ ì‚¬ìš©
                selected_indices = list(range(total_frames))
            else:
                # ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                step = total_frames / target_frames
                selected_indices = [int(i * step) for i in range(target_frames)]

            progress(0.1, desc=f"{len(selected_indices)}ê°œ í”„ë ˆì„ ì„ íƒë¨ (ì „ì²´ {total_frames}ê°œ ì¤‘)...")

            # í”„ë ˆì„ ë° ë§ˆìŠ¤í¬ ì €ì¥
            saved_count = 0
            for idx, frame_idx in enumerate(selected_indices):
                if self.masks[frame_idx] is None:
                    continue

                frame = self.frames[frame_idx]
                mask = self.masks[frame_idx]

                # Fauna í˜•ì‹: {index:07d}_rgb.png, {index:07d}_mask.png
                rgb_path = output_dir / f"{idx:07d}_rgb.png"
                mask_path = output_dir / f"{idx:07d}_mask.png"

                # RGB ì €ì¥ (BGR â†’ RGB)
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                cv2.imwrite(str(rgb_path), cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR))

                # Mask ì €ì¥ (0-255 í˜•ì‹)
                mask_uint8 = (mask * 255).astype(np.uint8)
                cv2.imwrite(str(mask_path), mask_uint8)

                saved_count += 1
                progress(0.1 + 0.8 * (idx + 1) / len(selected_indices),
                        desc=f"ì €ì¥ ì¤‘... {idx+1}/{len(selected_indices)}")

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "animal_name": animal_name,
                "sequence": sequence_name,
                "split": "train",
                "total_frames": saved_count,
                "original_video_frames": total_frames,
                "sampling_strategy": "uniform" if total_frames > target_frames else "all",
                "annotations": {
                    "foreground_points": len(self.annotations['foreground']),
                    "background_points": len(self.annotations['background'])
                },
                "export_date": datetime.now().isoformat(),
                "source_video": str(self.video_path) if self.video_path else None
            }

            metadata_path = output_dir / "metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            progress(1.0, desc="Fauna ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")

            return f"""
### Fauna ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ âœ…

**ì €ì¥ ìœ„ì¹˜**: `{output_dir}`
**ì‹œí€€ìŠ¤**: `{sequence_name}` (ìë™ í• ë‹¹ - ê¸°ì¡´ ë°ì´í„° ë³´ì¡´)

**ë°ì´í„°ì…‹ êµ¬ì¡°**:
```
{animal_name}/train/{sequence_name}/
â”œâ”€â”€ 0000000_rgb.png
â”œâ”€â”€ 0000000_mask.png
â”œâ”€â”€ 0000001_rgb.png
â”œâ”€â”€ 0000001_mask.png
...
â”œâ”€â”€ {saved_count-1:07d}_rgb.png
â”œâ”€â”€ {saved_count-1:07d}_mask.png
â””â”€â”€ metadata.json
```

**í†µê³„**:
- ì €ì¥ëœ í”„ë ˆì„: {saved_count}ê°œ
- ì›ë³¸ ë¹„ë””ì˜¤: {total_frames} í”„ë ˆì„
- ìƒ˜í”Œë§: {"ê· ë“± ê°„ê²© " + str(target_frames) + "ê°œ" if total_frames > target_frames else "ì „ì²´ ì‚¬ìš©"}

**ë‹¤ìŒ ë‹¨ê³„**:
1. ë°ì´í„° ê²€ì¦: `ls {output_dir} | head -20`
2. 3DAnimals í•™ìŠµ ì‹¤í–‰
3. ê²°ê³¼ í™•ì¸ ë° ì‹œê°í™”

**Config ì„¤ì • ì˜ˆì‹œ**:
```yaml
dataset:
  name: {animal_name}
  path: data/fauna/Fauna_dataset/large_scale/{animal_name}
  split: train
```
"""

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âŒ Fauna ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {str(e)}\n\n```\n{error_detail}\n```"

    def export_frames_and_masks(self, output_dir: str = None, progress=gr.Progress()) -> str:
        """
        í”„ë ˆì„ë³„ë¡œ ì›ë³¸ ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ë¥¼ ë³„ë„ í´ë”ì— ì €ì¥

        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ìë™ ìƒì„±)

        Returns:
            ìƒíƒœ ë©”ì‹œì§€
        """
        if len(self.frames) == 0:
            return "âŒ ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        if all(m is None for m in self.masks):
            return "âŒ ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Segment ë˜ëŠ” Propagateë¥¼ ì‹¤í–‰í•˜ì„¸ìš”"

        try:
            progress(0, desc="ì €ì¥ ì¤€ë¹„ ì¤‘...")

            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            if output_dir is None:
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_dir = Path(self.config.output_dir if self.config else "outputs") / f"frames_export_{timestamp}"
            else:
                output_dir = Path(output_dir)

            output_dir.mkdir(parents=True, exist_ok=True)

            # ì„œë¸Œë””ë ‰í† ë¦¬ ìƒì„±
            images_dir = output_dir / "images"
            masks_dir = output_dir / "masks"
            overlays_dir = output_dir / "overlays"

            images_dir.mkdir(exist_ok=True)
            masks_dir.mkdir(exist_ok=True)
            overlays_dir.mkdir(exist_ok=True)

            progress(0.1, desc="í”„ë ˆì„ ì €ì¥ ì¤‘...")

            saved_count = 0
            for i, frame in enumerate(self.frames):
                # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
                image_path = images_dir / f"frame_{i:05d}.png"
                cv2.imwrite(str(image_path), frame)

                # ë§ˆìŠ¤í¬ ì €ì¥ (ìˆì„ ê²½ìš°)
                if self.masks[i] is not None:
                    mask = self.masks[i]
                    mask_path = masks_dir / f"frame_{i:05d}.png"
                    mask_uint8 = (mask * 255).astype(np.uint8)
                    cv2.imwrite(str(mask_path), mask_uint8)

                    # ì˜¤ë²„ë ˆì´ ì €ì¥ (ì‹œê°í™”)
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    overlay = frame_rgb.copy()
                    overlay[mask > 0] = [0, 255, 0]
                    result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)
                    result_bgr = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)

                    overlay_path = overlays_dir / f"frame_{i:05d}.png"
                    cv2.imwrite(str(overlay_path), result_bgr)

                    saved_count += 1

                progress((i + 1) / len(self.frames), desc=f"ì €ì¥ ì¤‘... {i+1}/{len(self.frames)}")

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "video_path": str(self.video_path) if self.video_path else None,
                "total_frames": len(self.frames),
                "frames_with_masks": saved_count,
                "annotations": {
                    "foreground": self.annotations['foreground'],
                    "background": self.annotations['background']
                },
                "export_date": datetime.now().isoformat()
            }

            metadata_path = output_dir / "metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            progress(1.0, desc="ì €ì¥ ì™„ë£Œ!")

            return f"""
### í”„ë ˆì„/ë§ˆìŠ¤í¬ ì €ì¥ ì™„ë£Œ âœ…

**ì €ì¥ ìœ„ì¹˜**: `{output_dir}`

**ì €ì¥ëœ íŒŒì¼**:
- ğŸ“ **images/**: ì›ë³¸ í”„ë ˆì„ {len(self.frames)}ê°œ
- ğŸ“ **masks/**: ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ {saved_count}ê°œ
- ğŸ“ **overlays/**: ì‹œê°í™” ì´ë¯¸ì§€ {saved_count}ê°œ
- ğŸ“„ **metadata.json**: ë©”íƒ€ë°ì´í„°

**íŒŒì¼ í˜•ì‹**: PNG (ë¬´ì†ì‹¤)

**ë‹¤ìŒ ë‹¨ê³„**:
- ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©
- í•™ìŠµ ë°ì´í„°ì…‹ìœ¼ë¡œ í™œìš©
- ì™¸ë¶€ ë„êµ¬ë¡œ ì¶”ê°€ ë¶„ì„
"""

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}\n\n```\n{error_detail}\n```"

    # ===== Lite Annotator Event Handlers =====

    def _lite_load_source(
        self,
        input_path: str,
        input_type: str,
        pattern: str
    ) -> Tuple[str, gr.Slider, Dict]:
        """Load video or image folder"""
        if self.lite_annotator is None:
            return "âŒ Lite Annotator not initialized", gr.Slider(maximum=100), {}

        success, msg, total_frames = self.lite_annotator.change_input_source(
            input_path, input_type, pattern
        )

        if success:
            # Update slider maximum
            new_slider = gr.Slider(minimum=0, maximum=max(0, total_frames - 1), value=0, step=1)
            info = self.lite_annotator.get_info()
            return msg, new_slider, info
        else:
            return msg, gr.Slider(maximum=100), {}

    def _lite_load_model(self, model_name: str) -> str:
        """Load SAM 2.1 model"""
        if self.lite_annotator is None:
            return "âŒ Lite Annotator not initialized"

        msg = self.lite_annotator.load_model(model_name)
        return msg

    def _lite_load_frame(self, frame_idx: int) -> Tuple[Optional[np.ndarray], str, Dict]:
        """Load frame at index"""
        if self.lite_annotator is None:
            return None, "âŒ Lite Annotator not initialized", {}

        frame, msg = self.lite_annotator.load_frame(int(frame_idx))
        info = self.lite_annotator.get_info()

        return frame, msg, info

    def _lite_add_point(self, evt: gr.SelectData, point_type: str) -> Tuple[np.ndarray, str]:
        """Add point from click event"""
        if self.lite_annotator is None:
            return None, "âŒ Lite Annotator not initialized"

        x, y = evt.index[0], evt.index[1]
        frame, msg = self.lite_annotator.add_point(x, y, point_type)

        return frame, msg

    def _lite_generate_mask(self) -> Tuple[np.ndarray, Optional[np.ndarray], str]:
        """Generate mask from points"""
        if self.lite_annotator is None:
            return None, None, "âŒ Lite Annotator not initialized"

        frame_vis, mask_binary, msg = self.lite_annotator.generate_mask()

        return frame_vis, mask_binary, msg

    def _lite_save_annotation(self) -> str:
        """Save current annotation"""
        if self.lite_annotator is None:
            return "âŒ Lite Annotator not initialized"

        msg = self.lite_annotator.save_annotation()
        return msg

    def _lite_clear_points(self) -> Tuple[np.ndarray, str]:
        """Clear all points"""
        if self.lite_annotator is None:
            return None, "âŒ Lite Annotator not initialized"

        frame, msg = self.lite_annotator.clear_points()
        return frame, msg

    def create_interface(self):
        """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± - í†µí•© ë²„ì „"""

        with gr.Blocks(title="SAM 3D GUI - Unified Interface") as demo:
            gr.Markdown("""
            # ğŸ¬ SAM 3D GUI - í†µí•© ì›¹ ì¸í„°í˜ì´ìŠ¤

            **ì‘ì—… ëª¨ë“œ:**
            - ğŸ¨ **Interactive Mode**: ë‹¨ì¼ ë¹„ë””ì˜¤ ìˆ˜ë™ annotation & propagation
            - ğŸ“¦ **Batch Mode**: ë‹¤ì¤‘ ë¹„ë””ì˜¤ ì¼ê´„ ì²˜ë¦¬ ë° ì„¸ì…˜ ê´€ë¦¬
            - ğŸ“ **Lite Annotator**: íš¨ìœ¨ì  ë‹¨ì¼ í”„ë ˆì„ annotation
            """)

            # ===== SAM2 ëª¨ë¸ ìƒíƒœ (ì»´íŒ©íŠ¸ í•œ ì¤„) =====
            with gr.Row(equal_height=True):
                sam2_status = gr.Markdown(
                    self._get_sam2_status_markdown(),
                    elem_id="sam2-status"
                )
                sam2_download_btn = gr.Button(
                    "ğŸ”„ ë‹¤ìš´ë¡œë“œ/ë¡œë“œ" if self.sam2_predictor is None else "âœ… ë¡œë“œë¨",
                    variant="primary" if self.sam2_predictor is None else "secondary",
                    size="sm",
                    scale=0,
                    min_width=120
                )
                sam2_progress_text = gr.Textbox(
                    value="",
                    show_label=False,
                    scale=1,
                    max_lines=1,
                    placeholder="ì§„í–‰ ìƒíƒœ..."
                )

            def download_and_load_sam2(progress=gr.Progress()):
                """SAM2 ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ë° ëª¨ë¸ ë¡œë“œ"""
                progress(0, desc="SAM2 í™•ì¸ ì¤‘...")

                # ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´
                if self.sam2_predictor is not None and self.sam2_video_predictor is not None:
                    return self._get_sam2_status_markdown(), "âœ… SAM2 ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤."

                if not SAM2_AVAILABLE:
                    return self._get_sam2_status_markdown(), "âŒ SAM2 íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install sam2"

                checkpoint = self.SAM2_CHECKPOINT_PATH
                if self.config:
                    config_checkpoint = Path(self.config.sam2_checkpoint)
                    if config_checkpoint.exists():
                        checkpoint = config_checkpoint

                # ë‹¤ìš´ë¡œë“œ í•„ìš” ì—¬ë¶€
                if not checkpoint.exists():
                    progress(0.1, desc="ğŸ“¥ SAM2 ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 900MB)")

                    def update_progress(p):
                        progress(0.1 + p * 0.7, desc=f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘... {p*100:.0f}%")

                    success, msg = self.download_sam2_checkpoint(update_progress)
                    if not success:
                        return self._get_sam2_status_markdown(), f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {msg}"

                # ëª¨ë¸ ë¡œë“œ
                progress(0.85, desc="ğŸ”„ SAM2 ëª¨ë¸ ë¡œë”© ì¤‘...")
                success, msg = self.load_sam2_models()

                progress(1.0, desc="ì™„ë£Œ!")

                if success:
                    return self._get_sam2_status_markdown(), f"âœ… {msg}"
                else:
                    return self._get_sam2_status_markdown(), f"âŒ {msg}"

            sam2_download_btn.click(
                fn=download_and_load_sam2,
                outputs=[sam2_status, sam2_progress_text]
            )

            # ë¹„ë””ì˜¤ ìë™ ìŠ¤ìº” (Interactive Modeìš©)
            initial_videos = self.scan_videos(self.default_data_dir)
            initial_video = initial_videos[0] if initial_videos else None

            # ì„¸ì…˜ ìë™ ìŠ¤ìº”
            initial_sessions = self.get_session_ids()

            with gr.Tabs():
                # ===== Tab 1: Interactive Mode (ê¸°ë³¸) =====
                with gr.Tab("ğŸ¨ Interactive Mode"):
                    gr.Markdown("### ëŒ€í™”í˜• Annotation & Propagation")

                    with gr.Row():
                        with gr.Column(scale=1):
                            gr.Markdown("### ğŸ“ ë¹„ë””ì˜¤ ë¡œë“œ")

                            data_dir = gr.Textbox(
                                label="ë°ì´í„° ë””ë ‰í† ë¦¬",
                                value=self.default_data_dir
                            )

                            scan_video_btn = gr.Button("ğŸ“‚ ë¹„ë””ì˜¤ ìŠ¤ìº”")

                            video_file = gr.Dropdown(
                                label="ë¹„ë””ì˜¤ íŒŒì¼",
                                choices=initial_videos,
                                value=initial_video,
                                interactive=True
                            )

                            with gr.Row():
                                start_time = gr.Number(label="ì‹œì‘ (ì´ˆ)", value=0.0, minimum=0)
                                duration = gr.Number(label="ê¸¸ì´ (ì´ˆ)", value=3.0, minimum=0.1)

                            load_btn = gr.Button("ğŸ“¹ ë¹„ë””ì˜¤ ë¡œë“œ", variant="primary")

                            gr.Markdown("### ğŸ¯ Annotation")

                            annotation_mode = gr.Radio(
                                label="Point íƒ€ì…",
                                choices=["foreground", "background"],
                                value="foreground"
                            )

                            clear_btn = gr.Button("ğŸ—‘ï¸ Points ì´ˆê¸°í™”")
                            clear_all_btn = gr.Button("ğŸ”„ All Annotations ì´ˆê¸°í™”", variant="stop")
                            segment_btn = gr.Button("âœ‚ï¸ Segment Current Frame", variant="secondary")

                            gr.Markdown("### ğŸ¬ Propagation")

                            with gr.Row():
                                target_frames = gr.Number(
                                    label="ëª©í‘œ í”„ë ˆì„ ìˆ˜",
                                    value=300,
                                    minimum=10,
                                    maximum=1000,
                                    step=10,
                                    info="ì²˜ë¦¬í•  ì´ í”„ë ˆì„ ìˆ˜ (Stride ìë™ ê³„ì‚°)"
                                )
                                auto_stride = gr.Number(
                                    label="ìë™ Stride",
                                    value=10,
                                    interactive=False,
                                    info="ëª©í‘œ í”„ë ˆì„ ìˆ˜ ê¸°ë°˜ ìë™ ê³„ì‚°"
                                )

                            propagate_btn = gr.Button("ğŸ”„ Propagate to All Frames", variant="primary")

                            gr.Markdown("### ğŸï¸ í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜")

                            # í”„ë ˆì„ í”„ë¡œê·¸ë ˆìŠ¤ ë°” (ì§ì ‘ ì´ë™ ê°€ëŠ¥)
                            frame_slider = gr.Slider(
                                label="í”„ë ˆì„ ìœ„ì¹˜",
                                minimum=0,
                                maximum=100,
                                value=0,
                                step=1,
                                interactive=True,
                                info="ìŠ¬ë¼ì´ë”ë¥¼ ë“œë˜ê·¸í•˜ì—¬ í”„ë ˆì„ ì´ë™"
                            )

                            with gr.Row():
                                first_btn = gr.Button("â®ï¸ ì²˜ìŒ", size="sm")
                                prev_btn = gr.Button("â—€ï¸ ì´ì „", size="sm")
                                next_btn = gr.Button("â–¶ï¸ ë‹¤ìŒ", size="sm")
                                last_btn = gr.Button("â­ï¸ ë§ˆì§€ë§‰", size="sm")

                            with gr.Row():
                                frame_step = gr.Slider(
                                    label="ì´ë™ ê°„ê²© (Stride)",
                                    minimum=1,
                                    maximum=100,
                                    value=1,
                                    step=1,
                                    info="Propagate ì‹œì—ë„ ì´ ê°„ê²©ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤"
                                )

                            with gr.Row():
                                goto_frame = gr.Number(
                                    label="í”„ë ˆì„ ë²ˆí˜¸",
                                    value=1,
                                    minimum=1,
                                    step=1,
                                    scale=2
                                )
                                goto_btn = gr.Button("ì´ë™", scale=1)

                            gr.Markdown("### ğŸ’¾ ì„¸ì…˜ ê´€ë¦¬")

                            gr.Markdown("**ì„¸ì…˜ ë¡œë“œ**")
                            with gr.Row():
                                session_refresh_btn = gr.Button("ğŸ”„ ëª©ë¡ ìƒˆë¡œê³ ì¹¨", size="sm")

                            session_id_dropdown = gr.Dropdown(
                                label="ì„¸ì…˜ ì„ íƒ",
                                choices=initial_sessions,
                                value=initial_sessions[0] if initial_sessions else None,
                                interactive=True,
                                scale=2
                            )

                            with gr.Row():
                                load_session_btn = gr.Button("ğŸ“‚ ë¡œë“œ", variant="primary", scale=1)
                                delete_session_btn = gr.Button("ğŸ—‘ï¸ ì‚­ì œ", variant="stop", scale=1)

                            with gr.Accordion("âœï¸ ì„¸ì…˜ ì´ë¦„ ë³€ê²½", open=False):
                                rename_session_input = gr.Textbox(
                                    label="ìƒˆ ì´ë¦„",
                                    placeholder="ìƒˆ ì„¸ì…˜ ì´ë¦„ ì…ë ¥",
                                    info="ì„ íƒí•œ ì„¸ì…˜ì˜ ì´ë¦„ì„ ë³€ê²½í•©ë‹ˆë‹¤"
                                )
                                rename_session_btn = gr.Button("âœï¸ ì´ë¦„ ë³€ê²½", size="sm")

                            gr.Markdown("### ğŸ’¾ ì„¸ì…˜ ì €ì¥")

                            session_name_input = gr.Textbox(
                                label="ì„¸ì…˜ ì´ë¦„ (ìƒˆë¡œ ì €ì¥ ì‹œ)",
                                placeholder="ì˜ˆ: mouse_experiment_1",
                                info="ìƒˆë¡œ ì €ì¥ ì‹œì—ë§Œ ì‚¬ìš© (ë¹„ì–´ìˆìœ¼ë©´ timestamp)"
                            )

                            with gr.Row():
                                save_session_btn = gr.Button("ğŸ’¾ ì €ì¥", variant="secondary", scale=1)
                                save_session_new_btn = gr.Button("ğŸ“ ìƒˆë¡œ ì €ì¥", variant="secondary", scale=1)

                            gr.Markdown("### ğŸ² 3D Mesh ì„¤ì •")

                            with gr.Accordion("âš™ï¸ Mesh íŒŒë¼ë¯¸í„°", open=False):
                                mesh_seed = gr.Number(
                                    label="Seed (ì¬í˜„ì„±)",
                                    value=42,
                                    precision=0,
                                    info="ë™ì¼ seed = ë™ì¼ ê²°ê³¼"
                                )
                                with gr.Row():
                                    mesh_stage1_steps = gr.Slider(
                                        label="Stage1 Steps",
                                        minimum=5,
                                        maximum=50,
                                        value=25,
                                        step=5,
                                        info="Sparse structure í’ˆì§ˆ"
                                    )
                                    mesh_stage2_steps = gr.Slider(
                                        label="Stage2 Steps",
                                        minimum=5,
                                        maximum=50,
                                        value=25,
                                        step=5,
                                        info="Latent feature í’ˆì§ˆ"
                                    )
                                mesh_postprocess = gr.Checkbox(
                                    label="Mesh í›„ì²˜ë¦¬ (ë‹¨ìˆœí™”, í™€ ì±„ìš°ê¸°)",
                                    value=False,
                                    info="í™œì„±í™” ì‹œ ì²˜ë¦¬ ì‹œê°„ ì¦ê°€"
                                )
                                mesh_simplify_ratio = gr.Slider(
                                    label="Simplify Ratio",
                                    minimum=0.5,
                                    maximum=0.99,
                                    value=0.95,
                                    step=0.05,
                                    info="Face ìœ ì§€ ë¹„ìœ¨ (0.95 = 5% ì œê±°)",
                                    visible=False
                                )
                                mesh_texture_baking = gr.Checkbox(
                                    label="Texture Baking",
                                    value=False,
                                    info="í…ìŠ¤ì²˜ ë§µ ìƒì„± (ì¶”ê°€ ì‹œê°„ í•„ìš”)"
                                )
                                mesh_texture_size = gr.Dropdown(
                                    label="Texture Size",
                                    choices=[512, 1024, 2048],
                                    value=1024,
                                    visible=False
                                )
                                mesh_vertex_color = gr.Checkbox(
                                    label="Vertex Color ì‚¬ìš©",
                                    value=True,
                                    info="ë²„í…ìŠ¤ì— ìƒ‰ìƒ ì €ì¥"
                                )

                                # í›„ì²˜ë¦¬ ì²´í¬ë°•ìŠ¤ì— ë”°ë¼ simplify_ratio í‘œì‹œ
                                mesh_postprocess.change(
                                    fn=lambda x: gr.update(visible=x),
                                    inputs=[mesh_postprocess],
                                    outputs=[mesh_simplify_ratio]
                                )
                                # í…ìŠ¤ì²˜ ë² ì´í‚¹ ì²´í¬ë°•ìŠ¤ì— ë”°ë¼ texture_size í‘œì‹œ
                                mesh_texture_baking.change(
                                    fn=lambda x: gr.update(visible=x),
                                    inputs=[mesh_texture_baking],
                                    outputs=[mesh_texture_size]
                                )

                            mesh_btn = gr.Button("ğŸ² Generate 3D Mesh", variant="primary")
                            save_masks_btn = gr.Button("ğŸ’¾ Save Masks Only")
                            export_frames_btn = gr.Button("ğŸ“¤ Export Frames & Masks")

                            gr.Markdown("### ğŸ¦ Fauna ë°ì´í„°ì…‹ ì €ì¥")

                            with gr.Row():
                                fauna_animal_name = gr.Textbox(
                                    label="ë™ë¬¼ ì´ë¦„",
                                    value="mouse",
                                    placeholder="ì˜ˆ: mouse, cat, dog"
                                )
                                fauna_target_frames = gr.Number(
                                    label="ëª©í‘œ í”„ë ˆì„ ìˆ˜",
                                    value=50,
                                    minimum=10,
                                    maximum=500,
                                    step=10
                                )

                            export_fauna_btn = gr.Button("ğŸ¾ Fauna í˜•ì‹ìœ¼ë¡œ ì €ì¥", variant="primary")

                        # ìš°ì¸¡: ì´ë¯¸ì§€ & ê²°ê³¼
                        with gr.Column(scale=2):
                            gr.Markdown("### ğŸ–¼ï¸ Annotation & Results")

                            image_display = gr.Image(
                                label="ì´ë¯¸ì§€ (í´ë¦­í•˜ì—¬ point ì¶”ê°€)",
                                type="numpy",
                                height=500,
                                interactive=True
                            )

                            status_text = gr.Markdown("ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”")

                            mesh_file = gr.File(label="3D Mesh íŒŒì¼")

                    # Interactive Mode ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
                    # ë¹„ë””ì˜¤ íŒŒì¼ ì„ íƒ ì‹œ duration ìë™ ì—…ë°ì´íŠ¸
                    video_file.change(
                        fn=self.get_video_duration,
                        inputs=[data_dir, video_file],
                        outputs=[duration]
                    )

                    load_btn.click(
                        fn=self.load_video,
                        inputs=[data_dir, video_file, start_time, duration],
                        outputs=[image_display, status_text, frame_slider]
                    )

                    # ìŠ¬ë¼ì´ë”ë¡œ í”„ë ˆì„ ì´ë™
                    frame_slider.change(
                        fn=lambda frame_idx: self.navigate_frame("goto", int(frame_idx)),
                        inputs=[frame_slider],
                        outputs=[image_display, status_text]
                    )

                    # ì´ë¯¸ì§€ í´ë¦­ ì‹œ point ì¶”ê°€
                    def handle_click(mode, evt: gr.SelectData):
                        """ì´ë¯¸ì§€ í´ë¦­ í•¸ë“¤ëŸ¬ - img íŒŒë¼ë¯¸í„° ì œê±°"""
                        if len(self.frames) == 0:
                            return None, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

                        # í´ë¦­ ì¢Œí‘œ
                        x, y = evt.index[0], evt.index[1]

                        # Point ì¶”ê°€
                        self.annotations[mode].append((x, y))

                        # í˜„ì¬ í”„ë ˆì„ì— point í‘œì‹œ
                        frame = self.frames[self.current_frame_idx].copy()
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                        # Foreground points (ë…¹ìƒ‰)
                        for px, py in self.annotations['foreground']:
                            cv2.circle(frame_rgb, (px, py), 5, (0, 255, 0), -1)
                            cv2.circle(frame_rgb, (px, py), 7, (255, 255, 255), 2)

                        # Background points (ë¹¨ê°„ìƒ‰)
                        for px, py in self.annotations['background']:
                            cv2.circle(frame_rgb, (px, py), 5, (255, 0, 0), -1)
                            cv2.circle(frame_rgb, (px, py), 7, (255, 255, 255), 2)

                        status = f"""
**Annotations:**
- Foreground: {len(self.annotations['foreground'])} points
- Background: {len(self.annotations['background'])} points

í´ë¦­í•œ ìœ„ì¹˜: ({x}, {y}) - {mode}
"""

                        return frame_rgb, status

                    image_display.select(
                        fn=handle_click,
                        inputs=[annotation_mode],
                        outputs=[image_display, status_text]
                    )

                    segment_btn.click(
                        fn=self.segment_current_frame,
                        outputs=[image_display, status_text]
                    )

                    # ëª©í‘œ í”„ë ˆì„ ìˆ˜ ë³€ê²½ ì‹œ auto_stride ìë™ ê³„ì‚°
                    target_frames.change(
                        fn=self.calculate_stride_from_target,
                        inputs=[target_frames],
                        outputs=[auto_stride]
                    )

                    propagate_btn.click(
                        fn=self.propagate_to_all_frames,
                        inputs=[auto_stride],  # frame_step ëŒ€ì‹  auto_stride ì‚¬ìš©
                        outputs=[image_display, status_text]
                    )

                    # ì €ì¥ (ê¸°ì¡´ ì„¸ì…˜ ë®ì–´ì“°ê¸°)
                    save_session_btn.click(
                        fn=lambda: self.save_annotation_session(save_as_new=False),
                        outputs=[status_text]
                    )

                    # ìƒˆë¡œ ì €ì¥ (ìƒˆ ì„¸ì…˜ ìƒì„±)
                    save_session_new_btn.click(
                        fn=lambda name: self.save_annotation_session(session_name=name, save_as_new=True),
                        inputs=[session_name_input],
                        outputs=[status_text]
                    )

                    mesh_btn.click(
                        fn=self.generate_3d_mesh,
                        inputs=[
                            mesh_seed,
                            mesh_stage1_steps,
                            mesh_stage2_steps,
                            mesh_postprocess,
                            mesh_simplify_ratio,
                            mesh_texture_baking,
                            mesh_texture_size,
                            mesh_vertex_color
                        ],
                        outputs=[mesh_file, status_text]
                    )

                    save_masks_btn.click(
                        fn=self.save_masks,
                        outputs=[status_text]
                    )

                    export_frames_btn.click(
                        fn=self.export_frames_and_masks,
                        outputs=[status_text]
                    )

                    export_fauna_btn.click(
                        fn=self.export_fauna_dataset,
                        inputs=[fauna_animal_name, fauna_target_frames],
                        outputs=[status_text]
                    )

                    clear_all_btn.click(
                        fn=self.clear_annotations,
                        outputs=[image_display, status_text]
                    )

                    # ì„¸ì…˜ ê´€ë¦¬ ì´ë²¤íŠ¸
                    session_refresh_btn.click(
                        fn=lambda: gr.Dropdown(choices=self.get_session_ids()),
                        outputs=[session_id_dropdown]
                    )

                    # ì„¸ì…˜ ë¡œë“œ í•¸ë“¤ëŸ¬ (batch/single ëª¨ë‘ ì§€ì›)
                    def load_session_handler(session_id):
                        """ì„¸ì…˜ ë¡œë“œ - batch ì„¸ì…˜ì¸ ê²½ìš° ë¹„ë””ì˜¤ ëª©ë¡ ì •ë³´ë„ ìƒíƒœì— í¬í•¨"""
                        return self.load_annotation_session(session_id)

                    load_session_btn.click(
                        fn=load_session_handler,
                        inputs=[session_id_dropdown],
                        outputs=[image_display, status_text]
                    )

                    # ì„¸ì…˜ ì‚­ì œ
                    def delete_session_handler(session_id):
                        msg, sessions = self.delete_session(session_id)
                        return msg, gr.Dropdown(choices=sessions, value=sessions[0] if sessions else None)

                    delete_session_btn.click(
                        fn=delete_session_handler,
                        inputs=[session_id_dropdown],
                        outputs=[status_text, session_id_dropdown]
                    )

                    # ì„¸ì…˜ ì´ë¦„ ë³€ê²½
                    def rename_session_handler(session_id, new_name):
                        msg, sessions, new_id = self.rename_session(session_id, new_name)
                        return msg, gr.Dropdown(choices=sessions, value=new_id if new_id else (sessions[0] if sessions else None)), ""

                    rename_session_btn.click(
                        fn=rename_session_handler,
                        inputs=[session_id_dropdown, rename_session_input],
                        outputs=[status_text, session_id_dropdown, rename_session_input]
                    )

                    def clear_points():
                        self.annotations = {'foreground': [], 'background': []}
                        if len(self.frames) > 0:
                            frame_rgb = self.frames[self.current_frame_idx].copy()  # ì´ë¯¸ RGB
                            return frame_rgb, "Points ì´ˆê¸°í™”ë¨"
                        return None, "Points ì´ˆê¸°í™”ë¨"

                    clear_btn.click(
                        fn=clear_points,
                        outputs=[image_display, status_text]
                    )

                    # ë¹„ë””ì˜¤ ìŠ¤ìº” ë²„íŠ¼
                    scan_video_btn.click(
                        fn=lambda d: gr.Dropdown(choices=self.scan_videos(d)),
                        inputs=[data_dir],
                        outputs=[video_file]
                    )

                    # í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜ ì´ë²¤íŠ¸
                    first_btn.click(
                        fn=lambda: self.navigate_frame("first"),
                        outputs=[image_display, status_text]
                    )

                    prev_btn.click(
                        fn=lambda step: self.navigate_frame("prev", int(step)),
                        inputs=[frame_step],
                        outputs=[image_display, status_text]
                    )

                    next_btn.click(
                        fn=lambda step: self.navigate_frame("next", int(step)),
                        inputs=[frame_step],
                        outputs=[image_display, status_text]
                    )

                    last_btn.click(
                        fn=lambda: self.navigate_frame("last"),
                        outputs=[image_display, status_text]
                    )

                    goto_btn.click(
                        fn=lambda frame_num: self.navigate_frame("goto", int(frame_num) - 1),  # 1-indexed to 0-indexed
                        inputs=[goto_frame],
                        outputs=[image_display, status_text]
                    )

                # ===== Tab 2: Batch Mode =====
                with gr.Tab("ğŸ“¦ Batch Mode"):
                    gr.Markdown("### ì—¬ëŸ¬ ë¹„ë””ì˜¤ ì¼ê´„ ì²˜ë¦¬")

                    with gr.Row():
                        with gr.Column(scale=1):
                            gr.Markdown("### ğŸ“‚ ë¹„ë””ì˜¤ ìŠ¤ìº”")

                            batch_data_dir = gr.Textbox(
                                label="ë¹„ë””ì˜¤ í´ë”",
                                value=self.default_data_dir
                            )

                            batch_pattern = gr.Textbox(
                                label="íŒŒì¼ íŒ¨í„´",
                                value="*.mp4",
                                info="ì˜ˆ: *.mp4, *.avi, video_*.mp4"
                            )

                            batch_scan_btn = gr.Button("ğŸ“‚ ë¹„ë””ì˜¤ ìŠ¤ìº”", variant="primary")

                            batch_info = gr.Markdown("ë¹„ë””ì˜¤ë¥¼ ìŠ¤ìº”í•˜ì„¸ìš”")

                            # ë¹„ë””ì˜¤ ì„ íƒ UI (Accordionìœ¼ë¡œ ê°ì‹¸ê¸°)
                            with gr.Accordion("ğŸ¬ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ì„ íƒ", open=True):
                                # ë¹„ë””ì˜¤ ëª©ë¡ì„ ì ‘ì„ ìˆ˜ ìˆëŠ” Accordionìœ¼ë¡œ ê°ì‹¸ê¸°
                                with gr.Accordion("ğŸ“‹ ë¹„ë””ì˜¤ ëª©ë¡ (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°/ì ‘ê¸°)", open=True) as batch_video_accordion:
                                    # ì „ì²´ ì„ íƒ/í•´ì œ ë²„íŠ¼ (ìƒë‹¨)
                                    with gr.Row():
                                        batch_select_all_btn = gr.Button("âœ… ì „ì²´ ì„ íƒ", size="sm")
                                        batch_deselect_all_btn = gr.Button("âŒ ì „ì²´ í•´ì œ", size="sm")

                                    batch_video_selection = gr.CheckboxGroup(
                                        label="ë¹„ë””ì˜¤ ëª©ë¡",
                                        choices=[],
                                        value=[],
                                        interactive=True,
                                        info="ì„ íƒëœ ë¹„ë””ì˜¤ë§Œ ì²˜ë¦¬ë©ë‹ˆë‹¤"
                                    )

                                    batch_video_count_info = gr.Markdown("**ì„ íƒëœ ë¹„ë””ì˜¤**: 0ê°œ")

                            gr.Markdown("### ğŸ¯ Reference Annotation")

                            gr.Markdown("""
ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ì˜ ëŒ€í‘œ í”„ë ˆì„ì— annotationì„ ì¶”ê°€í•˜ì„¸ìš”.
ëª¨ë“  ë¹„ë””ì˜¤ì— ë™ì¼í•œ annotationì´ ì ìš©ë©ë‹ˆë‹¤.
                            """)

                            # Interactive Modeì—ì„œ ì‚¬ìš©í•˜ëŠ” annotation UI ì¬ì‚¬ìš©
                            batch_load_ref_btn = gr.Button("ğŸ“¹ Reference í”„ë ˆì„ ë¡œë“œ")

                            batch_annotation_mode = gr.Radio(
                                label="Point íƒ€ì…",
                                choices=["foreground", "background"],
                                value="foreground"
                            )

                            with gr.Row():
                                batch_segment_btn = gr.Button("ğŸ¯ Segment (ë¯¸ë¦¬ë³´ê¸°)", variant="secondary", size="sm")
                                batch_clear_btn = gr.Button("ğŸ—‘ï¸ Points ì´ˆê¸°í™”", size="sm")

                            # ===== ë¹„ë””ì˜¤ë³„ ê°œë³„ Annotation ì„¹ì…˜ =====
                            with gr.Accordion("ğŸ¬ ë¹„ë””ì˜¤ë³„ ê°œë³„ Annotation (ì„ íƒì‚¬í•­)", open=False):
                                gr.Markdown("""
**ê° ë¹„ë””ì˜¤ë§ˆë‹¤ ê°œë³„ annotationì„ ì§€ì •í•˜ë©´ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**
1. ë¹„ë””ì˜¤ ì„ íƒ â†’ ë¡œë“œ â†’ annotation â†’ ì €ì¥
2. ëª¨ë“  ë¹„ë””ì˜¤ annotation í›„ â†’ "ë¹„ë””ì˜¤ë³„ Batch Propagate" ì‹¤í–‰
                                """)

                                batch_per_video_select = gr.Dropdown(
                                    label="ë¹„ë””ì˜¤ ì„ íƒ",
                                    choices=[],
                                    interactive=True,
                                    info="annotationí•  ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”"
                                )

                                with gr.Row():
                                    batch_load_video_btn = gr.Button("ğŸ“¹ ë¡œë“œ", variant="secondary", size="sm")
                                    batch_save_video_anno_btn = gr.Button("ğŸ’¾ ì €ì¥", variant="primary", size="sm")

                                batch_per_video_status = gr.Markdown("### ğŸ“‹ ë¹„ë””ì˜¤ë³„ Annotation: ì—†ìŒ")

                                batch_propagate_per_video_btn = gr.Button(
                                    "ğŸ”„ ë¹„ë””ì˜¤ë³„ Batch Propagate",
                                    variant="primary",
                                    size="lg"
                                )

                                gr.Markdown("---")
                                gr.Markdown("**ğŸ’¾ Annotation íŒŒì¼ ì €ì¥/ë¡œë“œ** (propagation ì „ì—ë„ ê°€ëŠ¥)")

                                with gr.Row():
                                    batch_save_anno_file_btn = gr.Button("ğŸ’¾ Annotation íŒŒì¼ ì €ì¥", size="sm")
                                    batch_scan_anno_files_btn = gr.Button("ğŸ” íŒŒì¼ ìŠ¤ìº”", size="sm")

                                batch_anno_file_dropdown = gr.Dropdown(
                                    label="ì €ì¥ëœ Annotation íŒŒì¼",
                                    choices=[],
                                    interactive=True
                                )

                                batch_load_anno_file_btn = gr.Button("ğŸ“‚ Annotation íŒŒì¼ ë¡œë“œ", variant="secondary", size="sm")

                            gr.Markdown("### âš™ï¸ Batch ì„¤ì •")

                            with gr.Row():
                                batch_target_frames = gr.Number(
                                    label="ë¹„ë””ì˜¤ë‹¹ ëª©í‘œ í”„ë ˆì„ ìˆ˜",
                                    value=100,
                                    minimum=10,
                                    maximum=500,
                                    step=10,
                                    info="ê° ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œí•  ëª©í‘œ í”„ë ˆì„ ìˆ˜ (ì‹¤ì œ stride ìë™ ê³„ì‚°)"
                                )

                            batch_propagate_btn = gr.Button("ğŸ”„ Batch Propagate", variant="primary", size="lg")

                            gr.Markdown("### ğŸ’¾ ì„¸ì…˜ ê´€ë¦¬")

                            # ì„¸ì…˜ ë¡œë“œ
                            with gr.Accordion("ğŸ“‚ ì„¸ì…˜ ë¶ˆëŸ¬ì˜¤ê¸° / ê´€ë¦¬", open=False):
                                batch_session_scan_btn = gr.Button("ğŸ” ì„¸ì…˜ ìŠ¤ìº”", size="sm")
                                batch_load_session_dropdown = gr.Dropdown(
                                    label="ì„¸ì…˜ ì„ íƒ",
                                    choices=[],
                                    interactive=True
                                )
                                with gr.Row():
                                    batch_load_session_btn = gr.Button("ğŸ“‚ ë¡œë“œ", variant="secondary", size="sm")
                                    batch_delete_session_btn = gr.Button("ğŸ—‘ï¸ ì‚­ì œ", variant="stop", size="sm")

                                with gr.Accordion("âœï¸ ì„¸ì…˜ ì´ë¦„ ë³€ê²½", open=False):
                                    batch_rename_session_input = gr.Textbox(
                                        label="ìƒˆ ì´ë¦„",
                                        placeholder="ìƒˆ ì„¸ì…˜ ì´ë¦„ ì…ë ¥"
                                    )
                                    batch_rename_session_btn = gr.Button("âœï¸ ì´ë¦„ ë³€ê²½", size="sm")

                            # ì„¸ì…˜ ì €ì¥ ë° Export
                            batch_session_name = gr.Textbox(
                                label="ì„¸ì…˜/ë°ì´í„°ì…‹ ì´ë¦„",
                                value="mouse_batch",
                                placeholder="ì˜ˆ: mouse_experiment_batch",
                                info="ì„¸ì…˜ ì €ì¥ ë° Fauna export ì‹œ ì‚¬ìš©"
                            )

                            batch_file_structure = gr.Radio(
                                choices=[
                                    ("ğŸ“ ë¹„ë””ì˜¤ë³„ í´ë” (video001/frame_0000_rgb.png)", "video_folders"),
                                    ("ğŸ“„ ì™„ì „ í‰ë©´ (video001_frame_0000_rgb.png)", "flat")
                                ],
                                value="video_folders",
                                label="ğŸ“ íŒŒì¼ êµ¬ì¡° (Export ì‹œ)",
                                info="ë¹„ë””ì˜¤ë³„ë¡œ í´ë” êµ¬ì¡° vs ëª¨ë“  íŒŒì¼ í‰ë©´ êµ¬ì¡°"
                            )

                            with gr.Row():
                                batch_save_session_btn = gr.Button("ğŸ’¾ Save Session", variant="secondary")
                                batch_export_btn = gr.Button("ğŸ“¦ Export to Fauna", variant="secondary")

                        with gr.Column(scale=2):
                            batch_image_display = gr.Image(
                                label="Reference Frame / ê²°ê³¼ ì‹œê°í™”",
                                type="numpy"
                            )

                            # í”„ë ˆì„ ìŠ¬ë¼ì´ë” - ì´ë¯¸ì§€ ë°”ë¡œ ì•„ë˜ì— ë°°ì¹˜
                            batch_vis_slider = gr.Slider(
                                label="ğŸï¸ í”„ë ˆì„ (ìŠ¬ë¼ì´ë”ë¡œ ë¹ ë¥´ê²Œ íƒìƒ‰)",
                                minimum=0,
                                maximum=1,
                                value=0,
                                step=1,
                                interactive=True
                            )

                            # í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜ ë²„íŠ¼
                            with gr.Row():
                                batch_vis_prev_btn = gr.Button("â—€ï¸ ì´ì „", size="sm")
                                batch_vis_next_btn = gr.Button("â–¶ï¸ ë‹¤ìŒ", size="sm")
                                batch_vis_first_btn = gr.Button("â®ï¸ ì²˜ìŒ", size="sm")
                                batch_vis_last_btn = gr.Button("â­ï¸ ë", size="sm")

                            batch_vis_info = gr.Markdown("ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ê³  í”„ë ˆì„ì„ íƒìƒ‰í•˜ì„¸ìš”.")

                            batch_status_text = gr.Markdown("### ìƒíƒœ: ëŒ€ê¸° ì¤‘")

                            batch_output_path = gr.Textbox(
                                label="ì¶œë ¥ ê²½ë¡œ",
                                interactive=False
                            )

                            # ===== ê²°ê³¼ ì‹œê°í™” & í€„ë¦¬í‹° ì²´í¬ ì„¹ì…˜ =====
                            with gr.Accordion("ğŸ¬ ê²°ê³¼ ì‹œê°í™” & í€„ë¦¬í‹° ì²´í¬", open=True):
                                gr.Markdown("**ë¹„ë””ì˜¤ë³„ë¡œ ë§ˆìŠ¤í¬ ê²°ê³¼ë¥¼ ë¹ ë¥´ê²Œ í™•ì¸í•˜ì„¸ìš”**")

                                # ë¹„ë””ì˜¤ ì„ íƒ
                                with gr.Row():
                                    batch_preview_video_select = gr.Dropdown(
                                        label="ğŸ“¹ ë¹„ë””ì˜¤ ì„ íƒ",
                                        choices=[],
                                        interactive=True,
                                        scale=2
                                    )
                                    batch_preview_refresh_btn = gr.Button("ğŸ”„", size="sm", scale=0)

                                # ë””ìŠ¤í”Œë ˆì´ ëª¨ë“œ
                                batch_preview_mode = gr.Radio(
                                    label="í‘œì‹œ ëª¨ë“œ",
                                    choices=[
                                        ("ğŸ­ Binary Mask", "mask"),
                                        ("ğŸŸ¢ Overlay", "overlay"),
                                        ("ğŸ“Š Side by Side", "side_by_side")
                                    ],
                                    value="overlay",
                                    interactive=True
                                )

                                # í”„ë ˆì„ ìŠ¬ë¼ì´ë”ì™€ ë„¤ë¹„ê²Œì´ì…˜ ë²„íŠ¼ì€ ì´ë¯¸ì§€ ë°”ë¡œ ì•„ë˜ë¡œ ì´ë™ë¨ (ìœ„ ì°¸ì¡°)

                                gr.Markdown("---")
                                gr.Markdown("**ğŸ¬ í”„ë¦¬ë·° ì˜ìƒ ìƒì„±** (ì €í•´ìƒë„ ë¹ ë¥¸ í™•ì¸)")

                                with gr.Row():
                                    batch_preview_fps = gr.Slider(
                                        label="FPS",
                                        minimum=5,
                                        maximum=30,
                                        value=15,
                                        step=5,
                                        scale=1
                                    )
                                    batch_preview_scale = gr.Slider(
                                        label="í•´ìƒë„ %",
                                        minimum=25,
                                        maximum=100,
                                        value=50,
                                        step=25,
                                        scale=1
                                    )

                                with gr.Row():
                                    batch_gen_preview_btn = gr.Button("ğŸ¬ í”„ë¦¬ë·° ì˜ìƒ ìƒì„±", variant="primary")
                                    batch_gen_all_preview_btn = gr.Button("ğŸ“¦ ì „ì²´ ë¹„ë””ì˜¤ í”„ë¦¬ë·°", variant="secondary")

                                # ë¹„ë””ì˜¤ í”Œë ˆì´ì–´
                                batch_preview_video = gr.Video(
                                    label="í”„ë¦¬ë·° ì˜ìƒ",
                                    interactive=False,
                                    autoplay=True
                                )

                                gr.Markdown("---")
                                gr.Markdown("**ğŸ¯ 3D Mesh ìƒì„±**")

                                with gr.Accordion("âš™ï¸ Mesh íŒŒë¼ë¯¸í„°", open=False):
                                    batch_mesh_seed = gr.Number(
                                        label="Seed (ì¬í˜„ì„±)",
                                        value=42,
                                        precision=0,
                                        info="ë™ì¼ seed = ë™ì¼ ê²°ê³¼"
                                    )
                                    with gr.Row():
                                        batch_mesh_stage1_steps = gr.Slider(
                                            label="Stage1 Steps",
                                            minimum=5,
                                            maximum=50,
                                            value=25,
                                            step=5,
                                            info="Sparse structure í’ˆì§ˆ"
                                        )
                                        batch_mesh_stage2_steps = gr.Slider(
                                            label="Stage2 Steps",
                                            minimum=5,
                                            maximum=50,
                                            value=25,
                                            step=5,
                                            info="Latent feature í’ˆì§ˆ"
                                        )
                                    batch_mesh_postprocess = gr.Checkbox(
                                        label="Mesh í›„ì²˜ë¦¬ (ë‹¨ìˆœí™”, í™€ ì±„ìš°ê¸°)",
                                        value=False,
                                        info="âš ï¸ nvdiffrast í•„ìš” - ë¯¸ì„¤ì¹˜ ì‹œ ë¹„í™œì„±í™” ê¶Œì¥"
                                    )
                                    batch_mesh_simplify_ratio = gr.Slider(
                                        label="Simplify Ratio",
                                        minimum=0.5,
                                        maximum=0.99,
                                        value=0.95,
                                        step=0.05,
                                        info="Face ìœ ì§€ ë¹„ìœ¨ (0.95 = 5% ì œê±°)",
                                        visible=False
                                    )
                                    batch_mesh_texture_baking = gr.Checkbox(
                                        label="Texture Baking",
                                        value=False,
                                        info="í…ìŠ¤ì²˜ ë§µ ìƒì„± (ì¶”ê°€ ì‹œê°„ í•„ìš”)"
                                    )
                                    batch_mesh_texture_size = gr.Dropdown(
                                        label="Texture Size",
                                        choices=[512, 1024, 2048],
                                        value=1024,
                                        visible=False
                                    )
                                    batch_mesh_vertex_color = gr.Checkbox(
                                        label="Vertex Color ì‚¬ìš©",
                                        value=True,
                                        info="ë²„í…ìŠ¤ì— ìƒ‰ìƒ ì €ì¥"
                                    )

                                    # í›„ì²˜ë¦¬ ì²´í¬ë°•ìŠ¤ì— ë”°ë¼ simplify_ratio í‘œì‹œ
                                    batch_mesh_postprocess.change(
                                        fn=lambda x: gr.update(visible=x),
                                        inputs=[batch_mesh_postprocess],
                                        outputs=[batch_mesh_simplify_ratio]
                                    )
                                    # í…ìŠ¤ì²˜ ë² ì´í‚¹ ì²´í¬ë°•ìŠ¤ì— ë”°ë¼ texture_size í‘œì‹œ
                                    batch_mesh_texture_baking.change(
                                        fn=lambda x: gr.update(visible=x),
                                        inputs=[batch_mesh_texture_baking],
                                        outputs=[batch_mesh_texture_size]
                                    )

                                with gr.Row():
                                    batch_gen_mesh_btn = gr.Button("ğŸ¯ í˜„ì¬ í”„ë ˆì„ 3D Mesh", variant="primary")
                                    batch_gen_all_mesh_btn = gr.Button("ğŸ“¦ ì „ì²´ ë¹„ë””ì˜¤ 3D Mesh", variant="secondary")
                                batch_mesh_output = gr.File(label="ìƒì„±ëœ 3D Mesh", interactive=False)

                                # ===== í”„ë ˆì„ ì„ íƒ ë¦¬ìŠ¤íŠ¸ ê¸°ëŠ¥ =====
                                gr.Markdown("---")
                                gr.Markdown("**ğŸ“‹ ì„ íƒ í”„ë ˆì„ ì¼ê´„ 3D Mesh ìƒì„±**")
                                gr.Markdown("í”„ë ˆì„ íƒìƒ‰ ì¤‘ ì›í•˜ëŠ” í”„ë ˆì„ì„ ì„ íƒí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•œ ë’¤ ì¼ê´„ ìƒì„±í•˜ì„¸ìš”.")

                                with gr.Row():
                                    batch_add_frame_btn = gr.Button("â• í˜„ì¬ í”„ë ˆì„ ì¶”ê°€", variant="secondary", size="sm")
                                    batch_clear_frame_list_btn = gr.Button("ğŸ—‘ï¸ ëª©ë¡ ì´ˆê¸°í™”", size="sm")

                                batch_selected_frames_display = gr.Markdown("**ì„ íƒëœ í”„ë ˆì„**: ì—†ìŒ")

                                # ì„ íƒëœ í”„ë ˆì„ ì €ì¥ìš© State
                                batch_selected_frames_state = gr.State([])

                                with gr.Row():
                                    batch_gen_selected_mesh_btn = gr.Button(
                                        "ğŸ¯ ì„ íƒ í”„ë ˆì„ ì¼ê´„ 3D Mesh ìƒì„±",
                                        variant="primary",
                                        size="lg"
                                    )

                                batch_selected_mesh_status = gr.Markdown("")

                                gr.Markdown("---")
                                gr.Markdown("**ğŸ“¤ ë‚´ë³´ë‚´ê¸°**")
                                with gr.Row():
                                    batch_gen_vis_btn = gr.Button("ğŸ¨ ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥", variant="secondary")
                                    batch_gen_video_btn = gr.Button("ğŸ“¹ ì „ì²´ ì˜ìƒ ìƒì„±", variant="secondary")

                    # Event handlers
                    batch_scan_btn.click(
                        fn=self.scan_batch_videos,
                        inputs=[batch_data_dir, batch_pattern],
                        outputs=[gr.State(), batch_info, batch_video_selection]
                    )

                    # ì „ì²´ ì„ íƒ/í•´ì œ ë²„íŠ¼
                    def select_all_videos():
                        if hasattr(self, 'batch_video_label_map'):
                            all_labels = list(self.batch_video_label_map.keys())
                            count = len(all_labels)
                            return gr.CheckboxGroup(value=all_labels), f"**ì„ íƒëœ ë¹„ë””ì˜¤**: {count}ê°œ"
                        return gr.CheckboxGroup(value=[]), "**ì„ íƒëœ ë¹„ë””ì˜¤**: 0ê°œ"

                    def deselect_all_videos():
                        return gr.CheckboxGroup(value=[]), "**ì„ íƒëœ ë¹„ë””ì˜¤**: 0ê°œ"

                    def update_video_count(selected):
                        """ë¹„ë””ì˜¤ ì„ íƒ ìˆ˜ ì—…ë°ì´íŠ¸"""
                        count = len(selected) if selected else 0
                        return f"**ì„ íƒëœ ë¹„ë””ì˜¤**: {count}ê°œ"

                    batch_select_all_btn.click(
                        fn=select_all_videos,
                        outputs=[batch_video_selection, batch_video_count_info]
                    )

                    batch_deselect_all_btn.click(
                        fn=deselect_all_videos,
                        outputs=[batch_video_selection, batch_video_count_info]
                    )

                    batch_video_selection.change(
                        fn=update_video_count,
                        inputs=[batch_video_selection],
                        outputs=[batch_video_count_info]
                    )

                    # Reference frame ë¡œë“œ
                    batch_load_ref_btn.click(
                        fn=self.batch_load_reference_frame,
                        inputs=[batch_video_selection],
                        outputs=[batch_image_display, batch_status_text]
                    )

                    # Batch ëª¨ë“œ point annotation í´ë¦­ ì´ë²¤íŠ¸
                    batch_image_display.select(
                        fn=self.add_point,
                        inputs=[batch_image_display, batch_annotation_mode],
                        outputs=[batch_image_display, batch_status_text]
                    )

                    # Batch segment (ë¯¸ë¦¬ë³´ê¸°)
                    batch_segment_btn.click(
                        fn=self.segment_current_frame,
                        outputs=[batch_image_display, batch_status_text]
                    )

                    # Batch clear points
                    def batch_clear_points():
                        self.annotations = {'foreground': [], 'background': []}
                        if len(self.frames) > 0:
                            frame_rgb = self.frames[self.current_frame_idx].copy()  # ì´ë¯¸ RGB
                            return frame_rgb, "Points ì´ˆê¸°í™”ë¨"
                        return None, "Points ì´ˆê¸°í™”ë¨"

                    batch_clear_btn.click(
                        fn=batch_clear_points,
                        outputs=[batch_image_display, batch_status_text]
                    )

                    batch_propagate_btn.click(
                        fn=self.batch_propagate_videos,
                        inputs=[batch_target_frames, batch_video_selection],
                        outputs=[batch_status_text, gr.State()]
                    )

                    # ì„¸ì…˜ ìŠ¤ìº”
                    def scan_batch_sessions():
                        """Batch ì„¸ì…˜ ë””ë ‰í† ë¦¬ ìŠ¤ìº”"""
                        sessions_dir = Path(self.default_output_dir) / "sessions"
                        if not sessions_dir.exists():
                            return gr.Dropdown(choices=[])

                        sessions = []
                        for session_dir in sessions_dir.iterdir():
                            if session_dir.is_dir():
                                # Check for batch session metadata
                                meta_file = session_dir / "session_metadata.json"
                                if meta_file.exists():
                                    sessions.append(str(session_dir))
                        return gr.Dropdown(choices=sorted(sessions, reverse=True))

                    batch_session_scan_btn.click(
                        fn=scan_batch_sessions,
                        outputs=[batch_load_session_dropdown]
                    )

                    # ì„¸ì…˜ ë¡œë“œ (ë¹„ë””ì˜¤ ëª©ë¡ë„ í•¨ê»˜ ì—…ë°ì´íŠ¸)
                    def load_batch_session_and_refresh(session_path):
                        """Batch ì„¸ì…˜ ë¡œë“œ í›„ ë¹„ë””ì˜¤ ëª©ë¡ë„ ì—…ë°ì´íŠ¸"""
                        status_msg, output_path = self.load_batch_session(session_path)

                        # ë¹„ë””ì˜¤ ëª©ë¡ ì—…ë°ì´íŠ¸ (unique_id í¬í•¨: mouse+camera+frame)
                        video_list = self.get_batch_video_list()
                        if video_list:
                            choices = []
                            for v in video_list:
                                # unique_id ì‚¬ìš© (ì˜ˆ: m1_cam1_0)
                                unique_id = v.get('unique_id', v['video_name'])
                                label = f"[{v['video_idx']}] {unique_id} ({v['num_frames']}f)"
                                choices.append((label, v['video_idx']))
                            video_dropdown = gr.Dropdown(choices=choices, value=choices[0][1] if choices else None)
                        else:
                            video_dropdown = gr.Dropdown(choices=[], value=None)

                        return status_msg, output_path, video_dropdown

                    batch_load_session_btn.click(
                        fn=load_batch_session_and_refresh,
                        inputs=[batch_load_session_dropdown],
                        outputs=[batch_status_text, batch_output_path, batch_preview_video_select]
                    )

                    # ì„¸ì…˜ ì‚­ì œ
                    def delete_session_handler(session_path):
                        msg, sessions = self.delete_batch_session(session_path)
                        return msg, gr.Dropdown(choices=sessions, value=sessions[0] if sessions else None)

                    batch_delete_session_btn.click(
                        fn=delete_session_handler,
                        inputs=[batch_load_session_dropdown],
                        outputs=[batch_status_text, batch_load_session_dropdown]
                    )

                    # ì„¸ì…˜ ì´ë¦„ ë³€ê²½
                    def rename_session_handler(session_path, new_name):
                        msg, sessions = self.rename_batch_session(session_path, new_name)
                        return msg, gr.Dropdown(choices=sessions, value=sessions[0] if sessions else None), ""

                    batch_rename_session_btn.click(
                        fn=rename_session_handler,
                        inputs=[batch_load_session_dropdown, batch_rename_session_input],
                        outputs=[batch_status_text, batch_load_session_dropdown, batch_rename_session_input]
                    )

                    # ì„¸ì…˜ ì €ì¥
                    batch_save_session_btn.click(
                        fn=self.save_batch_session,
                        inputs=[batch_session_name],
                        outputs=[batch_output_path, batch_status_text]
                    )

                    # Fauna export
                    batch_export_btn.click(
                        fn=self.export_batch_to_fauna,
                        inputs=[batch_session_name, batch_file_structure],
                        outputs=[batch_output_path, batch_status_text]
                    )

                    # ===== ë¹„ë””ì˜¤ë³„ Annotation ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ =====

                    # ë¹„ë””ì˜¤ ìŠ¤ìº” ì‹œ per-video ë“œë¡­ë‹¤ìš´ë„ ì—…ë°ì´íŠ¸
                    def update_per_video_dropdown():
                        if hasattr(self, 'batch_video_label_map'):
                            labels = list(self.batch_video_label_map.keys())
                            return gr.Dropdown(choices=labels, value=labels[0] if labels else None)
                        return gr.Dropdown(choices=[])

                    batch_scan_btn.click(
                        fn=update_per_video_dropdown,
                        outputs=[batch_per_video_select]
                    )

                    # ë¹„ë””ì˜¤ ë¡œë“œ (ê°œë³„ annotationìš©)
                    batch_load_video_btn.click(
                        fn=self.load_video_for_annotation,
                        inputs=[batch_per_video_select],
                        outputs=[batch_image_display, batch_status_text]
                    )

                    # ë¹„ë””ì˜¤ë³„ annotation ì €ì¥
                    def save_video_anno_handler(video_label):
                        msg = self.save_current_annotation_for_video(video_label)
                        status = self.get_per_video_annotation_status()
                        return msg, status

                    batch_save_video_anno_btn.click(
                        fn=save_video_anno_handler,
                        inputs=[batch_per_video_select],
                        outputs=[batch_status_text, batch_per_video_status]
                    )

                    # ===== Annotation íŒŒì¼ ì €ì¥/ë¡œë“œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ =====

                    # Annotation íŒŒì¼ ì €ì¥
                    def save_anno_file_handler():
                        path, msg = self.save_per_video_annotations_to_file()
                        status = self.get_per_video_annotation_status()
                        return msg, status

                    batch_save_anno_file_btn.click(
                        fn=save_anno_file_handler,
                        outputs=[batch_status_text, batch_per_video_status]
                    )

                    # Annotation íŒŒì¼ ìŠ¤ìº”
                    def scan_anno_files_handler():
                        files = self.scan_annotation_files()
                        return gr.Dropdown(choices=files, value=files[0] if files else None)

                    batch_scan_anno_files_btn.click(
                        fn=scan_anno_files_handler,
                        outputs=[batch_anno_file_dropdown]
                    )

                    # Annotation íŒŒì¼ ë¡œë“œ
                    def load_anno_file_handler(filepath):
                        msg, status = self.load_per_video_annotations_from_file(filepath)
                        return msg, status

                    batch_load_anno_file_btn.click(
                        fn=load_anno_file_handler,
                        inputs=[batch_anno_file_dropdown],
                        outputs=[batch_status_text, batch_per_video_status]
                    )

                    # ë¹„ë””ì˜¤ë³„ Batch Propagate
                    batch_propagate_per_video_btn.click(
                        fn=self.batch_propagate_with_per_video_annotations,
                        inputs=[batch_target_frames, batch_video_selection],
                        outputs=[batch_status_text, gr.State()]
                    )

                    # ===== ê²°ê³¼ ì‹œê°í™” & í€„ë¦¬í‹° ì²´í¬ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ =====

                    # í˜„ì¬ ì„ íƒëœ ë¹„ë””ì˜¤ ì¸ë±ìŠ¤ ì €ì¥
                    current_preview_video_idx = gr.State(value=0)

                    # ë¹„ë””ì˜¤ ëª©ë¡ ìƒˆë¡œê³ ì¹¨
                    def refresh_preview_video_list():
                        """í”„ë¦¬ë·°ìš© ë¹„ë””ì˜¤ ëª©ë¡ ì—…ë°ì´íŠ¸ (unique_id í¬í•¨: mouse+camera+frame)"""
                        video_list = self.get_batch_video_list()
                        if video_list:
                            choices = []
                            for v in video_list:
                                # unique_id ì‚¬ìš© (ì˜ˆ: m1_cam1_0)
                                unique_id = v.get('unique_id', v['video_name'])
                                label = f"[{v['video_idx']}] {unique_id} ({v['num_frames']}f)"
                                choices.append((label, v['video_idx']))
                            return gr.Dropdown(choices=choices, value=choices[0][1] if choices else None)
                        return gr.Dropdown(choices=[], value=None)

                    batch_preview_refresh_btn.click(
                        fn=refresh_preview_video_list,
                        outputs=[batch_preview_video_select]
                    )

                    # ë¹„ë””ì˜¤ ì„ íƒ ì‹œ ìŠ¬ë¼ì´ë” ë²”ìœ„ ì—…ë°ì´íŠ¸
                    def on_video_select(video_idx, display_mode):
                        if video_idx is None:
                            return None, "ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”.", gr.Slider(maximum=1, value=0), video_idx

                        video_list = self.get_batch_video_list()
                        video_info = None
                        for v in video_list:
                            if v['video_idx'] == video_idx:
                                video_info = v
                                break

                        if video_info is None:
                            return None, "ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", gr.Slider(maximum=1, value=0), video_idx

                        num_frames = video_info['num_frames']
                        img, status = self.get_video_frame_for_preview(video_idx, 0, display_mode)
                        return img, status, gr.Slider(maximum=max(1, num_frames-1), value=0), video_idx

                    batch_preview_video_select.change(
                        fn=on_video_select,
                        inputs=[batch_preview_video_select, batch_preview_mode],
                        outputs=[batch_image_display, batch_vis_info, batch_vis_slider, current_preview_video_idx]
                    )

                    # ë””ìŠ¤í”Œë ˆì´ ëª¨ë“œ ë³€ê²½ ì‹œ
                    def on_mode_change(video_idx, frame_idx, display_mode):
                        if video_idx is None:
                            return None, "ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”."
                        img, status = self.get_video_frame_for_preview(video_idx, int(frame_idx), display_mode)
                        return img, status

                    batch_preview_mode.change(
                        fn=on_mode_change,
                        inputs=[current_preview_video_idx, batch_vis_slider, batch_preview_mode],
                        outputs=[batch_image_display, batch_vis_info]
                    )

                    # ìŠ¬ë¼ì´ë” ë³€ê²½ ì‹œ
                    def on_frame_slider_change(video_idx, frame_idx, display_mode):
                        if video_idx is None:
                            return None, "ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”."
                        img, status = self.get_video_frame_for_preview(video_idx, int(frame_idx), display_mode)
                        return img, status

                    batch_vis_slider.change(
                        fn=on_frame_slider_change,
                        inputs=[current_preview_video_idx, batch_vis_slider, batch_preview_mode],
                        outputs=[batch_image_display, batch_vis_info]
                    )

                    # í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜ ë²„íŠ¼
                    def frame_nav(video_idx, current_idx, display_mode, direction):
                        if video_idx is None:
                            return None, "ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”.", 0

                        video_list = self.get_batch_video_list()
                        video_info = None
                        for v in video_list:
                            if v['video_idx'] == video_idx:
                                video_info = v
                                break

                        if video_info is None:
                            return None, "ë¹„ë””ì˜¤ ì—†ìŒ", 0

                        max_idx = video_info['num_frames'] - 1

                        if direction == "prev":
                            new_idx = max(0, int(current_idx) - 1)
                        elif direction == "next":
                            new_idx = min(max_idx, int(current_idx) + 1)
                        elif direction == "first":
                            new_idx = 0
                        elif direction == "last":
                            new_idx = max_idx
                        else:
                            new_idx = int(current_idx)

                        img, status = self.get_video_frame_for_preview(video_idx, new_idx, display_mode)
                        return img, status, new_idx

                    batch_vis_prev_btn.click(
                        fn=lambda v, c, m: frame_nav(v, c, m, "prev"),
                        inputs=[current_preview_video_idx, batch_vis_slider, batch_preview_mode],
                        outputs=[batch_image_display, batch_vis_info, batch_vis_slider]
                    )

                    batch_vis_next_btn.click(
                        fn=lambda v, c, m: frame_nav(v, c, m, "next"),
                        inputs=[current_preview_video_idx, batch_vis_slider, batch_preview_mode],
                        outputs=[batch_image_display, batch_vis_info, batch_vis_slider]
                    )

                    batch_vis_first_btn.click(
                        fn=lambda v, c, m: frame_nav(v, c, m, "first"),
                        inputs=[current_preview_video_idx, batch_vis_slider, batch_preview_mode],
                        outputs=[batch_image_display, batch_vis_info, batch_vis_slider]
                    )

                    batch_vis_last_btn.click(
                        fn=lambda v, c, m: frame_nav(v, c, m, "last"),
                        inputs=[current_preview_video_idx, batch_vis_slider, batch_preview_mode],
                        outputs=[batch_image_display, batch_vis_info, batch_vis_slider]
                    )

                    # í”„ë¦¬ë·° ì˜ìƒ ìƒì„±
                    def generate_single_preview(video_idx, display_mode, fps, scale_percent):
                        if video_idx is None:
                            return None, "ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”."
                        scale = scale_percent / 100.0
                        video_path, status = self.generate_preview_video(video_idx, display_mode, int(fps), scale)
                        return video_path if video_path else None, status

                    batch_gen_preview_btn.click(
                        fn=generate_single_preview,
                        inputs=[current_preview_video_idx, batch_preview_mode, batch_preview_fps, batch_preview_scale],
                        outputs=[batch_preview_video, batch_status_text]
                    )

                    # ì „ì²´ ë¹„ë””ì˜¤ í”„ë¦¬ë·° ìƒì„±
                    def generate_all_previews(display_mode, fps, scale_percent, progress=gr.Progress()):
                        video_list = self.get_batch_video_list()
                        if not video_list:
                            return None, "ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

                        scale = scale_percent / 100.0
                        last_video_path = None
                        results = []

                        for i, video_info in enumerate(video_list):
                            # unique_id ì‚¬ìš© (m1_cam1_0 í˜•ì‹)
                            unique_id = video_info.get('unique_id', video_info['video_name'])
                            progress(i / len(video_list), desc=f"ğŸ¬ {unique_id} ìƒì„± ì¤‘... ({i+1}/{len(video_list)})")
                            video_path, status = self.generate_preview_video(
                                video_info['video_idx'], display_mode, int(fps), scale
                            )
                            if video_path:
                                last_video_path = video_path
                                results.append(unique_id)

                        progress(1.0, desc="âœ… ì™„ë£Œ!")

                        status = f"""
### ğŸ“¦ ì „ì²´ í”„ë¦¬ë·° ìƒì„± ì™„ë£Œ âœ…

- **ìƒì„±ëœ ë¹„ë””ì˜¤**: {len(results)}ê°œ / {len(video_list)}ê°œ
- **ì €ì¥ ìœ„ì¹˜**: `{Path(self.default_output_dir) / 'previews'}`

<details>
<summary><b>ğŸ“‹ ìƒì„±ëœ í”„ë¦¬ë·° ëª©ë¡ ({len(results)}ê°œ) - í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°</b></summary>

{chr(10).join([f'- {r}' for r in results])}

</details>
"""
                        return last_video_path, status

                    batch_gen_all_preview_btn.click(
                        fn=generate_all_previews,
                        inputs=[batch_preview_mode, batch_preview_fps, batch_preview_scale],
                        outputs=[batch_preview_video, batch_status_text]
                    )

                    # ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥
                    batch_gen_vis_btn.click(
                        fn=lambda: self.generate_batch_visualization(output_format="images"),
                        outputs=[batch_output_path, batch_status_text]
                    )

                    # ì „ì²´ ì‹œê°í™” ì˜ìƒ ìƒì„±
                    batch_gen_video_btn.click(
                        fn=lambda: self.generate_batch_visualization(output_format="video"),
                        outputs=[batch_output_path, batch_status_text]
                    )

                    # í˜„ì¬ í”„ë ˆì„ 3D Mesh ìƒì„±
                    batch_gen_mesh_btn.click(
                        fn=lambda video_idx, frame_idx, seed, s1, s2, pp, sr, tb, ts, vc: self.batch_generate_3d_mesh_current(
                            video_idx, int(frame_idx), seed, s1, s2, pp, sr, tb, ts, vc
                        ),
                        inputs=[
                            current_preview_video_idx, batch_vis_slider,
                            batch_mesh_seed, batch_mesh_stage1_steps, batch_mesh_stage2_steps,
                            batch_mesh_postprocess, batch_mesh_simplify_ratio,
                            batch_mesh_texture_baking, batch_mesh_texture_size, batch_mesh_vertex_color
                        ],
                        outputs=[batch_mesh_output, batch_status_text]
                    )

                    # ì „ì²´ ë¹„ë””ì˜¤ 3D Mesh ìƒì„±
                    batch_gen_all_mesh_btn.click(
                        fn=self.batch_generate_3d_mesh_all,
                        inputs=[
                            batch_mesh_seed, batch_mesh_stage1_steps, batch_mesh_stage2_steps,
                            batch_mesh_postprocess, batch_mesh_simplify_ratio,
                            batch_mesh_texture_baking, batch_mesh_texture_size, batch_mesh_vertex_color
                        ],
                        outputs=[batch_mesh_output, batch_status_text]
                    )

                    # ===== í”„ë ˆì„ ì„ íƒ ë¦¬ìŠ¤íŠ¸ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ =====
                    def add_current_frame_to_list(video_idx, frame_idx, selected_frames):
                        """í˜„ì¬ í”„ë ˆì„ì„ ì„ íƒ ëª©ë¡ì— ì¶”ê°€"""
                        if video_idx is None or frame_idx is None:
                            return selected_frames, "**ì„ íƒëœ í”„ë ˆì„**: ë¹„ë””ì˜¤/í”„ë ˆì„ì„ ë¨¼ì € ì„ íƒí•˜ì„¸ìš”"

                        video_list = self.get_batch_video_list()
                        video_name = "unknown"
                        for v in video_list:
                            if v['video_idx'] == video_idx:
                                video_name = v['video_name']
                                break

                        frame_info = {
                            'video_idx': int(video_idx),
                            'video_name': video_name,
                            'frame_idx': int(frame_idx)
                        }

                        # ì¤‘ë³µ ì²´í¬
                        for existing in selected_frames:
                            if existing['video_idx'] == frame_info['video_idx'] and existing['frame_idx'] == frame_info['frame_idx']:
                                # ì´ë¯¸ ì¡´ì¬í•¨
                                display = format_selected_frames_display(selected_frames)
                                return selected_frames, display + "\n\nâš ï¸ ì´ë¯¸ ì¶”ê°€ëœ í”„ë ˆì„ì…ë‹ˆë‹¤."

                        selected_frames.append(frame_info)
                        display = format_selected_frames_display(selected_frames)
                        return selected_frames, display

                    def clear_frame_list():
                        """í”„ë ˆì„ ëª©ë¡ ì´ˆê¸°í™”"""
                        return [], "**ì„ íƒëœ í”„ë ˆì„**: ì—†ìŒ"

                    def format_selected_frames_display(selected_frames):
                        """ì„ íƒëœ í”„ë ˆì„ ëª©ë¡ í‘œì‹œ í…ìŠ¤íŠ¸ ìƒì„±"""
                        if not selected_frames:
                            return "**ì„ íƒëœ í”„ë ˆì„**: ì—†ìŒ"

                        display = f"**ì„ íƒëœ í”„ë ˆì„**: {len(selected_frames)}ê°œ\n\n"
                        for i, f in enumerate(selected_frames):
                            display += f"{i+1}. **{f['video_name']}** - frame {f['frame_idx']}\n"
                        return display

                    def generate_selected_meshes(selected_frames, seed, s1, s2, pp, sr, tb, ts, vc, progress=gr.Progress()):
                        """ì„ íƒëœ í”„ë ˆì„ë“¤ì˜ 3D Mesh ìƒì„±"""
                        output_path, status = self.batch_generate_3d_mesh_selected(
                            selected_frames, seed, s1, s2, pp, sr, tb, ts, vc, progress
                        )
                        return status

                    batch_add_frame_btn.click(
                        fn=add_current_frame_to_list,
                        inputs=[current_preview_video_idx, batch_vis_slider, batch_selected_frames_state],
                        outputs=[batch_selected_frames_state, batch_selected_frames_display]
                    )

                    batch_clear_frame_list_btn.click(
                        fn=clear_frame_list,
                        outputs=[batch_selected_frames_state, batch_selected_frames_display]
                    )

                    batch_gen_selected_mesh_btn.click(
                        fn=generate_selected_meshes,
                        inputs=[
                            batch_selected_frames_state,
                            batch_mesh_seed, batch_mesh_stage1_steps, batch_mesh_stage2_steps,
                            batch_mesh_postprocess, batch_mesh_simplify_ratio,
                            batch_mesh_texture_baking, batch_mesh_texture_size, batch_mesh_vertex_color
                        ],
                        outputs=[batch_selected_mesh_status]
                    )

                    # Batch propagate ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ í”„ë¦¬ë·° ëª©ë¡ ì—…ë°ì´íŠ¸
                    def on_propagate_complete():
                        return refresh_preview_video_list()

                    batch_propagate_btn.click(
                        fn=on_propagate_complete,
                        outputs=[batch_preview_video_select]
                    )

                    batch_propagate_per_video_btn.click(
                        fn=on_propagate_complete,
                        outputs=[batch_preview_video_select]
                    )

                # ===== Tab 3: Lite Annotator =====
                with gr.Tab("ğŸ“ Lite Annotator"):
                    gr.Markdown("### íš¨ìœ¨ì  Annotation ëª¨ë“œ")
                    gr.Markdown("Direct video/image loading, multi-model selection, auto-restore")

                    with gr.Row():
                        # Left column: Input & Frame Display
                        with gr.Column(scale=2):
                            # Input source section
                            gr.Markdown("#### ğŸ“‚ Input Source")
                            with gr.Row():
                                lite_input_path = gr.Textbox(
                                    label="Video/Image Folder Path",
                                    placeholder="/path/to/video.mp4 or /path/to/images/",
                                    scale=3
                                )
                                lite_input_type = gr.Radio(
                                    choices=["video", "images"],
                                    value="video",
                                    label="Type",
                                    scale=1
                                )

                            with gr.Row():
                                lite_pattern = gr.Textbox(
                                    label="Image Pattern (for images type)",
                                    value="*.png",
                                    scale=2
                                )
                                lite_load_btn = gr.Button("ğŸ“¥ Load Source", variant="primary", scale=1)

                            lite_load_status = gr.Markdown("No input loaded")

                            # Frame display
                            gr.Markdown("#### ğŸ–¼ï¸ Frame")
                            lite_frame_display = gr.Image(
                                label="Current Frame",
                                type="numpy",
                                height=500,
                                interactive=True  # Enable click events
                            )

                            # Frame navigation
                            with gr.Row():
                                lite_frame_slider = gr.Slider(
                                    label="Frame Index",
                                    minimum=0,
                                    maximum=100,
                                    value=0,
                                    step=1,
                                    interactive=True
                                )

                        # Right column: Controls & Mask Display
                        with gr.Column(scale=1):
                            # SAM2 ëª¨ë¸ ìƒíƒœ ì•ˆë‚´
                            gr.Markdown("#### ğŸ¤– SAM2 Model")
                            gr.Markdown("*ìƒë‹¨ì˜ ê³µìš© SAM2 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° ìƒë‹¨ì—ì„œ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”.*")

                            # Point annotation
                            gr.Markdown("#### ğŸ¨ Annotation")
                            lite_point_type = gr.Radio(
                                choices=["foreground", "background"],
                                value="foreground",
                                label="Point Type"
                            )

                            # Action buttons
                            with gr.Column():
                                lite_generate_btn = gr.Button("ğŸ¯ Generate Mask", variant="primary")
                                lite_save_btn = gr.Button("ğŸ’¾ Save Annotation", variant="secondary")
                                lite_clear_btn = gr.Button("ğŸ”„ Clear Points", variant="stop")

                            # Mask display
                            gr.Markdown("#### ğŸ­ Mask")
                            lite_mask_display = gr.Image(
                                label="Generated Mask",
                                type="numpy",
                                height=200
                            )

                            # Status
                            lite_status = gr.Markdown("**Status:** Load source to start")

                            # Info panel
                            gr.Markdown("#### â„¹ï¸ Info")
                            lite_info = gr.JSON(label="Current State", value={})

                    # Event handlers for Lite Annotator

                    # Load source
                    lite_load_btn.click(
                        fn=self._lite_load_source,
                        inputs=[lite_input_path, lite_input_type, lite_pattern],
                        outputs=[lite_load_status, lite_frame_slider, lite_info]
                    )

                    # Frame slider change
                    lite_frame_slider.change(
                        fn=self._lite_load_frame,
                        inputs=[lite_frame_slider],
                        outputs=[lite_frame_display, lite_status, lite_info]
                    )

                    # Click on frame to add point
                    lite_frame_display.select(
                        fn=self._lite_add_point,
                        inputs=[lite_point_type],
                        outputs=[lite_frame_display, lite_status]
                    )

                    # Generate mask
                    lite_generate_btn.click(
                        fn=self._lite_generate_mask,
                        outputs=[lite_frame_display, lite_mask_display, lite_status]
                    )

                    # Save annotation
                    lite_save_btn.click(
                        fn=self._lite_save_annotation,
                        outputs=[lite_status]
                    )

                    # Clear points
                    lite_clear_btn.click(
                        fn=self._lite_clear_points,
                        outputs=[lite_frame_display, lite_status]
                    )

                # ===== Tab 4: Data Augmentation =====
                with gr.Tab("ğŸ² Data Augmentation"):
                    gr.Markdown("### ë°ì´í„° ì¦ê°•")
                    gr.Markdown("RGB ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ë¥¼ í•¨ê»˜ ì¦ê°•í•©ë‹ˆë‹¤. ê¸°í•˜í•™ì  ë³€í™˜ì€ ë™ì¼í•˜ê²Œ, ìƒ‰ìƒ ë³€í™˜ì€ RGBë§Œ ì ìš©ë©ë‹ˆë‹¤.")

                    with gr.Row():
                        # Left column: Input & Controls
                        with gr.Column(scale=1):
                            gr.Markdown("#### ğŸ“‚ Input Source")

                            # Session selection
                            aug_session_dir = gr.Textbox(
                                label="Session Directory",
                                value=str(Path(self.default_output_dir) / "sessions"),
                                placeholder="Path to saved annotation sessions"
                            )

                            aug_scan_btn = gr.Button("ğŸ“‚ Scan Sessions", size="sm")

                            aug_session_list = gr.Dropdown(
                                label="Select Session",
                                choices=[],
                                interactive=True
                            )

                            aug_load_session_btn = gr.Button("ğŸ“¥ Load Session", variant="primary")

                            aug_session_info = gr.Markdown("No session loaded")

                            # Augmentation parameters
                            gr.Markdown("#### âš™ï¸ Augmentation Parameters")

                            with gr.Accordion("ğŸ”„ Geometric Transforms (RGB + Mask)", open=True):
                                aug_scale_enable = gr.Checkbox(label="Enable Scale", value=True)
                                with gr.Row():
                                    aug_scale_min = gr.Slider(
                                        label="Scale Min",
                                        minimum=0.3, maximum=1.0, value=0.5, step=0.05
                                    )
                                    aug_scale_max = gr.Slider(
                                        label="Scale Max",
                                        minimum=1.0, maximum=3.0, value=2.0, step=0.1
                                    )

                                aug_fill_color = gr.Dropdown(
                                    label="Fill Color (for shrinking)",
                                    choices=["white", "black", "nearest"],
                                    value="white"
                                )

                                aug_rotation_enable = gr.Checkbox(label="Enable Rotation", value=True)
                                with gr.Row():
                                    aug_rotation_min = gr.Slider(
                                        label="Rotation Min (deg)",
                                        minimum=-180, maximum=0, value=-30, step=5
                                    )
                                    aug_rotation_max = gr.Slider(
                                        label="Rotation Max (deg)",
                                        minimum=0, maximum=180, value=30, step=5
                                    )

                                aug_flip_enable = gr.Checkbox(label="Enable Random Flip", value=True)

                            with gr.Accordion("âœ‚ï¸ Crop-Based Scale Augmentation (Advanced)", open=True):
                                aug_crop_enable = gr.Checkbox(
                                    label="Enable Crop-Based Scale",
                                    value=False,
                                    info="Crop mask region, scale it, and paste on white background"
                                )
                                with gr.Row():
                                    aug_crop_scale_min = gr.Slider(
                                        label="Crop Scale Min",
                                        minimum=0.3, maximum=1.0, value=0.5, step=0.05
                                    )
                                    aug_crop_scale_max = gr.Slider(
                                        label="Crop Scale Max",
                                        minimum=1.0, maximum=3.0, value=2.0, step=0.1
                                    )

                                with gr.Row():
                                    aug_offset_x_max = gr.Slider(
                                        label="Max Horizontal Offset (ratio)",
                                        minimum=0.0, maximum=0.5, value=0.2, step=0.05,
                                        info="Offset as ratio of image width"
                                    )
                                    aug_offset_y_max = gr.Slider(
                                        label="Max Vertical Offset (ratio)",
                                        minimum=0.0, maximum=0.5, value=0.2, step=0.05,
                                        info="Offset as ratio of image height"
                                    )

                                aug_crop_padding = gr.Slider(
                                    label="Crop Padding (pixels)",
                                    minimum=0, maximum=100, value=20, step=5,
                                    info="Extra padding around mask bbox"
                                )

                            with gr.Accordion("ğŸ¨ Photometric Transforms (RGB only)", open=False):
                                aug_noise_enable = gr.Checkbox(label="Enable Gaussian Noise", value=True)
                                aug_noise_std = gr.Slider(
                                    label="Noise Std",
                                    minimum=0, maximum=30, value=10, step=1
                                )

                                aug_brightness_enable = gr.Checkbox(label="Enable Brightness", value=True)
                                with gr.Row():
                                    aug_brightness_min = gr.Slider(
                                        label="Brightness Min",
                                        minimum=0.5, maximum=1.0, value=0.7, step=0.05
                                    )
                                    aug_brightness_max = gr.Slider(
                                        label="Brightness Max",
                                        minimum=1.0, maximum=1.5, value=1.3, step=0.05
                                    )

                                aug_contrast_enable = gr.Checkbox(label="Enable Contrast", value=False)
                                aug_color_jitter_enable = gr.Checkbox(label="Enable Color Jitter", value=False)
                                aug_blur_enable = gr.Checkbox(label="Enable Gaussian Blur", value=False)

                            with gr.Accordion("ğŸ–¼ï¸ Background Replacement", open=True):
                                aug_replace_bg = gr.Checkbox(
                                    label="Enable Background Replacement",
                                    value=True,
                                    info="Replace background with images or solid color"
                                )
                                aug_bg_image_ratio = gr.Slider(
                                    label="Background Image Ratio",
                                    minimum=0.0, maximum=1.0, value=0.5, step=0.1,
                                    info="Probability of using background image (vs solid color)"
                                )
                                aug_bg_folder = gr.Textbox(
                                    label="Background Images Folder",
                                    value=self.config.augmentation_background_folder if self.config else "",
                                    placeholder="/path/to/background/images"
                                )
                                aug_load_bg_btn = gr.Button("ğŸ“‚ Load Background Images", size="sm")
                                aug_bg_status = gr.Markdown("No background images loaded")

                            # Safety options
                            gr.Markdown("#### ğŸ›¡ï¸ Safety Options")
                            aug_prevent_clipping = gr.Checkbox(
                                label="Prevent Object Clipping",
                                value=True,
                                info="Auto-offset to prevent object from being clipped at image boundaries"
                            )

                            # Preview settings
                            gr.Markdown("#### ğŸ‘€ Preview Settings")
                            with gr.Row():
                                aug_preview_rows = gr.Slider(
                                    label="Grid Rows",
                                    minimum=1, maximum=5, value=3, step=1
                                )
                                aug_preview_cols = gr.Slider(
                                    label="Grid Cols",
                                    minimum=1, maximum=5, value=3, step=1
                                )

                            aug_preview_btn = gr.Button("ğŸ” Generate Preview", variant="secondary", size="lg")

                            # Batch augmentation settings
                            gr.Markdown("#### ğŸš€ Batch Augmentation")
                            aug_multiplier = gr.Number(
                                label="Augmentation Multiplier",
                                value=5,
                                minimum=1,
                                maximum=20,
                                step=1,
                                info="Number of augmented versions per sample"
                            )

                            aug_output_dir = gr.Textbox(
                                label="Output Directory",
                                value=str(Path(self.default_output_dir) / "augmented"),
                                placeholder="Output path for augmented data"
                            )

                            aug_apply_btn = gr.Button("âœ¨ Apply Augmentation", variant="primary", size="lg")

                            # Quality Report
                            gr.Markdown("#### ğŸ“Š Quality Analysis")
                            with gr.Row():
                                aug_analyze_btn = gr.Button(
                                    "ğŸ“ˆ Generate Quality Report",
                                    variant="secondary",
                                    size="lg"
                                )

                            with gr.Accordion("âš™ï¸ Analysis Settings", open=False):
                                aug_feature_type = gr.Dropdown(
                                    label="Feature Type",
                                    choices=["simple", "resnet"],
                                    value="simple",
                                    info="Simple: histogram-based, ResNet: deep learning features"
                                )
                                aug_cluster_method = gr.Dropdown(
                                    label="Clustering Method",
                                    choices=["kmeans", "dbscan"],
                                    value="kmeans"
                                )
                                aug_n_clusters = gr.Slider(
                                    label="Number of Clusters",
                                    minimum=2, maximum=10, value=5, step=1
                                )

                        # Right column: Preview & Results
                        with gr.Column(scale=2):
                            gr.Markdown("#### ğŸ–¼ï¸ Preview Grid")
                            aug_preview_display = gr.Image(
                                label="Augmentation Preview",
                                type="numpy",
                                height=600
                            )

                            aug_status = gr.Markdown("Load a session to start")

                            aug_progress = gr.Markdown("")

                    # Event handlers for Data Augmentation

                    # Scan sessions
                    def scan_aug_sessions(session_dir):
                        """Scan for available annotation sessions"""
                        try:
                            session_path = Path(session_dir)
                            if not session_path.exists():
                                return gr.Dropdown(choices=[]), "âŒ Session directory not found"

                            # Find all session files (both session.json and session_metadata.json)
                            sessions = set()  # Use set to avoid duplicates

                            # Search for all session_metadata.json files (both interactive and batch)
                            for session_file in session_path.rglob("session_metadata.json"):
                                try:
                                    with open(session_file, 'r') as f:
                                        metadata = json.load(f)
                                        # Include all sessions regardless of type
                                        # (interactive, batch, or unspecified)
                                        session_dir_path = str(session_file.parent)
                                        sessions.add(session_dir_path)
                                except Exception as e:
                                    # If can't read metadata, still add it
                                    session_dir_path = str(session_file.parent)
                                    sessions.add(session_dir_path)

                            # Also search for legacy session.json files (for backward compatibility)
                            for session_file in session_path.rglob("session.json"):
                                try:
                                    session_dir_path = str(session_file.parent)
                                    # Only add if not already in set from session_metadata.json
                                    if session_dir_path not in sessions:
                                        with open(session_file, 'r') as f:
                                            metadata = json.load(f)
                                            # Skip batch sessions (they should have session_metadata.json)
                                            if metadata.get('session_type') != 'batch':
                                                sessions.add(session_dir_path)
                                except:
                                    # If can't read, add it anyway
                                    session_dir_path = str(session_file.parent)
                                    if session_dir_path not in sessions:
                                        sessions.add(session_dir_path)

                            if not sessions:
                                return gr.Dropdown(choices=[]), "âš ï¸ No sessions found"

                            # Convert set to sorted list
                            session_list = sorted(list(sessions))
                            return gr.Dropdown(choices=session_list), f"âœ… Found {len(session_list)} sessions"
                        except Exception as e:
                            return gr.Dropdown(choices=[]), f"âŒ Error: {str(e)}"

                    aug_scan_btn.click(
                        fn=scan_aug_sessions,
                        inputs=[aug_session_dir],
                        outputs=[aug_session_list, aug_session_info]
                    )

                    # Load background images
                    def load_bg_images(folder_path):
                        """Load background images from folder"""
                        if not folder_path or not Path(folder_path).exists():
                            return f"âŒ Folder not found: {folder_path}"

                        count = self.augmentor.load_background_images(folder_path)
                        if count > 0:
                            return f"âœ… Loaded {count} background images"
                        else:
                            return "âš ï¸ No valid images found (jpg, jpeg, png)"

                    aug_load_bg_btn.click(
                        fn=load_bg_images,
                        inputs=[aug_bg_folder],
                        outputs=[aug_bg_status]
                    )

                    # Load session
                    def load_aug_session(session_path):
                        """Load annotation session for augmentation"""
                        try:
                            if not session_path:
                                return None, "âš ï¸ Please select a session"

                            session_path = Path(session_path)

                            # Try both session metadata formats
                            session_metadata_file = session_path / "session_metadata.json"
                            session_file = session_path / "session.json"

                            metadata = None
                            if session_metadata_file.exists():
                                with open(session_metadata_file, 'r') as f:
                                    metadata = json.load(f)
                            elif session_file.exists():
                                with open(session_file, 'r') as f:
                                    metadata = json.load(f)
                            else:
                                return None, f"âŒ No session metadata found in {session_path}"

                            # Detect format (flat vs Fauna vs Batch)
                            flat_rgb_dir = session_path / "rgb"
                            flat_mask_dir = session_path / "masks"
                            is_flat_format = flat_rgb_dir.exists() and flat_mask_dir.exists()
                            is_batch_format = metadata.get('session_type') == 'batch'

                            # Count frames
                            frame_count = 0
                            if is_batch_format:
                                # Batch format: video_XXX/frame_XXXX/original.png
                                video_dirs = [d for d in session_path.iterdir()
                                              if d.is_dir() and d.name.startswith('video_')]
                                for video_dir in video_dirs:
                                    frame_dirs = [f for f in video_dir.iterdir()
                                                  if f.is_dir() and f.name.startswith('frame_')]
                                    # Count frames with original.png (batch format)
                                    frame_count += len([f for f in frame_dirs
                                                        if (f / "original.png").exists()])
                                format_type = "Batch (video_XXX/frame_XXXX/)"
                            elif is_flat_format:
                                frame_count = len(list(flat_rgb_dir.glob("*.png")))
                                format_type = "Flat (rgb/, masks/)"
                            else:
                                # Fauna format - count frame directories
                                frame_dirs = [d for d in session_path.iterdir() if d.is_dir()]
                                frame_count = len([d for d in frame_dirs if (d / "rgb.png").exists()])
                                format_type = "Fauna (frame directories)"

                            # Store for augmentation
                            self.aug_session_path = session_path
                            self.aug_metadata = metadata

                            session_type = metadata.get('session_type', 'unknown')
                            fauna_compat = metadata.get('fauna_compatible', False)

                            info = f"""
âœ… Session loaded successfully

**Session ID:** {metadata.get('session_id', 'N/A')}
**Type:** {session_type}
**Format:** {format_type}
**Fauna Compatible:** {'âœ…' if fauna_compat else 'âš ï¸'}
**Frames:** {frame_count} frames
**Created:** {metadata.get('timestamp', metadata.get('created_at', 'N/A'))}
"""
                            return None, info
                        except Exception as e:
                            import traceback
                            return None, f"âŒ Error loading session: {str(e)}\n{traceback.format_exc()}"

                    aug_load_session_btn.click(
                        fn=load_aug_session,
                        inputs=[aug_session_list],
                        outputs=[aug_preview_display, aug_session_info]
                    )

                    # Generate preview
                    def generate_aug_preview(
                        rows, cols,
                        scale_enable, scale_min, scale_max, fill_color,
                        crop_enable, crop_scale_min, crop_scale_max,
                        offset_x_max, offset_y_max, crop_padding,
                        rotation_enable, rotation_min, rotation_max,
                        flip_enable,
                        noise_enable, noise_std,
                        brightness_enable, brightness_min, brightness_max,
                        contrast_enable, color_jitter_enable, blur_enable,
                        replace_bg, bg_image_ratio, prevent_clipping
                    ):
                        """Generate augmentation preview grid"""
                        try:
                            if not hasattr(self, 'aug_session_path'):
                                return None, "âŒ Please load a session first"

                            # Load first frame and mask (support flat, Fauna, and Batch formats)
                            rgb_files = []
                            mask_files = []

                            # Check for flat format
                            flat_rgb_dir = self.aug_session_path / "rgb"
                            flat_mask_dir = self.aug_session_path / "masks"

                            # Check for batch format
                            is_batch = hasattr(self, 'aug_metadata') and self.aug_metadata.get('session_type') == 'batch'

                            if flat_rgb_dir.exists() and flat_mask_dir.exists():
                                # Flat format
                                rgb_files = sorted(flat_rgb_dir.glob("*.png"))
                                mask_files = sorted(flat_mask_dir.glob("*.png"))
                            elif is_batch:
                                # Batch format: video_XXX/frame_XXXX/original.png + mask.png
                                video_dirs = sorted([d for d in self.aug_session_path.iterdir()
                                                    if d.is_dir() and d.name.startswith('video_')])
                                for video_dir in video_dirs:
                                    frame_dirs = sorted([f for f in video_dir.iterdir()
                                                        if f.is_dir() and f.name.startswith('frame_')])
                                    for frame_dir in frame_dirs:
                                        rgb_file = frame_dir / "original.png"
                                        mask_file = frame_dir / "mask.png"
                                        if rgb_file.exists() and mask_file.exists():
                                            rgb_files.append(rgb_file)
                                            mask_files.append(mask_file)
                                            break  # Just need first frame for preview
                                    if rgb_files:
                                        break
                            else:
                                # Fauna format
                                frame_dirs = sorted([d for d in self.aug_session_path.iterdir() if d.is_dir()])
                                for frame_dir in frame_dirs:
                                    rgb_file = frame_dir / "rgb.png"
                                    mask_file = frame_dir / "mask.png"
                                    if rgb_file.exists() and mask_file.exists():
                                        rgb_files.append(rgb_file)
                                        mask_files.append(mask_file)
                                        break  # Just need first frame for preview

                            if not rgb_files or not mask_files:
                                return None, "âŒ No RGB or mask files found in session"

                            # Load first frame
                            rgb = cv2.imread(str(rgb_files[0]))
                            rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)

                            mask = cv2.imread(str(mask_files[0]), cv2.IMREAD_GRAYSCALE)
                            mask = mask > 127  # Convert to boolean

                            # Generate random configs
                            num_variations = int(rows * cols)
                            configs = []

                            import random
                            for _ in range(num_variations):
                                config = {'fill_color': fill_color}

                                # Crop-based scale takes precedence over regular scale
                                if crop_enable:
                                    config['crop_scale'] = random.uniform(crop_scale_min, crop_scale_max)
                                    config['crop_offset_x'] = random.uniform(-offset_x_max, offset_x_max)
                                    config['crop_offset_y'] = random.uniform(-offset_y_max, offset_y_max)
                                    config['crop_padding'] = int(crop_padding)
                                elif scale_enable:
                                    config['scale'] = random.uniform(scale_min, scale_max)

                                if rotation_enable:
                                    config['rotation'] = random.uniform(rotation_min, rotation_max)

                                if flip_enable and random.random() > 0.5:
                                    config['flip'] = random.choice(['horizontal', 'vertical'])

                                if noise_enable:
                                    config['noise'] = random.uniform(5, noise_std)

                                if brightness_enable:
                                    config['brightness'] = random.uniform(brightness_min, brightness_max)

                                if contrast_enable:
                                    config['contrast'] = random.uniform(0.8, 1.2)

                                if color_jitter_enable:
                                    config['color_jitter'] = True

                                if blur_enable:
                                    config['blur'] = random.choice([3, 5, 7])

                                # Background replacement
                                if replace_bg:
                                    config['replace_background'] = True
                                    config['use_bg_image'] = True
                                    config['bg_image_ratio'] = bg_image_ratio

                                # Prevent clipping
                                if prevent_clipping:
                                    config['prevent_clipping'] = True

                                configs.append(config)

                            # Generate preview grid
                            grid = self.augmentor.generate_preview_grid(
                                rgb, mask, configs, grid_size=(int(rows), int(cols))
                            )

                            return grid, f"âœ… Preview generated with {num_variations} variations"

                        except Exception as e:
                            import traceback
                            return None, f"âŒ Error: {str(e)}\n{traceback.format_exc()}"

                    aug_preview_btn.click(
                        fn=generate_aug_preview,
                        inputs=[
                            aug_preview_rows, aug_preview_cols,
                            aug_scale_enable, aug_scale_min, aug_scale_max, aug_fill_color,
                            aug_crop_enable, aug_crop_scale_min, aug_crop_scale_max,
                            aug_offset_x_max, aug_offset_y_max, aug_crop_padding,
                            aug_rotation_enable, aug_rotation_min, aug_rotation_max,
                            aug_flip_enable,
                            aug_noise_enable, aug_noise_std,
                            aug_brightness_enable, aug_brightness_min, aug_brightness_max,
                            aug_contrast_enable, aug_color_jitter_enable, aug_blur_enable,
                            aug_replace_bg, aug_bg_image_ratio, aug_prevent_clipping
                        ],
                        outputs=[aug_preview_display, aug_status]
                    )

                    # Apply batch augmentation
                    def apply_batch_augmentation(
                        multiplier, output_dir,
                        scale_enable, scale_min, scale_max, fill_color,
                        crop_enable, crop_scale_min, crop_scale_max,
                        offset_x_max, offset_y_max, crop_padding,
                        rotation_enable, rotation_min, rotation_max,
                        flip_enable,
                        noise_enable, noise_std,
                        brightness_enable, brightness_min, brightness_max,
                        contrast_enable, color_jitter_enable, blur_enable,
                        replace_bg, bg_image_ratio,
                        prevent_clipping_enable=True,
                        progress=gr.Progress()
                    ):
                        """Apply augmentation to all frames in session"""
                        from datetime import datetime

                        try:
                            if not hasattr(self, 'aug_session_path'):
                                return "âŒ Please load a session first", ""

                            progress(0, desc="ğŸ” Loading frames...")

                            output_path = Path(output_dir)
                            output_path.mkdir(parents=True, exist_ok=True)

                            # Load all frames (support flat, Fauna, and Batch formats)
                            rgb_files = []
                            mask_files = []
                            frame_indices = []  # Track original frame index for naming

                            # Check for flat format (rgb/ and masks/ folders)
                            flat_rgb_dir = self.aug_session_path / "rgb"
                            flat_mask_dir = self.aug_session_path / "masks"

                            # Check for batch format
                            is_batch = hasattr(self, 'aug_metadata') and self.aug_metadata.get('session_type') == 'batch'

                            if flat_rgb_dir.exists() and flat_mask_dir.exists():
                                # Flat format
                                rgb_files = sorted(flat_rgb_dir.glob("*.png"))
                                mask_files = sorted(flat_mask_dir.glob("*.png"))
                                frame_indices = list(range(len(rgb_files)))
                            elif is_batch:
                                # Batch format: video_XXX/frame_XXXX/original.png + mask.png
                                global_idx = 0
                                video_dirs = sorted([d for d in self.aug_session_path.iterdir()
                                                    if d.is_dir() and d.name.startswith('video_')])
                                for video_dir in video_dirs:
                                    frame_dirs = sorted([f for f in video_dir.iterdir()
                                                        if f.is_dir() and f.name.startswith('frame_')])
                                    for frame_dir in frame_dirs:
                                        rgb_file = frame_dir / "original.png"
                                        mask_file = frame_dir / "mask.png"
                                        if rgb_file.exists() and mask_file.exists():
                                            rgb_files.append(rgb_file)
                                            mask_files.append(mask_file)
                                            frame_indices.append(global_idx)
                                            global_idx += 1
                            else:
                                # Fauna format (frame directories)
                                frame_dirs = sorted([d for d in self.aug_session_path.iterdir() if d.is_dir()])
                                for idx, frame_dir in enumerate(frame_dirs):
                                    rgb_file = frame_dir / "rgb.png"
                                    mask_file = frame_dir / "mask.png"
                                    if rgb_file.exists() and mask_file.exists():
                                        rgb_files.append(rgb_file)
                                        mask_files.append(mask_file)
                                        frame_indices.append(idx)

                            total_frames = len(rgb_files)
                            total_outputs = total_frames * int(multiplier)

                            progress(0.05, desc=f"ğŸš€ Processing {total_frames} frames Ã— {int(multiplier)} = {total_outputs} outputs...")

                            import random
                            processed = 0

                            for idx, (rgb_file, mask_file) in enumerate(zip(rgb_files, mask_files)):
                                # Load frame
                                rgb = cv2.imread(str(rgb_file))
                                rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)

                                mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)
                                mask = mask > 127

                                # Get frame index for naming (use tracked index if available)
                                frame_idx = frame_indices[idx] if idx < len(frame_indices) else idx

                                # Generate augmentations
                                for aug_idx in range(int(multiplier)):
                                    # Random config
                                    config = {'fill_color': fill_color}

                                    # Crop-based scale takes precedence
                                    if crop_enable:
                                        config['crop_scale'] = random.uniform(crop_scale_min, crop_scale_max)
                                        config['crop_offset_x'] = random.uniform(-offset_x_max, offset_x_max)
                                        config['crop_offset_y'] = random.uniform(-offset_y_max, offset_y_max)
                                        config['crop_padding'] = int(crop_padding)
                                    elif scale_enable:
                                        config['scale'] = random.uniform(scale_min, scale_max)

                                    if rotation_enable:
                                        config['rotation'] = random.uniform(rotation_min, rotation_max)

                                    if flip_enable and random.random() > 0.5:
                                        config['flip'] = random.choice(['horizontal', 'vertical'])

                                    if noise_enable:
                                        config['noise'] = random.uniform(5, noise_std)

                                    if brightness_enable:
                                        config['brightness'] = random.uniform(brightness_min, brightness_max)

                                    if contrast_enable:
                                        config['contrast'] = random.uniform(0.8, 1.2)

                                    if color_jitter_enable:
                                        config['color_jitter'] = True

                                    if blur_enable:
                                        config['blur'] = random.choice([3, 5, 7])

                                    # Background replacement
                                    if replace_bg:
                                        config['replace_background'] = True
                                        config['use_bg_image'] = True
                                        config['bg_image_ratio'] = bg_image_ratio

                                    # Prevent clipping option
                                    if prevent_clipping_enable:
                                        config['prevent_clipping'] = True

                                    # Apply augmentation
                                    aug_rgb, aug_mask, applied = self.augmentor.augment(rgb, mask, config)

                                    # Save in Fauna-compatible format (frame directories)
                                    # Use frame index for unique naming (avoids overwrite when all files are original.png)
                                    frame_dir_name = f"frame_{frame_idx:04d}_aug{aug_idx:02d}"
                                    frame_dir = output_path / frame_dir_name
                                    frame_dir.mkdir(parents=True, exist_ok=True)

                                    # Save RGB as rgb.png
                                    rgb_bgr = cv2.cvtColor(aug_rgb, cv2.COLOR_RGB2BGR)
                                    cv2.imwrite(str(frame_dir / "rgb.png"), rgb_bgr)

                                    # Save mask as mask.png
                                    mask_img = (aug_mask * 255).astype(np.uint8)
                                    cv2.imwrite(str(frame_dir / "mask.png"), mask_img)

                                    processed += 1

                                # Update progress
                                progress_pct = 0.05 + 0.90 * (idx + 1) / total_frames
                                progress(progress_pct, desc=f"â³ Frame {idx + 1}/{total_frames} ({processed}/{total_outputs} outputs)")

                            # Save metadata
                            metadata = {
                                'session_id': f"augmented_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                                'session_type': 'augmented',
                                'source_session': str(self.aug_session_path),
                                'original_frames': total_frames,
                                'multiplier': int(multiplier),
                                'total_augmented': processed,
                                'fauna_compatible': True,
                                'augmentation_params': {
                                    'crop_scale': {'enabled': crop_enable, 'min': crop_scale_min, 'max': crop_scale_max} if crop_enable else None,
                                    'scale': {'enabled': scale_enable, 'min': scale_min, 'max': scale_max} if scale_enable else None,
                                    'rotation': {'enabled': rotation_enable, 'min': rotation_min, 'max': rotation_max} if rotation_enable else None,
                                    'flip': flip_enable,
                                    'noise': {'enabled': noise_enable, 'std': noise_std} if noise_enable else None,
                                    'brightness': {'enabled': brightness_enable, 'min': brightness_min, 'max': brightness_max} if brightness_enable else None,
                                    'contrast': contrast_enable,
                                    'color_jitter': color_jitter_enable,
                                    'blur': blur_enable,
                                    'fill_color': fill_color,
                                    'offset_x_max': offset_x_max,
                                    'offset_y_max': offset_y_max,
                                    'crop_padding': int(crop_padding),
                                    'prevent_clipping': prevent_clipping_enable
                                },
                                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            }

                            progress(1.0, desc="âœ… Complete!")

                            # Save as both augmentation_metadata.json and session_metadata.json
                            with open(output_path / "augmentation_metadata.json", 'w') as f:
                                json.dump(metadata, f, indent=2)

                            with open(output_path / "session_metadata.json", 'w') as f:
                                json.dump(metadata, f, indent=2)

                            final_msg = f"""
âœ… Augmentation complete!

**Original frames:** {total_frames}
**Multiplier:** {int(multiplier)}Ã—
**Total generated:** {processed} augmented samples

**Output location:**
`{output_path}`
"""
                            return final_msg, f"Saved to: {output_path}"

                        except Exception as e:
                            import traceback
                            return f"âŒ Error: {str(e)}\n{traceback.format_exc()}", ""

                    aug_apply_btn.click(
                        fn=apply_batch_augmentation,
                        inputs=[
                            aug_multiplier, aug_output_dir,
                            aug_scale_enable, aug_scale_min, aug_scale_max, aug_fill_color,
                            aug_crop_enable, aug_crop_scale_min, aug_crop_scale_max,
                            aug_offset_x_max, aug_offset_y_max, aug_crop_padding,
                            aug_rotation_enable, aug_rotation_min, aug_rotation_max,
                            aug_flip_enable,
                            aug_noise_enable, aug_noise_std,
                            aug_brightness_enable, aug_brightness_min, aug_brightness_max,
                            aug_contrast_enable, aug_color_jitter_enable, aug_blur_enable,
                            aug_replace_bg, aug_bg_image_ratio,
                            aug_prevent_clipping
                        ],
                        outputs=[aug_status, aug_progress]
                    )

                    # Generate quality report
                    def generate_quality_report(
                        output_dir, feature_type, cluster_method, n_clusters
                    ):
                        """Generate HTML quality report for augmented images"""
                        try:
                            from feature_clustering import analyze_augmentation_quality
                            from html_report_generator import generate_html_report

                            output_path = Path(output_dir)
                            if not output_path.exists():
                                return "âŒ Output directory not found. Please run augmentation first."

                            # Find all RGB images (support both flat and Fauna formats)
                            image_paths = []

                            # Check for flat format
                            rgb_dir = output_path / "rgb"
                            if rgb_dir.exists():
                                # Flat format
                                image_paths = list(rgb_dir.glob("*.png")) + list(rgb_dir.glob("*.jpg"))
                            else:
                                # Fauna format - collect rgb.png from all frame directories
                                frame_dirs = [d for d in output_path.iterdir() if d.is_dir()]
                                for frame_dir in frame_dirs:
                                    rgb_file = frame_dir / "rgb.png"
                                    if rgb_file.exists():
                                        image_paths.append(rgb_file)

                            if len(image_paths) < 2:
                                return f"âŒ Not enough images for analysis ({len(image_paths)} found, need at least 2)"

                            # Run analysis
                            msg = f"ğŸ” Analyzing {len(image_paths)} images...\n"
                            results = analyze_augmentation_quality(
                                image_paths=image_paths,
                                output_dir=output_path,
                                feature_type=feature_type,
                                cluster_method=cluster_method,
                                n_clusters=int(n_clusters),
                                vis_method='tsne'
                            )

                            # Generate HTML report
                            html_path = output_path / "quality_report.html"
                            generate_html_report(
                                results=results,
                                output_path=html_path,
                                include_images=True,
                                max_images_per_cluster=5
                            )

                            metrics = results['metrics']
                            msg += f"\nâœ… Analysis complete!\n\n"
                            msg += f"**Metrics:**\n"
                            msg += f"- Clusters: {metrics['n_clusters']}\n"
                            msg += f"- Silhouette Score: {metrics.get('silhouette_score', 0):.3f}\n"
                            msg += f"- Davies-Bouldin Score: {metrics.get('davies_bouldin_score', 0):.3f}\n"
                            msg += f"\nğŸ“„ **Report saved:** {html_path}\n"
                            msg += f"Open in browser to view interactive results."

                            return msg

                        except Exception as e:
                            import traceback
                            return f"âŒ Error: {str(e)}\n{traceback.format_exc()}"

                    aug_analyze_btn.click(
                        fn=generate_quality_report,
                        inputs=[
                            aug_output_dir,
                            aug_feature_type,
                            aug_cluster_method,
                            aug_n_clusters
                        ],
                        outputs=[aug_status]
                    )

        return demo

def main():
    """ì›¹ ì•± ì‹¤í–‰"""
    import os
    import socket

    app = SAMInteractiveWebApp()
    demo = app.create_interface()

    # í¬íŠ¸ ì„¤ì •: í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” 7860-7900 ë²”ìœ„ì—ì„œ ìë™ ì„ íƒ
    start_port = int(os.getenv("GRADIO_SERVER_PORT", "7860"))

    # ì‚¬ìš© ê°€ëŠ¥í•œ í¬íŠ¸ ì°¾ê¸°
    def find_free_port(start, end=None):
        """Find a free port in the range [start, end]"""
        if end is None:
            end = start + 40  # 7860-7900

        for port in range(start, end + 1):
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.bind(('', port))
                    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    return port
            except OSError:
                continue
        return None

    port = find_free_port(start_port)
    if port is None:
        print(f"âŒ Cannot find free port in range {start_port}-{start_port + 40}")
        print("ğŸ’¡ Kill existing processes: pkill -f web_app.py")
        return

    print(f"âœ“ Using port: {port}")

    demo.launch(
        server_name="0.0.0.0",
        server_port=port,
        share=False,
        debug=True,
        max_threads=40  # ë™ì‹œ ì²˜ë¦¬ ì¦ê°€
    )

if __name__ == "__main__":
    main()
