#!/usr/bin/env python3
"""
SAM 3D GUI - Interactive SAM Annotation Web Interface
ëŒ€í™”í˜• SAM annotation: point í´ë¦­ìœ¼ë¡œ fg/bg ì§€ì • + ë¹„ë””ì˜¤ propagation
"""

import sys
import gradio as gr
import numpy as np
import cv2
import torch
from pathlib import Path
from typing import Optional, Tuple, List, Dict
import json
import os

# Add src directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

# Set environment variable to skip SAM3D init (which requires missing module)
os.environ['LIDRA_SKIP_INIT'] = '1'

from sam3d_processor import SAM3DProcessor
from config_loader import ModelConfig
from lite_annotator import LiteAnnotator
from augmentation import DataAugmentor, generate_augmentation_configs

# Load configuration
try:
    config = ModelConfig()
    print(f"âœ“ Config loaded from: {Path(__file__).parent.parent / 'config' / 'model_config.yaml'}")
except Exception as e:
    print(f"Warning: Failed to load config: {e}")
    config = None

# SAM 2 imports
SAM2_PATH = Path.home() / 'dev/segment-anything-2'
if SAM2_PATH.exists():
    sys.path.insert(0, str(SAM2_PATH))
    from sam2.sam2_image_predictor import SAM2ImagePredictor
    from sam2.sam2_video_predictor import SAM2VideoPredictor
    SAM2_AVAILABLE = True
else:
    SAM2ImagePredictor = None
    SAM2VideoPredictor = None
    SAM2_AVAILABLE = False
    print("Warning: SAM 2 not found. Interactive segmentation will use fallback method.")

class SAMInteractiveWebApp:
    """
    SAM 3D GUI - í†µí•© ì›¹ ì¸í„°í˜ì´ìŠ¤

    ëª¨ë“œ 1: ìë™ ì²˜ë¦¬ (Quick Mode)
    - ë¹„ë””ì˜¤ ì„ íƒ â†’ ìë™ ì„¸ê·¸ë©˜í…Œì´ì…˜ â†’ ëª¨ì…˜ ê°ì§€ â†’ ê²°ê³¼

    ëª¨ë“œ 2: ëŒ€í™”í˜• Annotation (Interactive Mode)
    - Point annotation (foreground/background)
    - ìˆ˜ë™ ì„¸ê·¸ë©˜í…Œì´ì…˜ â†’ Propagation â†’ 3D mesh
    """

    def __init__(self):
        # Config-based initialization
        self.config = config

        # SAM 3D processor ì´ˆê¸°í™”
        if config:
            sam3d_checkpoint = config.sam3d_checkpoint_dir
            self.processor = SAM3DProcessor(sam3d_checkpoint_path=sam3d_checkpoint)
        else:
            self.processor = SAM3DProcessor()

        # SAM2 predictor ì´ˆê¸°í™” (Interactive Modeìš©)
        self.sam2_predictor = None
        self.sam2_video_predictor = None
        self.sam2_device = None
        if SAM2_AVAILABLE and config:
            try:
                print("Loading SAM 2 for interactive segmentation...")
                checkpoint = Path(config.sam2_checkpoint)
                model_cfg = config.sam2_config
                device = config.sam2_device

                # Auto-detect device
                if device == "auto":
                    if torch.cuda.is_available():
                        device = "cuda"
                        gpu_name = torch.cuda.get_device_name(0)
                        print(f"âœ“ CUDA detected: {gpu_name}")
                    else:
                        device = "cpu"
                        print("Warning: CUDA not available, using CPU")
                elif device == "cuda" and not torch.cuda.is_available():
                    device = "cpu"
                    print("Warning: CUDA not available, using CPU")

                self.sam2_device = device

                if checkpoint.exists():
                    from sam2.build_sam import build_sam2, build_sam2_video_predictor

                    # Image predictor for single-frame segmentation
                    sam2_model = build_sam2(model_cfg, str(checkpoint), device=device)
                    self.sam2_predictor = SAM2ImagePredictor(sam2_model)

                    # Video predictor for memory-based tracking
                    self.sam2_video_predictor = build_sam2_video_predictor(model_cfg, str(checkpoint), device=device)

                    print(f"âœ“ SAM 2 loaded: {config.cfg.sam2.name} on {device}")
                    print(f"âœ“ SAM 2 Video Predictor initialized for propagation")
                else:
                    print(f"Warning: SAM 2 checkpoint not found at {checkpoint}")
            except Exception as e:
                print(f"Warning: Failed to load SAM 2: {e}")
                import traceback
                traceback.print_exc()
                self.sam2_predictor = None
                self.sam2_video_predictor = None

        # ìƒíƒœ ê´€ë¦¬
        self.video_path = None
        self.frames = []
        self.current_frame_idx = 0
        self.annotations = {
            'foreground': [],  # [(x, y), ...]
            'background': []   # [(x, y), ...]
        }
        self.masks = []  # ê° í”„ë ˆì„ì˜ ë§ˆìŠ¤í¬
        self.current_mask = None
        self.tracking_result = None

        # Default paths from config
        if config:
            self.default_data_dir = config.default_data_dir
            self.default_output_dir = config.output_dir
        else:
            self.default_data_dir = "/home/joon/dev/data/markerless_mouse/"
            self.default_output_dir = "/home/joon/dev/sam3d_gui/outputs/"

        # Data Augmentor ì´ˆê¸°í™”
        self.augmentor = DataAugmentor()
        self.augmentation_preview = None

        # LiteAnnotator ì´ˆê¸°í™” (Tab 3: Lite Mode)
        self.lite_annotator = None
        if SAM2_AVAILABLE:
            try:
                print("Initializing Lite Annotator...")
                self.lite_annotator = LiteAnnotator(
                    sam2_base_path=SAM2_PATH,
                    device=self.sam2_device if self.sam2_device else "auto"
                )
                print("âœ“ Lite Annotator initialized")
            except Exception as e:
                print(f"Warning: Failed to initialize Lite Annotator: {e}")
                self.lite_annotator = None

    def unload_sam2_models(self):
        """
        Unload SAM2 models to free GPU memory before SAM 3D inference

        This is critical for RTX 3060 12GB where:
        - SAM2 Large uses ~2-3GB
        - SAM3D uses ~8-10GB
        - Total 11-13GB > 12GB available

        By unloading SAM2 before SAM3D, we free ~3GB for SAM3D inference.
        """
        import gc

        if self.sam2_predictor is not None or self.sam2_video_predictor is not None:
            print("\nğŸ§¹ SAM2 ëª¨ë¸ ì–¸ë¡œë“œ ì‹œì‘ (ë©”ëª¨ë¦¬ í™•ë³´)...")

            # Print memory before cleanup
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                print(f"   í˜„ì¬ GPU ë©”ëª¨ë¦¬: {allocated:.2f} GB")

            # Delete SAM2 image predictor
            if self.sam2_predictor is not None:
                del self.sam2_predictor
                self.sam2_predictor = None
                print("   âœ“ SAM2 Image Predictor í•´ì œ")

            # Delete SAM2 video predictor
            if self.sam2_video_predictor is not None:
                del self.sam2_video_predictor
                self.sam2_video_predictor = None
                print("   âœ“ SAM2 Video Predictor í•´ì œ")

            # Force garbage collection
            gc.collect()

            # Clear CUDA cache
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                print("   âœ“ CUDA ìºì‹œ ì •ë¦¬ ì™„ë£Œ")

                # Print memory after cleanup
                allocated_after = torch.cuda.memory_allocated(0) / 1024**3
                freed = allocated - allocated_after
                print(f"   âœ“ GPU ë©”ëª¨ë¦¬ í•´ì œ: {freed:.2f} GB")
                print(f"   í˜„ì¬ GPU ë©”ëª¨ë¦¬: {allocated_after:.2f} GB")

            print("âœ… SAM2 ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ\n")
        else:
            print("â„¹ï¸  SAM2 ëª¨ë¸ì´ ì´ë¯¸ ì–¸ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    def reload_sam2_models(self):
        """
        Reload SAM2 models after SAM 3D inference completes

        This allows users to continue using interactive segmentation after 3D reconstruction.
        """
        if not SAM2_AVAILABLE or not self.config:
            print("âš ï¸  SAM2ë¥¼ ë‹¤ì‹œ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (SAM2 unavailable or no config)")
            return

        if self.sam2_predictor is not None and self.sam2_video_predictor is not None:
            print("â„¹ï¸  SAM2 ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        print("\nğŸ”„ SAM2 ëª¨ë¸ ì¬ë¡œë“œ ì¤‘...")

        try:
            checkpoint = Path(self.config.sam2_checkpoint)
            model_cfg = self.config.sam2_config
            device = self.sam2_device

            if checkpoint.exists():
                from sam2.build_sam import build_sam2, build_sam2_video_predictor

                # Rebuild models
                sam2_model = build_sam2(model_cfg, str(checkpoint), device=device)
                self.sam2_predictor = SAM2ImagePredictor(sam2_model)
                self.sam2_video_predictor = build_sam2_video_predictor(model_cfg, str(checkpoint), device=device)

                print(f"âœ… SAM2 ëª¨ë¸ ì¬ë¡œë“œ ì™„ë£Œ (device: {device})\n")
            else:
                print(f"âŒ SAM2 checkpointë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint}")
        except Exception as e:
            print(f"âŒ SAM2 ì¬ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

    def quick_process(self, data_dir: str, video_file: str,
                     start_time: float, duration: float,
                     motion_threshold: float, segmentation_method: str,
                     progress=gr.Progress()) -> Tuple[np.ndarray, str]:
        """
        Quick Mode: ìë™ ì²˜ë¦¬ (ê¸°ì¡´ web_app.py ê¸°ëŠ¥ í†µí•©)
        """
        if not video_file:
            return None, "ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”"

        video_path = Path(data_dir) / video_file
        if not video_path.exists():
            return None, f"ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}"

        self.video_path = str(video_path)

        progress(0, desc="ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘...")

        try:
            # ë¹„ë””ì˜¤ ì²˜ë¦¬
            result, reconstruction = self.processor.process_video_segment(
                video_path=self.video_path,
                start_time=start_time,
                duration=duration,
                output_dir="outputs/",
                motion_threshold=motion_threshold,
                segmentation_method=segmentation_method
            )

            self.tracking_result = result

            progress(0.5, desc="ê²°ê³¼ ìƒì„± ì¤‘...")

            # ê²°ê³¼ ì‹œê°í™”
            if result.segments:
                first_frame = result.segments[0]

                # í”„ë ˆì„ ë‹¤ì‹œ ì½ê¸°
                cap = cv2.VideoCapture(self.video_path)
                cap.set(cv2.CAP_PROP_POS_FRAMES, first_frame.frame_idx)
                ret, frame = cap.read()
                cap.release()

                if ret:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    mask_colored = np.zeros_like(frame_rgb)
                    mask_colored[first_frame.mask > 0] = [0, 255, 0]
                    overlay = cv2.addWeighted(frame_rgb, 0.7, mask_colored, 0.3, 0)

                    if first_frame.bbox:
                        x1, y1, x2, y2 = first_frame.bbox
                        cv2.rectangle(overlay, (x1, y1), (x2, y2), (255, 0, 0), 2)

                    visualization = overlay
                else:
                    visualization = None
            else:
                visualization = None

            progress(0.8, desc="í†µê³„ ê³„ì‚° ì¤‘...")

            # ê²°ê³¼ í…ìŠ¤íŠ¸
            result_text = f"""
### ğŸ¯ Quick Process ì™„ë£Œ

**ê¸°ë³¸ ì •ë³´**
- ë¶„ì„ëœ í”„ë ˆì„: {len(result.segments)}
- ì²˜ë¦¬ ì‹œê°„: {start_time}s - {start_time + duration}s

**ëª¨ì…˜ ê°ì§€**
- ê°ì§€ ì—¬ë¶€: {'âœ… ì˜ˆ' if result.motion_detected else 'âŒ ì•„ë‹ˆì˜¤'}
- ì„ê³„ê°’: {motion_threshold} í”½ì…€
"""

            if result.motion_detected and len(result.segments) > 1:
                displacements = []
                for i in range(1, len(result.segments)):
                    prev = result.segments[i-1].center
                    curr = result.segments[i].center
                    dx = curr[0] - prev[0]
                    dy = curr[1] - prev[1]
                    disp = (dx**2 + dy**2)**0.5
                    displacements.append(disp)

                if displacements:
                    result_text += f"""
**ë³€ìœ„ í†µê³„**
- ìµœëŒ€ ë³€ìœ„: {max(displacements):.1f} í”½ì…€
- í‰ê·  ë³€ìœ„: {sum(displacements)/len(displacements):.1f} í”½ì…€
"""

            if result.segments:
                first = result.segments[0]
                result_text += f"""
**ê°ì²´ ì •ë³´**
- ë°”ìš´ë”© ë°•ìŠ¤: {first.bbox}
- ì¤‘ì‹¬ì : {first.center}
- ë©´ì : {first.area:.0f} í”½ì…€Â²

**ì¶œë ¥**
- ì €ì¥ ìœ„ì¹˜: `outputs/`
"""

            progress(1.0, desc="ì™„ë£Œ!")

            return visualization, result_text

        except Exception as e:
            import traceback
            error_msg = f"ì˜¤ë¥˜:\n```\n{str(e)}\n{traceback.format_exc()}\n```"
            return None, error_msg

    def scan_videos(self, data_dir: str) -> List[str]:
        """ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ ìŠ¤ìº”"""
        data_path = Path(data_dir)
        if not data_path.exists():
            return []

        video_extensions = ['.mp4', '.avi', '.mov', '.mkv']
        videos = []

        for ext in video_extensions:
            videos.extend([str(p.relative_to(data_path))
                          for p in data_path.rglob(f'*{ext}')])

        return sorted(videos)

    def calculate_stride_from_target(self, target_frames: int) -> int:
        """
        ëª©í‘œ í”„ë ˆì„ ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ stride ìë™ ê³„ì‚°

        Args:
            target_frames: ì²˜ë¦¬í•  ëª©í‘œ í”„ë ˆì„ ìˆ˜

        Returns:
            ê³„ì‚°ëœ stride ê°’
        """
        if len(self.frames) == 0:
            return 1

        stride = max(1, len(self.frames) // target_frames)
        return stride

    def scan_batch_videos(self, data_dir: str, pattern: str = "*.mp4") -> Tuple[List[str], str, gr.CheckboxGroup]:
        """
        í´ë” ë‚´ ëª¨ë“  ë¹„ë””ì˜¤ ìŠ¤ìº” ë° ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ (recursive)
        í´ë”ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‘œì‹œ

        Args:
            data_dir: ë¹„ë””ì˜¤ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬
            pattern: ë¹„ë””ì˜¤ íŒŒì¼ íŒ¨í„´ (ì˜ˆ: *.mp4, *.avi)

        Returns:
            (ë¹„ë””ì˜¤ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸, ìƒíƒœ ë©”ì‹œì§€, CheckboxGroup ì—…ë°ì´íŠ¸)
        """
        try:
            data_path = Path(data_dir)
            if not data_path.exists():
                empty_checkbox = gr.CheckboxGroup(choices=[], value=[])
                return [], f"âŒ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}", empty_checkbox

            # ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸° (recursive)
            video_files = sorted(data_path.rglob(pattern))

            if not video_files:
                empty_checkbox = gr.CheckboxGroup(choices=[], value=[])
                return [], f"âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pattern} (recursive íƒìƒ‰)", empty_checkbox

            # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            total_frames = 0
            total_duration = 0
            video_info = []

            for video_path in video_files:
                try:
                    info = self.processor.get_video_info(str(video_path))
                    total_frames += info['frame_count']
                    total_duration += info['duration']
                    video_info.append({
                        'path': str(video_path),
                        'name': video_path.name,
                        'frames': info['frame_count'],
                        'duration': info['duration'],
                        'fps': info['fps'],
                        'resolution': f"{info['width']}x{info['height']}"
                    })
                except Exception as e:
                    print(f"âš ï¸ {video_path.name} ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {e}")
                    continue

            # í‰ê·  FPS ê³„ì‚°
            avg_fps = total_frames / total_duration if total_duration > 0 else 0

            # ìƒíƒœ ë©”ì‹œì§€ ìƒì„±
            status = f"""
### ğŸ“‚ Batch ë¹„ë””ì˜¤ ìŠ¤ìº” ì™„ë£Œ âœ…

- **ë¹„ë””ì˜¤ íŒŒì¼ ìˆ˜**: {len(video_info)}
- **ì´ í”„ë ˆì„ ìˆ˜**: {total_frames:,}
- **ì´ ê¸¸ì´**: {total_duration:.1f}ì´ˆ ({total_duration/60:.1f}ë¶„)
- **í‰ê·  FPS**: {avg_fps:.1f}

<details>
<summary><b>ğŸ“‹ ë¹„ë””ì˜¤ ëª©ë¡ ({len(video_info)}ê°œ) - í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°/ì ‘ê¸°</b></summary>

"""
            for idx, info in enumerate(video_info, 1):
                # ì „ì²´ ìƒëŒ€ ê²½ë¡œ í‘œì‹œ
                rel_path = str(Path(info['path']).relative_to(data_path))
                status += f"\n{idx}. **{rel_path}**"
                status += f"\n   - í”„ë ˆì„: {info['frames']}, ê¸¸ì´: {info['duration']:.1f}ì´ˆ, FPS: {info['fps']:.1f}, í•´ìƒë„: {info['resolution']}\n"

            status += "\n</details>"

            # ë¹„ë””ì˜¤ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            video_paths = [str(v) for v in video_files]
            self.batch_videos = video_paths
            self.batch_video_info = video_info

            # CheckboxGroup ì—…ë°ì´íŠ¸ - ì „ì²´ ìƒëŒ€ ê²½ë¡œë¡œ í‘œì‹œ (ê³ ìœ í•˜ê²Œ)
            video_relative_paths = [str(Path(info['path']).relative_to(data_path)) for info in video_info]

            # ì „ì²´ ê³„ì¸µì  ê²½ë¡œë¥¼ ë ˆì´ë¸”ë¡œ ì‚¬ìš©
            video_labels = video_relative_paths  # ì „ì²´ ê²½ë¡œ ì‚¬ìš©

            updated_checkbox = gr.CheckboxGroup(
                choices=video_labels,
                value=video_labels,  # ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë‘ ì„ íƒ
                label="ğŸ¬ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ì„ íƒ (ê³„ì¸µì  ê²½ë¡œ)",
                info="ì„ íƒëœ ë¹„ë””ì˜¤ë§Œ ì²˜ë¦¬ë©ë‹ˆë‹¤"
            )

            # ë ˆì´ë¸”ê³¼ ì‹¤ì œ ê²½ë¡œ ë§¤í•‘ ì €ì¥
            self.batch_video_label_map = dict(zip(video_labels, video_paths))

            return video_paths, status, updated_checkbox

        except Exception as e:
            import traceback
            empty_checkbox = gr.CheckboxGroup(choices=[], value=[])
            return [], f"âŒ ìŠ¤ìº” ì‹¤íŒ¨:\n{str(e)}\n{traceback.format_exc()}", empty_checkbox

    def batch_load_reference_frame(self, selected_videos: List[str]) -> Tuple[np.ndarray, str]:
        """
        Batch ëª¨ë“œì—ì„œ ì„ íƒëœ ë¹„ë””ì˜¤ ì¤‘ ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ì˜ ì²« í”„ë ˆì„ì„ referenceë¡œ ë¡œë“œ

        Args:
            selected_videos: ì„ íƒëœ ë¹„ë””ì˜¤ ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸

        Returns:
            (reference_frame, status_message)
        """
        if not hasattr(self, 'batch_videos') or not self.batch_videos:
            return None, "âŒ ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ìŠ¤ìº”í•˜ì„¸ìš”"

        if not selected_videos or len(selected_videos) == 0:
            return None, "âŒ ìµœì†Œ 1ê°œì˜ ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”"

        try:
            # ì„ íƒëœ ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ì˜ ì‹¤ì œ ê²½ë¡œ ì°¾ê¸°
            first_selected_label = selected_videos[0]

            if hasattr(self, 'batch_video_label_map') and first_selected_label in self.batch_video_label_map:
                first_video_path = self.batch_video_label_map[first_selected_label]
            else:
                # Fallback: ì „ì²´ ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸
                first_video_path = self.batch_videos[0]

            # ì²« í”„ë ˆì„ ì¶”ì¶œ
            frames = self.processor.extract_frames(
                first_video_path,
                start_frame=0,
                num_frames=1,
                stride=1
            )

            if not frames:
                return None, "âŒ Reference í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨"

            # í”„ë ˆì„ ì €ì¥ (annotationìš©)
            self.frames = frames
            self.current_frame_idx = 0
            self.annotations = {'foreground': [], 'background': []}
            self.masks = [None] * len(frames)

            # RGB ë³€í™˜ (GradioëŠ” RGB ì‚¬ìš©)
            frame_rgb = cv2.cvtColor(frames[0], cv2.COLOR_BGR2RGB)

            status = f"""
### âœ… Reference í”„ë ˆì„ ë¡œë“œ ì™„ë£Œ

- **ì„ íƒëœ ë¹„ë””ì˜¤**: {len(selected_videos)}ê°œ ì¤‘ ì²« ë²ˆì§¸
- **Reference ë¹„ë””ì˜¤**: {first_selected_label}
- **íŒŒì¼ëª…**: {Path(first_video_path).name}
- **í•´ìƒë„**: {frame_rgb.shape[1]} x {frame_rgb.shape[0]}

ì´ì œ ì´ë¯¸ì§€ë¥¼ í´ë¦­í•˜ì—¬ annotationì„ ì¶”ê°€í•˜ì„¸ìš”.
"""

            return frame_rgb, status

        except Exception as e:
            import traceback
            return None, f"âŒ Reference í”„ë ˆì„ ë¡œë“œ ì‹¤íŒ¨:\n{str(e)}\n{traceback.format_exc()}"

    def batch_propagate_videos(
        self,
        target_frames: int = 100,
        selected_videos: List[str] = None,
        progress=gr.Progress()
    ) -> Tuple[str, str]:
        """
        ì—¬ëŸ¬ ë¹„ë””ì˜¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (Batch Processing)

        ê° ë¹„ë””ì˜¤ë§ˆë‹¤:
        1. ë¡œë“œ (ëª©í‘œ í”„ë ˆì„ ìˆ˜ì— ë§ì¶° stride ìë™ ê³„ì‚°)
        2. í˜„ì¬ annotationìœ¼ë¡œ propagate
        3. ê²°ê³¼ ì„ì‹œ ì €ì¥
        4. ë©”ëª¨ë¦¬ í•´ì œ

        Args:
            target_frames: ê° ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œí•  ëª©í‘œ í”„ë ˆì„ ìˆ˜
            selected_videos: ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´)
            progress: Gradio progress bar

        Returns:
            (ìƒíƒœ ë©”ì‹œì§€, ì™„ë£Œ ë©”ì‹œì§€)
        """
        if not hasattr(self, 'batch_videos') or not self.batch_videos:
            return "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ìŠ¤ìº”í•˜ì„¸ìš”", "âŒ ë¹„ë””ì˜¤ ì—†ìŒ"

        if len(self.annotations['foreground']) == 0:
            return "Annotationì´ í•„ìš”í•©ë‹ˆë‹¤ (ìµœì†Œ 1ê°œì˜ foreground point)", "âŒ Annotation ì—†ìŒ"

        try:
            import tempfile
            import shutil
            import torch

            # ì„ì‹œ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            batch_temp_dir = Path(tempfile.mkdtemp(prefix="sam3d_batch_"))

            # Reference annotation ì €ì¥
            reference_annotations = {
                'foreground': self.annotations['foreground'].copy(),
                'background': self.annotations['background'].copy()
            }

            # ì„ íƒëœ ë¹„ë””ì˜¤ í•„í„°ë§ (ë ˆì´ë¸” â†’ ê²½ë¡œ ë§¤í•‘ ì‚¬ìš©)
            if selected_videos and len(selected_videos) > 0:
                # ì„ íƒëœ ë ˆì´ë¸”ì„ ì‹¤ì œ ê²½ë¡œë¡œ ë³€í™˜
                videos_to_process = []
                if hasattr(self, 'batch_video_label_map'):
                    for label in selected_videos:
                        if label in self.batch_video_label_map:
                            videos_to_process.append(self.batch_video_label_map[label])
                else:
                    # ë ˆì´ë¸” ë§µì´ ì—†ìœ¼ë©´ ì´ë¦„ìœ¼ë¡œ ë§¤ì¹­ (í•˜ìœ„ í˜¸í™˜ì„±)
                    for video_path in self.batch_videos:
                        video_name = Path(video_path).name
                        if video_name in selected_videos:
                            videos_to_process.append(video_path)
            else:
                # ì„ íƒì´ ì—†ìœ¼ë©´ ëª¨ë“  ë¹„ë””ì˜¤ ì²˜ë¦¬
                videos_to_process = self.batch_videos

            if not videos_to_process:
                return "ì²˜ë¦¬í•  ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”", "âŒ ì„ íƒëœ ë¹„ë””ì˜¤ ì—†ìŒ"

            total_videos = len(videos_to_process)
            total_processed_frames = 0
            video_results = []

            progress(0, desc=f"Batch ì²˜ë¦¬ ì‹œì‘: {total_videos}ê°œ ë¹„ë””ì˜¤...")

            for video_idx, video_path in enumerate(videos_to_process):
                video_name = Path(video_path).name
                progress(video_idx / total_videos, desc=f"ì²˜ë¦¬ ì¤‘: {video_name} ({video_idx+1}/{total_videos})")

                print(f"\n{'='*80}")
                print(f"ğŸ“¹ ë¹„ë””ì˜¤ {video_idx+1}/{total_videos}: {video_name}")
                print(f"{'='*80}")

                # 1. ë¹„ë””ì˜¤ ë¡œë“œ (stride ê°„ê²©)
                # strideë¥¼ ì°¾ì„ ë•ŒëŠ” batch_video_infoì—ì„œ ì „ì²´ í”„ë ˆì„ ì°¾ì•„ì•¼ í•¨
                # í˜„ì¬ video_idxëŠ” videos_to_processì˜ ì¸ë±ìŠ¤ì´ë¯€ë¡œ, ì›ë³¸ video_infoë¥¼ ì°¾ì•„ì•¼ í•¨
                matching_info = None
                for info in self.batch_video_info:
                    if info['path'] == video_path:
                        matching_info = info
                        break

                if matching_info is None:
                    print(f"âš ï¸ {video_name}: ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê±´ë„ˆëœ€")
                    continue

                num_frames = matching_info['frames']

                # stride ê³„ì‚°: target_framesì— ë§ì¶° ìë™ ì¡°ì •
                # ëª©í‘œ: target_frames í”„ë ˆì„ì„ ì¶”ì¶œí•˜ë„ë¡ stride ê³„ì‚°
                # stride = num_frames // target_frames (ìµœì†Œ 1)
                # ì‹¤ì œ ì¶”ì¶œë˜ëŠ” í”„ë ˆì„ ìˆ˜: ceil(num_frames / stride)
                calculated_stride = max(1, num_frames // target_frames)
                actual_num_frames_to_extract = (num_frames + calculated_stride - 1) // calculated_stride

                frame_indices = list(range(0, num_frames, calculated_stride))

                print(f"âœ“ í”„ë ˆì„ ì¶”ì¶œ ê³„íš:")
                print(f"  - ì´ ë¹„ë””ì˜¤ í”„ë ˆì„: {num_frames}")
                print(f"  - ëª©í‘œ í”„ë ˆì„ ìˆ˜: {target_frames}")
                print(f"  - ê³„ì‚°ëœ stride: {calculated_stride}")
                print(f"  - ì‹¤ì œ ì¶”ì¶œ í”„ë ˆì„ ìˆ˜: {actual_num_frames_to_extract}")
                print(f"  - ê³µì‹: ceil({num_frames} / {calculated_stride}) = {actual_num_frames_to_extract}")

                # Extract frames
                frames = self.processor.extract_frames(video_path, 0, num_frames, stride=calculated_stride)
                if not frames:
                    print(f"âš ï¸ {video_name}: í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
                    continue

                # 2. Propagate (SAM 2 Video Predictor)
                print(f"âœ“ Propagation ì‹œì‘...")

                # ì„ì‹œ ë””ë ‰í† ë¦¬ì— í”„ë ˆì„ ì €ì¥
                video_temp_dir = tempfile.mkdtemp(prefix=f"sam3d_video_{video_idx}_")

                try:
                    for idx, frame in enumerate(frames):
                        frame_path = Path(video_temp_dir) / f"{idx:05d}.jpg"
                        cv2.imwrite(str(frame_path), frame)

                    # SAM 2 inference
                    if self.sam2_video_predictor is not None:
                        inference_state = self.sam2_video_predictor.init_state(video_path=video_temp_dir)

                        # Reference annotations ì ìš© (ì²« í”„ë ˆì„)
                        point_coords = []
                        point_labels = []

                        for px, py in reference_annotations['foreground']:
                            point_coords.append([px, py])
                            point_labels.append(1)

                        for px, py in reference_annotations['background']:
                            point_coords.append([px, py])
                            point_labels.append(0)

                        point_coords = np.array(point_coords, dtype=np.float32)
                        point_labels = np.array(point_labels, dtype=np.int32)

                        # Add points to first frame
                        self.sam2_video_predictor.add_new_points_or_box(
                            inference_state=inference_state,
                            frame_idx=0,
                            obj_id=1,
                            points=point_coords,
                            labels=point_labels,
                        )

                        # Propagate
                        video_segments = {}
                        for frame_idx, obj_ids, mask_logits in self.sam2_video_predictor.propagate_in_video(
                            inference_state,
                            start_frame_idx=0
                        ):
                            video_segments[frame_idx] = (mask_logits[0] > 0.0).cpu().numpy()

                        # 3. ê²°ê³¼ ì €ì¥ (ë¹„ë””ì˜¤ë³„ ë””ë ‰í† ë¦¬)
                        video_result_dir = batch_temp_dir / f"video_{video_idx:03d}"
                        video_result_dir.mkdir(exist_ok=True)

                        for frame_idx, mask in video_segments.items():
                            frame_dir = video_result_dir / f"frame_{frame_idx:04d}"
                            frame_dir.mkdir(exist_ok=True)

                            # Save frame and mask
                            cv2.imwrite(str(frame_dir / "original.png"), frames[frame_idx])

                            mask_uint8 = mask.squeeze().astype(np.uint8) * 255
                            cv2.imwrite(str(frame_dir / "mask.png"), mask_uint8)

                        print(f"âœ“ {len(video_segments)} í”„ë ˆì„ ì €ì¥ ì™„ë£Œ")
                        total_processed_frames += len(video_segments)

                        video_results.append({
                            'video_idx': video_idx,
                            'video_name': video_name,
                            'video_path': video_path,
                            'frames': len(video_segments),
                            'result_dir': str(video_result_dir)
                        })

                finally:
                    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                    shutil.rmtree(video_temp_dir, ignore_errors=True)

                    # ì ê·¹ì ì¸ ë©”ëª¨ë¦¬ í•´ì œ
                    # SAM 2 inference_state ì •ë¦¬
                    if 'inference_state' in locals():
                        del inference_state
                    if 'video_segments' in locals():
                        del video_segments

                    # í”„ë ˆì„ ë©”ëª¨ë¦¬ í•´ì œ
                    del frames

                    # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()

                    # Python garbage collection ê°•ì œ ì‹¤í–‰
                    import gc
                    gc.collect()

                print(f"âœ“ {video_name} ì™„ë£Œ (ë©”ëª¨ë¦¬ í•´ì œë¨)")

            # ê²°ê³¼ ì €ì¥
            self.batch_results = {
                'temp_dir': str(batch_temp_dir),
                'videos': video_results,
                'total_frames': total_processed_frames,
                'target_frames': target_frames,
                'reference_annotations': reference_annotations
            }

            progress(1.0, desc="Batch ì²˜ë¦¬ ì™„ë£Œ!")

            status = f"""
### ğŸ‰ Batch Propagation ì™„ë£Œ âœ…

- **ì²˜ë¦¬ëœ ë¹„ë””ì˜¤**: {len(video_results)} / {total_videos}
- **ì´ í”„ë ˆì„ ìˆ˜**: {total_processed_frames}
- **ëª©í‘œ í”„ë ˆì„ ìˆ˜**: {target_frames} (ê° ë¹„ë””ì˜¤ë‹¹)
- **ì„ì‹œ ì €ì¥ ìœ„ì¹˜**: {batch_temp_dir}

### ë‹¤ìŒ ë‹¨ê³„:
- **Export to Fauna** í´ë¦­í•˜ì—¬ í†µí•© ë°ì´í„°ì…‹ ìƒì„±
"""

            return status, "âœ… ì™„ë£Œ"

        except Exception as e:
            import traceback
            error_msg = f"âŒ Batch ì²˜ë¦¬ ì‹¤íŒ¨:\n{str(e)}\n{traceback.format_exc()}"
            print(error_msg)
            return error_msg, "âŒ ì‹¤íŒ¨"

    def save_batch_session(self, session_name: str = "") -> Tuple[str, str]:
        """
        Batch ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì„¸ì…˜ìœ¼ë¡œ ì €ì¥ (Fauna í˜•ì‹ì´ ì•„ë‹Œ ê°œë³„ ë¹„ë””ì˜¤ë³„ ì €ì¥)

        Args:
            session_name: ì„¸ì…˜ ì´ë¦„

        Returns:
            (ì €ì¥ ê²½ë¡œ, ìƒíƒœ ë©”ì‹œì§€)
        """
        if not hasattr(self, 'batch_results') or not self.batch_results:
            return "", "âŒ ë¨¼ì € Batch Propagationì„ ì‹¤í–‰í•˜ì„¸ìš”"

        try:
            from datetime import datetime
            import shutil
            import json

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            if session_name and session_name.strip():
                session_id = f"{session_name.strip()}_{timestamp}"
            else:
                session_id = f"batch_{timestamp}"

            output_dir = Path(f"outputs/sessions/{session_id}")
            output_dir.mkdir(parents=True, exist_ok=True)

            print(f"\n{'='*80}")
            print(f"ğŸ’¾ Batch ì„¸ì…˜ ì €ì¥: {output_dir}")
            print(f"{'='*80}")

            batch_results = self.batch_results

            # ë©”íƒ€ë°ì´í„°
            metadata = {
                'session_id': session_id,
                'session_type': 'batch',
                'timestamp': timestamp,
                'total_videos': len(batch_results['videos']),
                'total_frames': batch_results['total_frames'],
                'target_frames': batch_results['target_frames'],
                'reference_annotations': batch_results['reference_annotations'],
                'videos': []
            }

            # ê° ë¹„ë””ì˜¤ ê²°ê³¼ë¥¼ ê°œë³„ í´ë”ì— ì €ì¥
            for video_result in batch_results['videos']:
                video_name = video_result['video_name']
                video_result_dir = Path(video_result['result_dir'])
                video_idx = video_result['video_idx']

                # ë¹„ë””ì˜¤ë³„ ì €ì¥ ë””ë ‰í† ë¦¬
                video_save_dir = output_dir / f"video_{video_idx:03d}_{Path(video_name).stem}"
                video_save_dir.mkdir(exist_ok=True)

                print(f"\nğŸ“¹ ì €ì¥ ì¤‘: {video_name}")

                # í”„ë ˆì„ ë³µì‚¬
                if video_result_dir.exists():
                    for frame_dir in video_result_dir.iterdir():
                        if frame_dir.is_dir() and frame_dir.name.startswith('frame_'):
                            dst = video_save_dir / frame_dir.name
                            shutil.copytree(frame_dir, dst, dirs_exist_ok=True)

                # ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„°
                video_meta = {
                    'video_idx': video_idx,
                    'video_name': video_name,
                    'video_path': video_result['video_path'],
                    'num_frames': video_result['frames'],
                    'saved_dir': str(video_save_dir.relative_to(output_dir))
                }
                metadata['videos'].append(video_meta)

                print(f"  âœ“ {video_result['frames']} í”„ë ˆì„ ì €ì¥ ì™„ë£Œ")

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_path = output_dir / "session_metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            if 'temp_dir' in batch_results:
                temp_dir = Path(batch_results['temp_dir'])
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)

            print(f"\nâœ… Batch ì„¸ì…˜ ì €ì¥ ì™„ë£Œ!")
            print(f"   ê²½ë¡œ: {output_dir}")

            status = f"""
### ğŸ’¾ Batch ì„¸ì…˜ ì €ì¥ ì™„ë£Œ âœ…

- **ì„¸ì…˜ ID**: `{session_id}`
- **ì €ì¥ ê²½ë¡œ**: `{output_dir}`
- **ë¹„ë””ì˜¤ ìˆ˜**: {len(batch_results['videos'])}
- **ì´ í”„ë ˆì„ ìˆ˜**: {batch_results['total_frames']}
- **ëª©í‘œ í”„ë ˆì„ ìˆ˜**: {batch_results['target_frames']} (ê° ë¹„ë””ì˜¤ë‹¹)

### ì €ì¥ëœ ë¹„ë””ì˜¤:
"""
            for video_meta in metadata['videos']:
                status += f"\n- **{video_meta['video_name']}**: {video_meta['num_frames']} í”„ë ˆì„ (â†’ `{video_meta['saved_dir']}`)"

            status += f"""

### ì„¸ì…˜ êµ¬ì¡°:
```
{session_id}/
â”œâ”€â”€ video_000_{Path(metadata['videos'][0]['video_name']).stem}/
â”‚   â”œâ”€â”€ frame_0000/
â”‚   â”‚   â”œâ”€â”€ original.png
â”‚   â”‚   â””â”€â”€ mask.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ video_001_.../
â””â”€â”€ session_metadata.json
```

### ë‹¤ìŒ ë‹¨ê³„:
- ì €ì¥ëœ ì„¸ì…˜ì€ ë‚˜ì¤‘ì— ë¡œë“œ ê°€ëŠ¥
- ë˜ëŠ” **Export to Fauna**ë¡œ í†µí•© ë°ì´í„°ì…‹ ìƒì„±
"""

            return str(output_dir), status

        except Exception as e:
            import traceback
            error_msg = f"âŒ Batch ì„¸ì…˜ ì €ì¥ ì‹¤íŒ¨:\n{str(e)}\n{traceback.format_exc()}"
            print(error_msg)
            return "", error_msg

    def load_batch_session(self, session_path: str) -> Tuple[str, str]:
        """
        ì €ì¥ëœ Batch ì„¸ì…˜ ë¡œë“œ

        Args:
            session_path: ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë˜ëŠ” session_metadata.json ê²½ë¡œ

        Returns:
            (ìƒíƒœ ë©”ì‹œì§€, ì„±ê³µ ì—¬ë¶€)
        """
        try:
            import json

            session_path = Path(session_path)

            # session_metadata.json ê²½ë¡œ ì°¾ê¸°
            if session_path.is_file() and session_path.name == "session_metadata.json":
                metadata_path = session_path
                session_dir = session_path.parent
            elif session_path.is_dir():
                metadata_path = session_path / "session_metadata.json"
                session_dir = session_path
            else:
                return "âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜ ê²½ë¡œì…ë‹ˆë‹¤", ""

            if not metadata_path.exists():
                return f"âŒ ì„¸ì…˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_path}", ""

            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)

            # ì„¸ì…˜ íƒ€ì… í™•ì¸
            if metadata.get('session_type') != 'batch':
                return f"âŒ Batch ì„¸ì…˜ì´ ì•„ë‹™ë‹ˆë‹¤. (íƒ€ì…: {metadata.get('session_type')})", ""

            print(f"\n{'='*80}")
            print(f"ğŸ“‚ Batch ì„¸ì…˜ ë¡œë“œ: {session_dir}")
            print(f"{'='*80}")

            # batch_results ë³µì›
            video_results = []
            for video_meta in metadata['videos']:
                video_result_dir = session_dir / video_meta['saved_dir']

                if not video_result_dir.exists():
                    print(f"  âš ï¸  ê²½ê³ : {video_result_dir} ì—†ìŒ")
                    continue

                # í”„ë ˆì„ ê°œìˆ˜ í™•ì¸
                num_frames = len(list(video_result_dir.glob("frame_*")))

                video_results.append({
                    'video_idx': video_meta['video_idx'],
                    'video_name': video_meta['video_name'],
                    'video_path': video_meta['video_path'],
                    'frames': num_frames,
                    'result_dir': str(video_result_dir)
                })

                print(f"  âœ“ {video_meta['video_name']}: {num_frames} í”„ë ˆì„")

            # batch_results ì„¤ì •
            self.batch_results = {
                'temp_dir': '',  # ë¡œë“œëœ ì„¸ì…˜ì€ ì„ì‹œ ë””ë ‰í† ë¦¬ ì—†ìŒ
                'videos': video_results,
                'total_frames': metadata['total_frames'],
                'target_frames': metadata['target_frames'],
                'reference_annotations': metadata['reference_annotations']
            }

            print(f"\nâœ… Batch ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ!")

            status = f"""
### ğŸ“‚ Batch ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ âœ…

- **ì„¸ì…˜ ID**: `{metadata['session_id']}`
- **ë¡œë“œ ê²½ë¡œ**: `{session_dir}`
- **ë¹„ë””ì˜¤ ìˆ˜**: {len(video_results)}
- **ì´ í”„ë ˆì„ ìˆ˜**: {metadata['total_frames']}
- **ëª©í‘œ í”„ë ˆì„ ìˆ˜**: {metadata['target_frames']} (ê° ë¹„ë””ì˜¤ë‹¹)

### ë¡œë“œëœ ë¹„ë””ì˜¤:
"""
            for video_result in video_results:
                status += f"\n- **{video_result['video_name']}**: {video_result['frames']} í”„ë ˆì„"

            status += """

### ë‹¤ìŒ ë‹¨ê³„:
- **Export to Fauna** í´ë¦­í•˜ì—¬ í†µí•© ë°ì´í„°ì…‹ ìƒì„±
- ë˜ëŠ” ì¶”ê°€ í¸ì§‘ ìˆ˜í–‰
"""

            return status, "âœ… ë¡œë“œ ì™„ë£Œ"

        except Exception as e:
            import traceback
            error_msg = f"âŒ Batch ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨:\n{str(e)}\n{traceback.format_exc()}"
            print(error_msg)
            return error_msg, ""

    def export_batch_to_fauna(self, output_name: str = "fauna_dataset", file_structure: str = "video_folders") -> Tuple[str, str]:
        """
        Batch ì²˜ë¦¬ ê²°ê³¼ë¥¼ Fauna ë°ì´í„°ì…‹ í˜•ì‹ìœ¼ë¡œ export

        Args:
            output_name: ì¶œë ¥ ë°ì´í„°ì…‹ ì´ë¦„
            file_structure: íŒŒì¼ êµ¬ì¡° ("video_folders" ë˜ëŠ” "flat")

        Returns:
            (Fauna ë°ì´í„°ì…‹ ê²½ë¡œ, ìƒíƒœ ë©”ì‹œì§€)
        """
        if not hasattr(self, 'batch_results') or not self.batch_results:
            return "", "âŒ ë¨¼ì € Batch Propagationì„ ì‹¤í–‰í•˜ì„¸ìš”"

        try:
            from datetime import datetime
            import shutil
            import json

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = Path(f"outputs/fauna_datasets/{output_name}_{timestamp}")
            output_dir.mkdir(parents=True, exist_ok=True)

            print(f"\n{'='*80}")
            print(f"ğŸ“¦ Fauna ë°ì´í„°ì…‹ ìƒì„±: {output_dir}")
            print(f"ğŸ“ íŒŒì¼ êµ¬ì¡°: {file_structure}")
            print(f"{'='*80}")

            batch_results = self.batch_results
            total_frames_exported = 0
            video_segments_info = []

            # ê° ë¹„ë””ì˜¤ ê²°ê³¼ë¥¼ export
            for video_idx, video_result in enumerate(batch_results['videos']):
                video_name = video_result['video_name']
                video_result_dir = Path(video_result['result_dir'])
                num_frames = video_result['frames']

                print(f"\nğŸ“¹ {video_name}: {num_frames} í”„ë ˆì„ export ì¤‘...")

                # ë¹„ë””ì˜¤ ì´ë¦„ì—ì„œ ì•ˆì „í•œ prefix ìƒì„± (ê²½ë¡œ êµ¬ë¶„ì ì œê±°)
                video_prefix = f"video{video_idx:03d}"

                video_segment = {
                    'video_name': video_name,
                    'video_path': video_result['video_path'],
                    'video_idx': video_result['video_idx'],
                    'video_prefix': video_prefix,
                    'num_frames': num_frames
                }
                video_segments_info.append(video_segment)

                # ë¹„ë””ì˜¤ë³„ í´ë” ìƒì„± (video_folders ëª¨ë“œì¸ ê²½ìš°)
                if file_structure == "video_folders":
                    video_output_dir = output_dir / video_prefix
                    video_output_dir.mkdir(exist_ok=True)

                # í”„ë ˆì„ ë³µì‚¬
                for local_frame_idx in range(num_frames):
                    src_frame_dir = video_result_dir / f"frame_{local_frame_idx:04d}"

                    if not src_frame_dir.exists():
                        print(f"  âš ï¸  ê²½ê³ : {src_frame_dir} ì—†ìŒ, ê±´ë„ˆëœ€")
                        continue

                    # ì›ë³¸ íŒŒì¼ ì½ê¸°
                    src_rgb = src_frame_dir / "original.png"
                    src_mask = src_frame_dir / "mask.png"

                    if not src_rgb.exists() or not src_mask.exists():
                        print(f"  âš ï¸  ê²½ê³ : frame_{local_frame_idx:04d} íŒŒì¼ ëˆ„ë½")
                        continue

                    # ëª©ì ì§€ ê²½ë¡œ ê²°ì •
                    if file_structure == "video_folders":
                        # video001/frame_0000_rgb.png
                        dst_rgb = video_output_dir / f"frame_{local_frame_idx:04d}_rgb.png"
                        dst_mask = video_output_dir / f"frame_{local_frame_idx:04d}_mask.png"
                    else:  # flat
                        # video001_frame_0000_rgb.png
                        dst_rgb = output_dir / f"{video_prefix}_frame_{local_frame_idx:04d}_rgb.png"
                        dst_mask = output_dir / f"{video_prefix}_frame_{local_frame_idx:04d}_mask.png"

                    # íŒŒì¼ ë³µì‚¬
                    shutil.copy2(src_rgb, dst_rgb)
                    shutil.copy2(src_mask, dst_mask)

                total_frames_exported += num_frames
                print(f"  âœ“ {num_frames} í”„ë ˆì„ ë³µì‚¬ ì™„ë£Œ")

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                'dataset_name': output_name,
                'timestamp': timestamp,
                'file_structure': file_structure,
                'total_frames': total_frames_exported,
                'num_videos': len(batch_results['videos']),
                'target_frames': batch_results['target_frames'],
                'reference_annotations': batch_results['reference_annotations'],
                'video_segments': video_segments_info
            }

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_path = output_dir / "dataset_metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            print(f"\nâœ… Fauna ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
            print(f"   ê²½ë¡œ: {output_dir}")
            print(f"   ì´ í”„ë ˆì„: {total_frames_exported}")

            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            if 'temp_dir' in batch_results:
                temp_dir = Path(batch_results['temp_dir'])
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    print(f"âœ“ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ")

            # ìƒíƒœ ë©”ì‹œì§€ ìƒì„±
            structure_example = ""
            if file_structure == "video_folders":
                structure_example = f"""
```
{output_dir.name}/
â”œâ”€â”€ video000/
â”‚   â”œâ”€â”€ frame_0000_rgb.png
â”‚   â”œâ”€â”€ frame_0000_mask.png
â”‚   â”œâ”€â”€ frame_0001_rgb.png
â”‚   â””â”€â”€ frame_0001_mask.png
â”œâ”€â”€ video001/
â”‚   â””â”€â”€ ...
â””â”€â”€ dataset_metadata.json
```
"""
            else:  # flat
                structure_example = f"""
```
{output_dir.name}/
â”œâ”€â”€ video000_frame_0000_rgb.png
â”œâ”€â”€ video000_frame_0000_mask.png
â”œâ”€â”€ video000_frame_0001_rgb.png
â”œâ”€â”€ video000_frame_0001_mask.png
â”œâ”€â”€ video001_frame_0000_rgb.png
â””â”€â”€ dataset_metadata.json
```
"""

            status = f"""
### ğŸ‰ Fauna ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ âœ…

- **ì¶œë ¥ ê²½ë¡œ**: `{output_dir}`
- **íŒŒì¼ êµ¬ì¡°**: {file_structure}
- **ì´ í”„ë ˆì„ ìˆ˜**: {total_frames_exported}
- **ë¹„ë””ì˜¤ ìˆ˜**: {len(batch_results['videos'])}
- **ëª©í‘œ í”„ë ˆì„ ìˆ˜**: {batch_results['target_frames']} (ê° ë¹„ë””ì˜¤ë‹¹)

### ë¹„ë””ì˜¤ ì •ë³´:
"""
            for seg in video_segments_info:
                status += f"\n- **{seg['video_name']}**: {seg['num_frames']} í”„ë ˆì„ (prefix: {seg['video_prefix']})"

            status += f"\n\n### ë°ì´í„°ì…‹ êµ¬ì¡°:{structure_example}"

            return str(output_dir), status

        except Exception as e:
            import traceback
            error_msg = f"âŒ Fauna Export ì‹¤íŒ¨: {str(e)}\n\n{traceback.format_exc()}"
            print(error_msg)
            return "", error_msg

    def get_video_duration(self, data_dir: str, video_file: str) -> float:
        """
        ë¹„ë””ì˜¤ íŒŒì¼ì˜ ì „ì²´ ê¸¸ì´(ì´ˆ) ë°˜í™˜

        Args:
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
            video_file: ë¹„ë””ì˜¤ íŒŒì¼ëª…

        Returns:
            ë¹„ë””ì˜¤ ê¸¸ì´(ì´ˆ), ì‹¤íŒ¨ ì‹œ 3.0 (ê¸°ë³¸ê°’)
        """
        if not video_file:
            return 3.0

        video_path = Path(data_dir) / video_file
        if not video_path.exists():
            return 3.0

        try:
            info = self.processor.get_video_info(str(video_path))
            duration = info['frame_count'] / info['fps']
            print(f"âœ“ ë¹„ë””ì˜¤ ê¸¸ì´: {duration:.2f}ì´ˆ ({info['frame_count']} í”„ë ˆì„, {info['fps']:.2f} fps)")
            return round(duration, 2)
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {e}")
            return 3.0

    def load_video(self, data_dir: str, video_file: str,
                   start_time: float, duration: float) -> Tuple[np.ndarray, str, gr.Slider]:
        """ë¹„ë””ì˜¤ ë¡œë“œ ë° í”„ë ˆì„ ì¶”ì¶œ"""
        default_slider = gr.Slider(label="í”„ë ˆì„ ìœ„ì¹˜", minimum=0, maximum=100, value=0, step=1)

        if not video_file:
            return None, "ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”", default_slider

        video_path = Path(data_dir) / video_file

        if not video_path.exists():
            return None, f"ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}", default_slider

        self.video_path = str(video_path)

        try:
            # ë¹„ë””ì˜¤ ì •ë³´
            info = self.processor.get_video_info(self.video_path)
            fps = info['fps']
            total_duration = info['frame_count'] / fps

            # durationì´ ë¹„ë””ì˜¤ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ë©´ ì „ì²´ ê¸¸ì´ ì‚¬ìš©
            if duration <= 0 or duration > total_duration:
                duration = total_duration
                print(f"âœ“ Durationì„ ë¹„ë””ì˜¤ ì „ì²´ ê¸¸ì´ë¡œ ì„¤ì •: {duration:.2f}ì´ˆ")

            # í”„ë ˆì„ ì¶”ì¶œ
            start_frame = int(start_time * fps)
            num_frames = int(duration * fps)

            self.frames = self.processor.extract_frames(
                self.video_path,
                start_frame,
                num_frames,
                stride=1
            )

            if not self.frames:
                return None, "âŒ í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤", default_slider

            # ì´ˆê¸°í™”
            self.current_frame_idx = 0
            self.annotations = {'foreground': [], 'background': []}
            self.masks = [None] * len(self.frames)
            self.current_mask = None

            info_text = f"""
### ë¹„ë””ì˜¤ ë¡œë“œ ì™„ë£Œ âœ…

- **í”„ë ˆì„ ìˆ˜**: {len(self.frames)}
- **í•´ìƒë„**: {info['width']} x {info['height']}
- **FPS**: {info['fps']:.2f}
- **êµ¬ê°„**: {start_time}s - {start_time + duration}s

### ë‹¤ìŒ ë‹¨ê³„:
1. **Foreground Point** í´ë¦­í•˜ì—¬ ê°ì²´ ìœ„ì¹˜ ì§€ì •
2. **Background Point** í´ë¦­í•˜ì—¬ ë°°ê²½ ìœ„ì¹˜ ì§€ì • (ì„ íƒì‚¬í•­)
3. **Segment Current Frame** í´ë¦­í•˜ì—¬ í˜„ì¬ í”„ë ˆì„ ì„¸ê·¸ë©˜í…Œì´ì…˜
4. **Propagate to All Frames** í´ë¦­í•˜ì—¬ ì „ì²´ ë¹„ë””ì˜¤ ì¶”ì 
5. **Generate 3D Mesh** í´ë¦­í•˜ì—¬ 3D ìƒì„±
            """

            # ì²« í”„ë ˆì„ ë°˜í™˜ + ìŠ¬ë¼ì´ë” ì—…ë°ì´íŠ¸
            frame_rgb = cv2.cvtColor(self.frames[0], cv2.COLOR_BGR2RGB)

            # ìŠ¬ë¼ì´ë” ë²”ìœ„ ì—…ë°ì´íŠ¸
            slider_update = gr.Slider(
                label="í”„ë ˆì„ ìœ„ì¹˜",
                minimum=0,
                maximum=len(self.frames) - 1,
                value=0,
                step=1,
                interactive=True,
                info=f"ìŠ¬ë¼ì´ë”ë¥¼ ë“œë˜ê·¸í•˜ì—¬ í”„ë ˆì„ ì´ë™ (ì´ {len(self.frames)}ê°œ)"
            )

            return frame_rgb, info_text, slider_update

        except Exception as e:
            import traceback
            error_msg = f"""
### âŒ ì˜¤ë¥˜ ë°œìƒ

**ì—ëŸ¬ ë©”ì‹œì§€:**
```
{str(e)}
```

**ìƒì„¸ ì •ë³´:**
```
{traceback.format_exc()}
```

**í™•ì¸ì‚¬í•­:**
- ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œê°€ ì •í™•í•œê°€ìš”?
- ë¹„ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ë‚˜ìš”?
- íŒŒì¼ í˜•ì‹ì´ ì§€ì›ë˜ë‚˜ìš”? (MP4, AVI, MOV, MKV)
"""
            print(f"[ERROR] {error_msg}")
            return None, error_msg, default_slider

    def add_point(self, image: np.ndarray, point_type: str, evt: gr.SelectData) -> Tuple[np.ndarray, str]:
        """
        ì´ë¯¸ì§€ í´ë¦­ ì‹œ point ì¶”ê°€

        Args:
            image: í˜„ì¬ ì´ë¯¸ì§€
            point_type: 'foreground' or 'background'
            evt: Gradio í´ë¦­ ì´ë²¤íŠ¸
        """
        if image is None or len(self.frames) == 0:
            return image, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

        # í´ë¦­ ì¢Œí‘œ
        x, y = evt.index[0], evt.index[1]

        # Point ì¶”ê°€
        self.annotations[point_type].append((x, y))

        # í˜„ì¬ í”„ë ˆì„ì— point í‘œì‹œ
        frame = self.frames[self.current_frame_idx].copy()
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Foreground points (ë…¹ìƒ‰)
        for px, py in self.annotations['foreground']:
            cv2.circle(frame_rgb, (px, py), 5, (0, 255, 0), -1)
            cv2.circle(frame_rgb, (px, py), 7, (255, 255, 255), 2)

        # Background points (ë¹¨ê°„ìƒ‰)
        for px, py in self.annotations['background']:
            cv2.circle(frame_rgb, (px, py), 5, (255, 0, 0), -1)
            cv2.circle(frame_rgb, (px, py), 7, (255, 255, 255), 2)

        status = f"""
**Annotations:**
- Foreground: {len(self.annotations['foreground'])} points
- Background: {len(self.annotations['background'])} points

í´ë¦­í•œ ìœ„ì¹˜: ({x}, {y}) - {point_type}
"""

        return frame_rgb, status

    def segment_current_frame(self) -> Tuple[np.ndarray, str]:
        """
        í˜„ì¬ í”„ë ˆì„ì„ SAMìœ¼ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜
        (ê°„ë‹¨í•œ contour ê¸°ë°˜, ì‹¤ì œ SAM ëª¨ë¸ í†µí•©ì€ ë³„ë„ í•„ìš”)
        """
        if len(self.frames) == 0:
            return None, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

        if len(self.annotations['foreground']) == 0:
            return None, "ìµœì†Œ 1ê°œì˜ foreground pointê°€ í•„ìš”í•©ë‹ˆë‹¤"

        try:
            frame = self.frames[self.current_frame_idx]
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # SAM2 ì‚¬ìš© (availableí•˜ë©´)
            if self.sam2_predictor is not None:
                # SAM2 inference
                self.sam2_predictor.set_image(frame_rgb)

                # Pointsì™€ labels ì¤€ë¹„
                point_coords = []
                point_labels = []

                for px, py in self.annotations['foreground']:
                    point_coords.append([px, py])
                    point_labels.append(1)  # foreground

                for px, py in self.annotations['background']:
                    point_coords.append([px, py])
                    point_labels.append(0)  # background

                point_coords = np.array(point_coords, dtype=np.float32)
                point_labels = np.array(point_labels, dtype=np.int32)

                # SAM2 predict
                masks, scores, _ = self.sam2_predictor.predict(
                    point_coords=point_coords,
                    point_labels=point_labels,
                    multimask_output=True
                )

                # Best mask ì„ íƒ
                best_idx = np.argmax(scores)
                mask = masks[best_idx]
                confidence = scores[best_idx]

                status_method = f"SAM2 (confidence: {confidence:.3f})"
            else:
                # Fallback: contour ê¸°ë°˜
                mask = self.processor.segment_object_interactive(frame, method='contour')
                confidence = 0.0
                status_method = "Contour (fallback)"

            # ë§ˆìŠ¤í¬ ì €ì¥
            self.masks[self.current_frame_idx] = mask
            self.current_mask = mask

            # ì‹œê°í™”
            overlay = frame_rgb.copy()
            overlay[mask > 0] = [0, 255, 0]  # ë…¹ìƒ‰ ë§ˆìŠ¤í¬
            result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)

            # Points í‘œì‹œ
            for px, py in self.annotations['foreground']:
                cv2.circle(result, (px, py), 5, (0, 255, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)
            for px, py in self.annotations['background']:
                cv2.circle(result, (px, py), 5, (255, 0, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)

            mask_area = np.sum(mask > 0)
            mask_pct = mask_area / mask.size * 100

            status = f"""
### Segmentation ì™„ë£Œ âœ…

- **Method**: {status_method}
- **í”„ë ˆì„**: {self.current_frame_idx + 1} / {len(self.frames)}
- **ë§ˆìŠ¤í¬ ì˜ì—­**: {mask_area} í”½ì…€ ({mask_pct:.1f}%)
- **Foreground points**: {len(self.annotations['foreground'])}
- **Background points**: {len(self.annotations['background'])}

### ë‹¤ìŒ:
- ë‹¤ë¥¸ í”„ë ˆì„ì—ë„ annotationí•˜ë ¤ë©´ í”„ë ˆì„ ì´ë™
- ë˜ëŠ” **Propagate to All Frames** í´ë¦­
"""

            return result, status

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return None, f"ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {str(e)}\n\n```\n{error_detail}\n```"

    def propagate_to_all_frames(self, stride: int = 1, progress=gr.Progress()) -> Tuple[np.ndarray, str]:
        """
        í˜„ì¬ í”„ë ˆì„ì˜ annotationì„ ì „ì²´ ë¹„ë””ì˜¤ì— propagation (tracking)
        SAM 2 Video Predictorë¥¼ ì‚¬ìš©í•œ ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì 

        Args:
            stride: í”„ë ˆì„ ì²˜ë¦¬ ê°„ê²© (1=ëª¨ë“  í”„ë ˆì„, 10=10í”„ë ˆì„ë§ˆë‹¤ ì²˜ë¦¬)

        ì¤‘ìš”: ê³ ì • pointsë¥¼ ëª¨ë“  í”„ë ˆì„ì— ì¬ì ìš©í•˜ì§€ ì•ŠìŒ!
        ëŒ€ì‹  SAM 2ì˜ memory mechanismì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ê°ì²´ ì¶”ì 
        """
        if len(self.frames) == 0:
            return None, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

        if len(self.annotations['foreground']) == 0:
            return None, "Annotation pointsê°€ í•„ìš”í•©ë‹ˆë‹¤ (ìµœì†Œ 1ê°œì˜ foreground point)"

        try:
            progress(0, desc="ë¹„ë””ì˜¤ tracking ì´ˆê¸°í™” (SAM 2 Video Predictor)...")

            # SAM 2 Video Predictor ì‚¬ìš© (ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì )
            if self.sam2_video_predictor is not None:
                # 1. ì„ì‹œ ë””ë ‰í† ë¦¬ì— í”„ë ˆì„ ì €ì¥ (SAM 2 Video PredictorëŠ” ë””ë ‰í† ë¦¬ ì…ë ¥ í•„ìš”)
                import tempfile
                import os
                temp_dir = tempfile.mkdtemp(prefix="sam3d_video_")

                try:
                    # ë©”ëª¨ë¦¬ ë³´í˜¸: ìµœëŒ€ 500 í”„ë ˆì„ìœ¼ë¡œ ì œí•œ (ì•½ 6GB ë©”ëª¨ë¦¬)
                    MAX_FRAMES = 500
                    effective_stride = stride
                    frame_indices = list(range(0, len(self.frames), stride))

                    if len(frame_indices) > MAX_FRAMES:
                        # stride ìë™ ì¡°ì •
                        effective_stride = max(stride, len(self.frames) // MAX_FRAMES)
                        frame_indices = list(range(0, len(self.frames), effective_stride))
                        print(f"âš ï¸ ë©”ëª¨ë¦¬ ë³´í˜¸: stride {stride} â†’ {effective_stride} ìë™ ì¡°ì • ({len(frame_indices)} í”„ë ˆì„)")

                    progress(0.05, desc=f"í”„ë ˆì„ ì €ì¥ ì¤‘ (stride={effective_stride}, ì´ {len(frame_indices)} í”„ë ˆì„)...")

                    # stride ê°„ê²©ìœ¼ë¡œë§Œ í”„ë ˆì„ ì €ì¥
                    for idx, i in enumerate(frame_indices):
                        frame_path = os.path.join(temp_dir, f"{idx:05d}.jpg")
                        cv2.imwrite(frame_path, self.frames[i])

                    # ì›ë³¸ ì¸ë±ìŠ¤ ë§¤í•‘ ì €ì¥ (ë‚˜ì¤‘ì— ê²°ê³¼ë¥¼ ì›ë³¸ ì¸ë±ìŠ¤ë¡œ ë³µì›)
                    self.stride_frame_mapping = {idx: i for idx, i in enumerate(frame_indices)}
                    self.effective_stride = effective_stride  # status ë©”ì‹œì§€ë¥¼ ìœ„í•´ ì €ì¥

                    print(f"âœ“ {len(frame_indices)} í”„ë ˆì„ ì €ì¥ ì™„ë£Œ (ì›ë³¸ {len(self.frames)}ê°œ ì¤‘)")

                    progress(0.1, desc="SAM 2 Video Predictor ì´ˆê¸°í™” ì¤‘...")

                    # 2. Inference state ì´ˆê¸°í™”
                    inference_state = self.sam2_video_predictor.init_state(video_path=temp_dir)

                    # 3. í˜„ì¬ í”„ë ˆì„ì—ë§Œ annotation points ì¶”ê°€ (conditioning frame)
                    point_coords = []
                    point_labels = []

                    for px, py in self.annotations['foreground']:
                        point_coords.append([px, py])
                        point_labels.append(1)

                    for px, py in self.annotations['background']:
                        point_coords.append([px, py])
                        point_labels.append(0)

                    point_coords = np.array(point_coords, dtype=np.float32)
                    point_labels = np.array(point_labels, dtype=np.int32)

                    # í˜„ì¬ í”„ë ˆì„ ì¸ë±ìŠ¤ë¥¼ stride ê¸°ë°˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                    # ì˜ˆ: ì›ë³¸ í”„ë ˆì„ 20, stride=10 -> stride ì¸ë±ìŠ¤ 2
                    stride_frame_idx = self.current_frame_idx // stride
                    if self.current_frame_idx not in frame_indices:
                        # í˜„ì¬ í”„ë ˆì„ì´ strideì— í¬í•¨ë˜ì§€ ì•Šìœ¼ë©´ ê°€ì¥ ê°€ê¹Œìš´ í”„ë ˆì„ ì‚¬ìš©
                        stride_frame_idx = min(range(len(frame_indices)),
                                              key=lambda i: abs(frame_indices[i] - self.current_frame_idx))

                    progress(0.15, desc=f"ì´ˆê¸° í”„ë ˆì„ ({self.current_frame_idx} -> stride idx {stride_frame_idx}) annotation ì¤‘...")

                    # í˜„ì¬ í”„ë ˆì„ì„ conditioning frameìœ¼ë¡œ ì„¤ì •
                    _, out_obj_ids, out_mask_logits = self.sam2_video_predictor.add_new_points_or_box(
                        inference_state=inference_state,
                        frame_idx=stride_frame_idx,
                        obj_id=1,  # Single object tracking
                        points=point_coords,
                        labels=point_labels,
                    )

                    progress(0.2, desc="ë©”ëª¨ë¦¬ ê¸°ë°˜ ì „íŒŒ ì‹œì‘...")

                    # 4. Propagate using memory-based tracking (NO points on other frames!)
                    video_segments = {}
                    for stride_idx, obj_ids, mask_logits in self.sam2_video_predictor.propagate_in_video(
                        inference_state,
                        start_frame_idx=stride_frame_idx
                    ):
                        # Memory-based tracking - ê° í”„ë ˆì„ì€ ì´ì „ í”„ë ˆì„ì˜ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©
                        # PointsëŠ” ì¬ì ìš©ë˜ì§€ ì•ŠìŒ!
                        video_segments[stride_idx] = (mask_logits[0] > 0.0).cpu().numpy()

                        progress_pct = 0.2 + 0.6 * (stride_idx + 1) / len(frame_indices)
                        progress(progress_pct, desc=f"Tracking... {stride_idx+1}/{len(frame_indices)} (stride {stride})")

                    # 5. ê²°ê³¼ë¥¼ self.masksì— ì €ì¥ (stride ê°„ê²©ì˜ í”„ë ˆì„ë§Œ)
                    self.masks = [None] * len(self.frames)
                    for stride_idx, mask in video_segments.items():
                        original_idx = self.stride_frame_mapping.get(stride_idx)
                        if original_idx is not None and original_idx < len(self.masks):
                            self.masks[original_idx] = mask.squeeze()

                    progress(0.9, desc="Tracking ì™„ë£Œ, ê²°ê³¼ ì²˜ë¦¬ ì¤‘...")

                finally:
                    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                    import shutil
                    shutil.rmtree(temp_dir, ignore_errors=True)

            else:
                # Fallback: Image predictor ì‚¬ìš© (êµ¬ë²„ì „ ë°©ì‹ - ì •í™•ë„ ë‚®ìŒ)
                progress(0, desc="Fallback: í”„ë ˆì„ë³„ ì„¸ê·¸ë©˜í…Œì´ì…˜...")

                for i, frame in enumerate(self.frames):
                    if self.masks[i] is None:
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                        if self.sam2_predictor is not None:
                            self.sam2_predictor.set_image(frame_rgb)

                            point_coords = []
                            point_labels = []

                            for px, py in self.annotations['foreground']:
                                point_coords.append([px, py])
                                point_labels.append(1)

                            for px, py in self.annotations['background']:
                                point_coords.append([px, py])
                                point_labels.append(0)

                            point_coords = np.array(point_coords, dtype=np.float32)
                            point_labels = np.array(point_labels, dtype=np.int32)

                            masks, scores, _ = self.sam2_predictor.predict(
                                point_coords=point_coords,
                                point_labels=point_labels,
                                multimask_output=True
                            )

                            best_idx = np.argmax(scores)
                            mask = masks[best_idx]
                        else:
                            mask = self.processor.segment_object_interactive(frame, method='contour')

                        self.masks[i] = mask

                    progress((i + 1) / len(self.frames), desc=f"Processing... {i+1}/{len(self.frames)}")

            progress(1.0, desc="ì‹œê°í™” ì¤€ë¹„ ì¤‘...")

            # í˜„ì¬ í”„ë ˆì„ ì‹œê°í™”
            self.current_frame_idx = min(self.current_frame_idx, len(self.frames) - 1)
            current_frame = self.frames[self.current_frame_idx]
            current_mask = self.masks[self.current_frame_idx]

            frame_rgb = cv2.cvtColor(current_frame, cv2.COLOR_BGR2RGB)
            overlay = frame_rgb.copy()
            if current_mask is not None:
                overlay[current_mask > 0] = [0, 255, 0]
            result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)

            # í†µê³„
            tracked_frames = sum(1 for m in self.masks if m is not None)

            method_used = "SAM 2 Video Predictor (Memory-based)" if self.sam2_video_predictor else "SAM 2 Image (Fallback)"

            # ì‹¤ì œ ì‚¬ìš©ëœ stride ê³„ì‚°
            used_stride = getattr(self, 'effective_stride', stride)

            status = f"""
### Propagation ì™„ë£Œ âœ…

- **Method**: {method_used}
- **Stride**: {used_stride} (ì²˜ë¦¬ëœ í”„ë ˆì„: {tracked_frames}, ì „ì²´: {len(self.frames)})
- **íš¨ìœ¨ì„±**: {100 * tracked_frames / len(self.frames):.1f}% í”„ë ˆì„ë§Œ ì²˜ë¦¬
- **í˜„ì¬ í”„ë ˆì„**: {self.current_frame_idx + 1} / {len(self.frames)}
- **Conditioning Frame**: {self.current_frame_idx} (Pointsë§Œ ì—¬ê¸° ì ìš©)

### ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì  (Stride ì ìš©):
- í˜„ì¬ í”„ë ˆì„ì˜ pointsë§Œ ì‚¬ìš©
- {used_stride} ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì²˜ë¦¬ (ì˜ˆ: 3000 í”„ë ˆì„ â†’ {tracked_frames} í”„ë ˆì„)
- ê°ì²´ ì´ë™ì—ë„ ì •í™•í•œ ë§ˆìŠ¤í¬ ìƒì„±
- ë©”ëª¨ë¦¬ ë³´í˜¸: ìµœëŒ€ 500 í”„ë ˆì„ìœ¼ë¡œ ìë™ ì œí•œ

### ë‹¤ìŒ:
- **í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜**ìœ¼ë¡œ ê²°ê³¼ í™•ì¸ (stride ê°„ê²©ë§Œ ë§ˆìŠ¤í¬ ì¡´ì¬)
- **Generate 3D Mesh** í´ë¦­í•˜ì—¬ 3D ìƒì„±
- ë˜ëŠ” **Save Masks** í´ë¦­í•˜ì—¬ ë§ˆìŠ¤í¬ ì €ì¥
"""

            return result, status

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return None, f"Propagation ì‹¤íŒ¨: {str(e)}\n\n```\n{error_detail}\n```"

    def download_sam3d_checkpoint(self, progress=gr.Progress()) -> bool:
        """
        SAM 3D ì²´í¬í¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
        """
        import subprocess
        import os
        from dotenv import load_dotenv

        progress(0, desc="SAM 3D ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘...")

        # .env íŒŒì¼ ë¡œë“œ
        env_path = Path(__file__).parent.parent / ".env"
        if env_path.exists():
            load_dotenv(env_path)
            print(f"âœ“ .env íŒŒì¼ ë¡œë“œë¨: {env_path}")
        else:
            print(f"âš ï¸ .env íŒŒì¼ ì—†ìŒ: {env_path}")

        # HuggingFace í† í° í™•ì¸
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            print("âš ï¸ HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ê°€ëŠ¥.")

        # Configì—ì„œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        if self.config:
            checkpoint_dir = Path(self.config.sam3d_checkpoint_dir).expanduser()
        else:
            checkpoint_dir = Path("~/dev/sam-3d-objects/checkpoints/hf").expanduser()

        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        progress(0.1, desc="Git LFS í™•ì¸ ì¤‘...")

        # Git LFS í™•ì¸ ë° ì„¤ì¹˜
        try:
            subprocess.run(["git", "lfs", "version"], check=True, capture_output=True)
        except:
            progress(0.2, desc="Git LFS ì„¤ì¹˜ ì¤‘...")
            try:
                subprocess.run(["sudo", "apt-get", "update"], check=True)
                subprocess.run(["sudo", "apt-get", "install", "-y", "git-lfs"], check=True)
                subprocess.run(["git", "lfs", "install"], check=True)
            except Exception as e:
                print(f"Git LFS ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
                return False

        progress(0.3, desc="SAM 3D ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (5-10GB, ì‹œê°„ ì†Œìš”)")

        # HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ (í† í° ì¸ì¦ ì‚¬ìš©)
        try:
            # í† í°ì´ ìˆìœ¼ë©´ ì¸ì¦ URL ì‚¬ìš©
            if hf_token:
                clone_url = f"https://oauth2:{hf_token}@huggingface.co/facebook/sam-3d-objects"
                pull_url = f"https://oauth2:{hf_token}@huggingface.co/facebook/sam-3d-objects"
            else:
                clone_url = "https://huggingface.co/facebook/sam-3d-objects"
                pull_url = "origin"

            if not (checkpoint_dir / "pipeline.yaml").exists():
                # ì²˜ìŒ ë‹¤ìš´ë¡œë“œ
                subprocess.run([
                    "git", "clone",
                    clone_url,
                    str(checkpoint_dir)
                ], check=True, cwd=checkpoint_dir.parent)
                progress(0.9, desc="ë‹¤ìš´ë¡œë“œ ì™„ë£Œ, ê²€ì¦ ì¤‘...")
            else:
                # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì—…ë°ì´íŠ¸
                if hf_token:
                    subprocess.run(["git", "pull", pull_url], check=True, cwd=checkpoint_dir)
                else:
                    subprocess.run(["git", "pull"], check=True, cwd=checkpoint_dir)
                progress(0.9, desc="ì—…ë°ì´íŠ¸ ì™„ë£Œ, ê²€ì¦ ì¤‘...")

            # ë‹¤ìš´ë¡œë“œ í™•ì¸
            if (checkpoint_dir / "pipeline.yaml").exists():
                progress(1.0, desc="SAM 3D ì²´í¬í¬ì¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ!")
                return True
            else:
                return False

        except Exception as e:
            print(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def generate_3d_mesh(self, progress=gr.Progress()) -> Tuple[str, str]:
        """
        ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ë¡œ 3D mesh ìƒì„±
        """
        print("\n" + "="*80)
        print("ğŸ”¹ generate_3d_mesh() ì‹œì‘")
        print("="*80)

        if len(self.frames) == 0 or all(m is None for m in self.masks):
            print("âŒ í”„ë ˆì„ ë˜ëŠ” ë§ˆìŠ¤í¬ ì—†ìŒ")
            return None, "ë¨¼ì € ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ì™„ë£Œí•˜ì„¸ìš”"

        try:
            progress(0, desc="3D mesh ìƒì„± ì¤€ë¹„ ì¤‘...")

            # SAM 3D ì²´í¬í¬ì¸íŠ¸ í™•ì¸
            if self.config:
                checkpoint_dir = Path(self.config.sam3d_checkpoint_dir).expanduser()
                print(f"âœ“ Configì—ì„œ checkpoint ê²½ë¡œ ë¡œë“œ: {checkpoint_dir}")
            else:
                checkpoint_dir = Path("~/dev/sam-3d-objects/checkpoints/hf/checkpoints").expanduser()
                print(f"âœ“ ê¸°ë³¸ checkpoint ê²½ë¡œ ì‚¬ìš©: {checkpoint_dir}")

            print(f"âœ“ Checkpoint ì¡´ì¬ í™•ì¸ ì¤‘: {checkpoint_dir}")
            print(f"   pipeline.yaml ì¡´ì¬: {(checkpoint_dir / 'pipeline.yaml').exists()}")

            if not (checkpoint_dir / "pipeline.yaml").exists():
                print("âŒ pipeline.yaml íŒŒì¼ì´ ì—†ìŒ")
                progress(0.1, desc="SAM 3D ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ë‹¤ìš´ë¡œë“œ ì‹œì‘...")

                download_success = self.download_sam3d_checkpoint(progress)

                if not download_success:
                    return None, """
### âŒ SAM 3D ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

**ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°©ë²•:**
```bash
cd /home/joon/dev/sam3d_gui
./download_sam3d.sh
```

ë˜ëŠ” ë‹¤ìŒ ëª…ë ¹ì–´:
```bash
cd ~/dev/sam-3d-objects
git clone https://huggingface.co/facebook/sam-3d-objects checkpoints/hf
```
"""

            # ëŒ€í‘œ í”„ë ˆì„ ì„ íƒ (ì¤‘ê°„ í”„ë ˆì„)
            mid_idx = len(self.frames) // 2
            frame = self.frames[mid_idx]
            mask = self.masks[mid_idx]

            print(f"\nâœ“ ëŒ€í‘œ í”„ë ˆì„ ì„ íƒ: {mid_idx + 1}/{len(self.frames)}")
            print(f"   Frame shape: {frame.shape}")
            print(f"   Mask shape: {mask.shape if mask is not None else 'None'}")
            print(f"   Mask type: {type(mask)}")

            if mask is None:
                print("âŒ ì¤‘ê°„ í”„ë ˆì„ì— ë§ˆìŠ¤í¬ ì—†ìŒ")
                return None, "ì¤‘ê°„ í”„ë ˆì„ì— ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤"

            # 3D ì¬êµ¬ì„± ì‹œë„
            print("\nâœ“ 3D ì¬êµ¬ì„± ì‹œì‘...")
            progress(0.5, desc="SAM 3D ì¬êµ¬ì„± ì¤‘...")

            # Unload SAM2 models to free GPU memory for SAM 3D
            # Critical for RTX 3060 12GB: SAM2 (3GB) + SAM3D (10GB) = 13GB > 12GB
            self.unload_sam2_models()

            try:
                reconstruction = self.processor.reconstruct_3d(frame, mask)
                print(f"âœ“ Reconstruction ì™„ë£Œ: {type(reconstruction)}")

                if reconstruction:
                    # PLY ì €ì¥
                    project_root = Path(__file__).parent.parent
                    output_dir = project_root / "outputs" / "3d_meshes"
                    output_dir.mkdir(parents=True, exist_ok=True)
                    output_path = output_dir / "reconstruction.ply"

                    print(f"\nâœ“ Mesh ì €ì¥ ì¤‘: {output_path}")
                    self.processor.export_mesh(reconstruction, str(output_path), format='ply')
                    print(f"âœ“ Mesh ì €ì¥ ì™„ë£Œ")

                    progress(1.0, desc="ì™„ë£Œ!")

                    status = f"""
### 3D Mesh ìƒì„± ì™„ë£Œ âœ…

- **í”„ë ˆì„**: {mid_idx + 1} / {len(self.frames)}
- **ì €ì¥ ìœ„ì¹˜**: `{output_path}`

### 3D ë·°ì–´ë¡œ í™•ì¸:
```bash
meshlab {output_path}
```

ë˜ëŠ” ì˜¨ë¼ì¸: https://3dviewer.net/
"""
                    print("âœ… generate_3d_mesh() ì™„ë£Œ")

                    # Reload SAM2 models for continued use
                    self.reload_sam2_models()

                    return str(output_path), status
                else:
                    print("âŒ Reconstructionì´ None")

                    # Reload SAM2 models even on failure
                    self.reload_sam2_models()

                    return None, "3D ì¬êµ¬ì„± ì‹¤íŒ¨ (SAM 3D ì²´í¬í¬ì¸íŠ¸ í•„ìš”)"

            except Exception as e:
                # SAM 3D ì—†ìœ¼ë©´ ê°„ë‹¨í•œ point cloudë§Œ ìƒì„±
                print(f"âŒ 3D ì¬êµ¬ì„± ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()

                # Reload SAM2 models even on failure
                self.reload_sam2_models()

                return None, f"3D ì¬êµ¬ì„± ì‹¤íŒ¨: {str(e)}\n\nSAM 3D ì²´í¬í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤."

        except Exception as e:
            import traceback
            return None, f"ì˜¤ë¥˜:\n{str(e)}\n{traceback.format_exc()}"

    def save_annotation_session(self, session_name: str = "") -> str:
        """
        Annotation ì„¸ì…˜ ì „ì²´ ì €ì¥ (annotation points + masks + metadata)

        Args:
            session_name: ì„¸ì…˜ ì´ë¦„ (ë¹„ì–´ìˆìœ¼ë©´ timestamp ì‚¬ìš©)
        """
        print("\n" + "="*80)
        print("ğŸ”¹ save_annotation_session() ì‹œì‘")
        print("="*80)

        if len(self.frames) == 0:
            print("âŒ ì €ì¥ ì‹¤íŒ¨: í”„ë ˆì„ ì—†ìŒ")
            return "ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"

        print(f"âœ“ í”„ë ˆì„ ìˆ˜: {len(self.frames)}")
        print(f"âœ“ ë§ˆìŠ¤í¬ ìˆ˜: {len(self.masks)}")
        print(f"âœ“ Foreground points: {len(self.annotations['foreground'])}")
        print(f"âœ“ Background points: {len(self.annotations['background'])}")

        try:
            # ì„¸ì…˜ ID ìƒì„±
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            if session_name and session_name.strip():
                # ì‚¬ìš©ì ì§€ì • ì´ë¦„ ì‚¬ìš© (timestamp ì¶”ê°€)
                session_id = f"{session_name.strip()}_{timestamp}"
            else:
                # timestampë§Œ ì‚¬ìš©
                session_id = timestamp

            print(f"âœ“ ì„¸ì…˜ ID ìƒì„±: {session_id}")

            output_dir = Path(f"outputs/sessions/{session_id}")
            output_dir.mkdir(parents=True, exist_ok=True)
            print(f"âœ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {output_dir}")

            # 1. Annotation ë©”íƒ€ë°ì´í„° ì €ì¥ (JSON)
            print("\nğŸ”¹ Step 1: ë©”íƒ€ë°ì´í„° êµ¬ì„± ì¤‘...")

            # Stride ê³„ì‚° (stride_frame_mappingì´ ìˆìœ¼ë©´ ì¶”ì¶œ, ì—†ìœ¼ë©´ 1ë¡œ ê°€ì •)
            effective_stride = 1
            if hasattr(self, 'stride_frame_mapping') and self.stride_frame_mapping:
                frame_indices = sorted(self.stride_frame_mapping.values())
                if len(frame_indices) > 1:
                    effective_stride = frame_indices[1] - frame_indices[0]

            num_frames_saved = sum(1 for m in self.masks if m is not None)

            metadata = {
                "session_id": session_id,
                "video_path": self.video_path,
                "num_frames": num_frames_saved,  # Load í•¨ìˆ˜ê°€ ì°¾ëŠ” í‚¤
                "num_frames_total": len(self.frames),
                "num_frames_saved": num_frames_saved,
                "stride": effective_stride,
                "current_frame_idx": self.current_frame_idx,
                "annotations": {
                    "foreground": self.annotations['foreground'],
                    "background": self.annotations['background']
                },
                "frame_info": []
            }
            print(f"âœ“ ë©”íƒ€ë°ì´í„° êµ¬ì„± ì™„ë£Œ (stride={effective_stride})")

            # 2. ê° í”„ë ˆì„ ì €ì¥ (stride ê°„ê²©ì˜ í”„ë ˆì„ë§Œ)
            print("\nğŸ”¹ Step 2: í”„ë ˆì„ë³„ ì €ì¥ ì‹œì‘ (ë§ˆìŠ¤í¬ê°€ ìˆëŠ” í”„ë ˆì„ë§Œ)...")
            saved_masks = 0
            saved_frame_idx = 0
            for i, (frame, mask) in enumerate(zip(self.frames, self.masks)):
                # ë§ˆìŠ¤í¬ê°€ ì—†ëŠ” í”„ë ˆì„ì€ ê±´ë„ˆë›°ê¸° (strideë¡œ ìƒëµëœ í”„ë ˆì„)
                if mask is None:
                    continue

                if saved_frame_idx % 10 == 0:  # 10í”„ë ˆì„ë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                    print(f"  ì§„í–‰: {saved_frame_idx} í”„ë ˆì„ ì €ì¥ë¨ (ì›ë³¸ ì¸ë±ìŠ¤: {i}/{len(self.frames)})...")

                frame_dir = output_dir / f"frame_{saved_frame_idx:04d}"
                frame_dir.mkdir(exist_ok=True)

                # ì›ë³¸ í”„ë ˆì„ ì €ì¥
                try:
                    frame_path = frame_dir / "original.png"
                    success = cv2.imwrite(str(frame_path), frame)
                    if not success:
                        print(f"  âš ï¸ í”„ë ˆì„ {i} ì €ì¥ ì‹¤íŒ¨: {frame_path}")
                except Exception as e:
                    print(f"  âŒ í”„ë ˆì„ {i} ì €ì¥ ì˜¤ë¥˜: {str(e)}")
                    raise

                # ë§ˆìŠ¤í¬ ì €ì¥ (ì´ë¯¸ mask is not None ì²´í¬ë¡œ ë“¤ì–´ì˜´)
                try:
                    mask_path = frame_dir / "mask.png"
                    mask_uint8 = mask.astype(np.uint8) * 255
                    success = cv2.imwrite(str(mask_path), mask_uint8)
                    if not success:
                        print(f"  âš ï¸ ë§ˆìŠ¤í¬ {i} ì €ì¥ ì‹¤íŒ¨: {mask_path}")

                    # ì‹œê°í™” (ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´) ì €ì¥
                    vis_path = frame_dir / "visualization.png"
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    overlay = frame_rgb.copy()
                    overlay[mask > 0] = [0, 255, 0]
                    result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)

                    # Annotation points í‘œì‹œ
                    for px, py in self.annotations['foreground']:
                        cv2.circle(result, (px, py), 5, (0, 255, 0), -1)
                        cv2.circle(result, (px, py), 7, (255, 255, 255), 2)
                    for px, py in self.annotations['background']:
                        cv2.circle(result, (px, py), 5, (255, 0, 0), -1)
                        cv2.circle(result, (px, py), 7, (255, 255, 255), 2)

                    result_bgr = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)
                    success = cv2.imwrite(str(vis_path), result_bgr)
                    if not success:
                        print(f"  âš ï¸ ì‹œê°í™” {i} ì €ì¥ ì‹¤íŒ¨: {vis_path}")

                    saved_masks += 1

                    # í”„ë ˆì„ ë©”íƒ€ë°ì´í„° (ì›ë³¸ ì¸ë±ìŠ¤ì™€ ì €ì¥ ì¸ë±ìŠ¤ ëª¨ë‘ ê¸°ë¡)
                    mask_area = np.sum(mask > 0)
                    metadata["frame_info"].append({
                        "saved_frame_idx": saved_frame_idx,  # Fauna í˜•ì‹ ì¸ë±ìŠ¤
                        "original_frame_idx": i,  # ì›ë³¸ ë¹„ë””ì˜¤ ì¸ë±ìŠ¤
                        "has_mask": True,
                        "mask_area": int(mask_area),
                        "mask_percentage": float(mask_area / mask.size * 100)
                    })

                    saved_frame_idx += 1

                except Exception as e:
                    print(f"  âŒ ë§ˆìŠ¤í¬ {i} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                    raise

            # ë§ˆìŠ¤í¬ê°€ ì—†ëŠ” í”„ë ˆì„ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°ëŠ” ë” ì´ìƒ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            # (strideë¡œ ìƒëµëœ í”„ë ˆì„)

            print(f"âœ“ í”„ë ˆì„ë³„ ì €ì¥ ì™„ë£Œ: ì›ë³¸ {len(self.frames)}ê°œ ì¤‘ ë§ˆìŠ¤í¬ê°€ ìˆëŠ” {saved_masks}ê°œë§Œ ì €ì¥")

            # 3. Metadata JSON ì €ì¥
            print("\nğŸ”¹ Step 3: ë©”íƒ€ë°ì´í„° JSON ì €ì¥ ì¤‘...")
            metadata_path = output_dir / "session_metadata.json"
            try:
                with open(metadata_path, 'w') as f:
                    json.dump(metadata, f, indent=2)
                print(f"âœ“ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_path}")
            except Exception as e:
                print(f"âŒ ë©”íƒ€ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {str(e)}")
                raise

            print("\n" + "="*80)
            print("âœ… save_annotation_session() ì™„ë£Œ!")
            print("="*80 + "\n")

            return f"""
### Annotation ì„¸ì…˜ ì €ì¥ ì™„ë£Œ âœ…

**ì„¸ì…˜ ID**: `{session_id}`

**ì €ì¥ ë‚´ìš©**:
- ğŸ“ ì›ë³¸ í”„ë ˆì„: {len(self.frames)}ê°œ
- ğŸ­ ë§ˆìŠ¤í¬: {saved_masks}ê°œ
- ğŸ“ Annotation points: {len(self.annotations['foreground'])} foreground, {len(self.annotations['background'])} background
- ğŸ“‹ ë©”íƒ€ë°ì´í„°: session_metadata.json

**ì €ì¥ ìœ„ì¹˜**: `{output_dir}/`

**ë””ë ‰í† ë¦¬ êµ¬ì¡°**:
```
{session_id}/
â”œâ”€â”€ session_metadata.json
â”œâ”€â”€ frame_0000/
â”‚   â”œâ”€â”€ original.png
â”‚   â”œâ”€â”€ mask.png
â”‚   â””â”€â”€ visualization.png
â”œâ”€â”€ frame_0001/
â”‚   â””â”€â”€ ...
```

**ì„¸ì…˜ ì¬ë¡œë“œ**: ì´ session_idë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚˜ì¤‘ì— ì¬ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print("\n" + "="*80)
            print("âŒ save_annotation_session() ì‹¤íŒ¨!")
            print("="*80)
            print(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            print(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
            print("\nì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
            print(error_detail)
            print("="*80 + "\n")
            return f"ì €ì¥ ì˜¤ë¥˜: {str(e)}\n\n```\n{error_detail}\n```"

    def load_annotation_session(self, session_id: str) -> Tuple[np.ndarray, str]:
        """
        ì €ì¥ëœ annotation ì„¸ì…˜ ë¡œë“œ
        """
        try:
            session_dir = Path(f"outputs/sessions/{session_id}")
            if not session_dir.exists():
                return None, f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}"

            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = session_dir / "session_metadata.json"
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)

            # í”„ë ˆì„ ë° ë§ˆìŠ¤í¬ ë¡œë“œ
            num_frames = metadata["num_frames"]
            self.frames = []
            self.masks = []

            for i in range(num_frames):
                frame_dir = session_dir / f"frame_{i:04d}"

                # ì›ë³¸ í”„ë ˆì„ ë¡œë“œ
                frame_path = frame_dir / "original.png"
                frame = cv2.imread(str(frame_path))
                self.frames.append(frame)

                # ë§ˆìŠ¤í¬ ë¡œë“œ (ìˆìœ¼ë©´)
                mask_path = frame_dir / "mask.png"
                if mask_path.exists():
                    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
                    self.masks.append((mask > 0).astype(bool))
                else:
                    self.masks.append(None)

            # Annotation points ë³µì›
            self.annotations = {
                'foreground': metadata["annotations"]["foreground"],
                'background': metadata["annotations"]["background"]
            }

            # ë¹„ë””ì˜¤ ê²½ë¡œ ë° í˜„ì¬ í”„ë ˆì„ ì¸ë±ìŠ¤ ë³µì›
            self.video_path = metadata["video_path"]
            self.current_frame_idx = metadata["current_frame_idx"]

            # í˜„ì¬ í”„ë ˆì„ ì‹œê°í™”
            current_frame = self.frames[self.current_frame_idx]
            current_mask = self.masks[self.current_frame_idx]

            frame_rgb = cv2.cvtColor(current_frame, cv2.COLOR_BGR2RGB)
            if current_mask is not None:
                overlay = frame_rgb.copy()
                overlay[current_mask > 0] = [0, 255, 0]
                result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)
            else:
                result = frame_rgb

            # Annotation points í‘œì‹œ
            for px, py in self.annotations['foreground']:
                cv2.circle(result, (px, py), 5, (0, 255, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)
            for px, py in self.annotations['background']:
                cv2.circle(result, (px, py), 5, (255, 0, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)

            masks_loaded = sum(1 for m in self.masks if m is not None)

            status = f"""
### ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ âœ…

**ì„¸ì…˜ ID**: `{session_id}`

**ë¡œë“œëœ ë°ì´í„°**:
- ğŸ“ í”„ë ˆì„: {len(self.frames)}ê°œ
- ğŸ­ ë§ˆìŠ¤í¬: {masks_loaded}ê°œ
- ğŸ“ Foreground points: {len(self.annotations['foreground'])}ê°œ
- ğŸ“ Background points: {len(self.annotations['background'])}ê°œ
- ğŸ“¹ ë¹„ë””ì˜¤: {self.video_path}

**í˜„ì¬ í”„ë ˆì„**: {self.current_frame_idx + 1} / {len(self.frames)}

ì´ì œ í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜, ì¶”ê°€ annotation, propagation ë“±ì„ ê³„ì†í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

            return result, status

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return None, f"ë¡œë“œ ì˜¤ë¥˜: {str(e)}\n\n```\n{error_detail}\n```"

    def get_session_ids(self) -> List[str]:
        """ì €ì¥ëœ ì„¸ì…˜ ID ëª©ë¡ ë°˜í™˜ (Dropdownìš©)"""
        try:
            sessions_dir = Path("outputs/sessions")
            if not sessions_dir.exists():
                return []

            sessions = sorted([d.name for d in sessions_dir.iterdir() if d.is_dir()], reverse=True)
            return sessions

        except Exception as e:
            print(f"ì„¸ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []

    def list_saved_sessions(self) -> str:
        """ì €ì¥ëœ ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ"""
        try:
            sessions_dir = Path("outputs/sessions")
            if not sessions_dir.exists():
                return "ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤"

            sessions = sorted([d.name for d in sessions_dir.iterdir() if d.is_dir()])

            if not sessions:
                return "ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤"

            result = "### ì €ì¥ëœ Annotation ì„¸ì…˜ ëª©ë¡\n\n"
            for session_id in sessions:
                metadata_path = sessions_dir / session_id / "session_metadata.json"
                if metadata_path.exists():
                    with open(metadata_path, 'r') as f:
                        metadata = json.load(f)
                    num_frames = metadata["num_frames"]
                    num_masks = sum(1 for info in metadata["frame_info"] if info.get("has_mask", False))
                    video_path = Path(metadata["video_path"]).name if metadata.get("video_path") else "Unknown"

                    result += f"""
**{session_id}**
- ë¹„ë””ì˜¤: `{video_path}`
- í”„ë ˆì„: {num_frames}ê°œ
- ë§ˆìŠ¤í¬: {num_masks}ê°œ
- ê²½ë¡œ: `outputs/sessions/{session_id}/`

---
"""

            return result

        except Exception as e:
            return f"ì˜¤ë¥˜: {str(e)}"

    def save_masks(self) -> str:
        """ë§ˆìŠ¤í¬ë§Œ ê°„ë‹¨íˆ ì €ì¥ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        if all(m is None for m in self.masks):
            return "ì €ì¥í•  ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤"

        try:
            output_dir = Path("outputs/masks")
            output_dir.mkdir(parents=True, exist_ok=True)

            saved_count = 0
            for i, mask in enumerate(self.masks):
                if mask is not None:
                    output_path = output_dir / f"mask_{i:04d}.png"
                    cv2.imwrite(str(output_path), mask.astype(np.uint8) * 255)
                    saved_count += 1

            return f"""
### ë§ˆìŠ¤í¬ ì €ì¥ ì™„ë£Œ âœ…

- **ì €ì¥ëœ ë§ˆìŠ¤í¬**: {saved_count} / {len(self.masks)}
- **ì €ì¥ ìœ„ì¹˜**: `{output_dir}/`

**ì°¸ê³ **: ì „ì²´ ì„¸ì…˜(annotation + masks + metadata)ì„ ì €ì¥í•˜ë ¤ë©´ **"ğŸ’¾ Save Session"** ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.
"""

        except Exception as e:
            return f"ì˜¤ë¥˜: {str(e)}"

    def navigate_frame(self, direction: str, step: int = 1) -> Tuple[np.ndarray, str]:
        """
        í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜

        Args:
            direction: "prev", "next", "first", "last", "goto"
            step: ì´ë™í•  í”„ë ˆì„ ìˆ˜
        """
        if len(self.frames) == 0:
            return None, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

        old_idx = self.current_frame_idx

        if direction == "prev":
            self.current_frame_idx = max(0, self.current_frame_idx - step)
        elif direction == "next":
            self.current_frame_idx = min(len(self.frames) - 1, self.current_frame_idx + step)
        elif direction == "first":
            self.current_frame_idx = 0
        elif direction == "last":
            self.current_frame_idx = len(self.frames) - 1
        elif direction == "goto":
            # stepì€ ì‹¤ì œ í”„ë ˆì„ ë²ˆí˜¸ (0-indexed)
            self.current_frame_idx = max(0, min(len(self.frames) - 1, step))

        # í˜„ì¬ í”„ë ˆì„ ì‹œê°í™”
        frame = self.frames[self.current_frame_idx]
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # ë§ˆìŠ¤í¬ê°€ ìˆìœ¼ë©´ í‘œì‹œ
        mask = self.masks[self.current_frame_idx]
        if mask is not None:
            overlay = frame_rgb.copy()
            overlay[mask > 0] = [0, 255, 0]
            result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)

            mask_area = np.sum(mask > 0)
            mask_pct = mask_area / mask.size * 100
            mask_info = f"ë§ˆìŠ¤í¬: {mask_area} í”½ì…€ ({mask_pct:.1f}%)"
        else:
            result = frame_rgb
            mask_info = "ë§ˆìŠ¤í¬ ì—†ìŒ"

        # Points í‘œì‹œ (annotationì´ ìˆìœ¼ë©´)
        if len(self.annotations['foreground']) > 0 or len(self.annotations['background']) > 0:
            for px, py in self.annotations['foreground']:
                cv2.circle(result, (px, py), 5, (0, 255, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)
            for px, py in self.annotations['background']:
                cv2.circle(result, (px, py), 5, (255, 0, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)

        status = f"""
### í”„ë ˆì„ {self.current_frame_idx + 1} / {len(self.frames)}

- **ì´ë™**: {old_idx + 1} â†’ {self.current_frame_idx + 1}
- **{mask_info}**
"""

        return result, status

    def clear_annotations(self) -> Tuple[np.ndarray, str]:
        """
        ëª¨ë“  annotation pointsì™€ masks ì´ˆê¸°í™”

        Returns:
            í˜„ì¬ í”„ë ˆì„ ì´ë¯¸ì§€, ìƒíƒœ ë©”ì‹œì§€
        """
        # Annotations ì´ˆê¸°í™”
        self.annotations['foreground'] = []
        self.annotations['background'] = []

        # Masks ì´ˆê¸°í™”
        self.masks = [None] * len(self.frames) if self.frames else []
        self.current_mask = None

        # í˜„ì¬ í”„ë ˆì„ ì´ë¯¸ì§€ ë°˜í™˜ (annotation ì—†ì´)
        if len(self.frames) > 0:
            current_frame = self.frames[self.current_frame_idx]
            frame_rgb = cv2.cvtColor(current_frame, cv2.COLOR_BGR2RGB)

            status = """
### Annotations ì´ˆê¸°í™” ì™„ë£Œ âœ…

- ëª¨ë“  foreground/background points ì œê±°
- ëª¨ë“  ë§ˆìŠ¤í¬ ì´ˆê¸°í™”
- ìƒˆë¡œ annotation ì‹œì‘ ê°€ëŠ¥

**ë‹¤ìŒ ë‹¨ê³„**: ì´ë¯¸ì§€ í´ë¦­í•˜ì—¬ ìƒˆë¡œìš´ annotation ì‹œì‘
"""
            return frame_rgb, status
        else:
            return None, "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

    def _find_next_sequence(self, fauna_root: Path, animal_name: str) -> str:
        """
        ë‹¤ìŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ ë²ˆí˜¸ ì°¾ê¸°

        Args:
            fauna_root: Fauna ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ
            animal_name: ë™ë¬¼ ì´ë¦„

        Returns:
            "seq_XXX" í˜•ì‹ì˜ ì‹œí€€ìŠ¤ ì´ë¦„
        """
        train_dir = fauna_root / animal_name / "train"

        # train ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ seq_000ë¶€í„° ì‹œì‘
        if not train_dir.exists():
            return "seq_000"

        # ê¸°ì¡´ seq_XXX ë””ë ‰í† ë¦¬ ì°¾ê¸°
        existing_sequences = [
            d.name for d in train_dir.iterdir()
            if d.is_dir() and d.name.startswith("seq_")
        ]

        # ê¸°ì¡´ ì‹œí€€ìŠ¤ê°€ ì—†ìœ¼ë©´ seq_000
        if not existing_sequences:
            return "seq_000"

        # ê°€ì¥ í° ì‹œí€€ìŠ¤ ë²ˆí˜¸ ì°¾ê¸°
        try:
            max_seq_num = max([int(s.split("_")[1]) for s in existing_sequences])
            next_seq_num = max_seq_num + 1
            return f"seq_{next_seq_num:03d}"
        except (IndexError, ValueError):
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•˜ê²Œ seq_000 ë°˜í™˜
            return "seq_000"

    def export_fauna_dataset(
        self,
        animal_name: str = "mouse",
        target_frames: int = 50,
        progress=gr.Progress()
    ) -> str:
        """
        Fauna ë°ì´í„°ì…‹ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§: ì „ì²´ ë¹„ë””ì˜¤ì—ì„œ target_frames ê°œë§Œ ê· ë“± ê°„ê²©ìœ¼ë¡œ ì„ íƒ
        ìë™ ì‹œí€€ìŠ¤ ë²ˆí˜¸ í• ë‹¹: ê¸°ì¡´ ì‹œí€€ìŠ¤ë¥¼ ë®ì–´ì“°ì§€ ì•ŠìŒ

        Args:
            animal_name: ë™ë¬¼ ì´ë¦„ (í´ë”ëª…)
            target_frames: ì €ì¥í•  í”„ë ˆì„ ìˆ˜ (ê¸°ë³¸ 50ê°œ)

        Returns:
            ìƒíƒœ ë©”ì‹œì§€
        """
        if len(self.frames) == 0:
            return "âŒ ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        if all(m is None for m in self.masks):
            return "âŒ ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Propagateë¥¼ ì‹¤í–‰í•˜ì„¸ìš”"

        try:
            from datetime import datetime

            progress(0, desc="Fauna ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")

            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì • - outputs í•˜ìœ„ì— ì²´ê³„ì ìœ¼ë¡œ ì €ì¥
            project_root = Path(__file__).parent.parent
            fauna_root = project_root / "outputs" / "fauna_datasets"
            sequence_name = self._find_next_sequence(fauna_root, animal_name)
            output_dir = fauna_root / animal_name / "train" / sequence_name
            output_dir.mkdir(parents=True, exist_ok=True)

            print(f"\nğŸ”¹ Fauna ë°ì´í„°ì…‹ ì €ì¥:")
            print(f"   Animal: {animal_name}")
            print(f"   Sequence: {sequence_name}")
            print(f"   Path: {output_dir}")

            # ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§: target_framesê°œë¥¼ ê· ë“± ê°„ê²©ìœ¼ë¡œ ì„ íƒ
            total_frames = len(self.frames)
            if total_frames <= target_frames:
                # í”„ë ˆì„ ìˆ˜ê°€ ì ìœ¼ë©´ ì „ë¶€ ì‚¬ìš©
                selected_indices = list(range(total_frames))
            else:
                # ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                step = total_frames / target_frames
                selected_indices = [int(i * step) for i in range(target_frames)]

            progress(0.1, desc=f"{len(selected_indices)}ê°œ í”„ë ˆì„ ì„ íƒë¨ (ì „ì²´ {total_frames}ê°œ ì¤‘)...")

            # í”„ë ˆì„ ë° ë§ˆìŠ¤í¬ ì €ì¥
            saved_count = 0
            for idx, frame_idx in enumerate(selected_indices):
                if self.masks[frame_idx] is None:
                    continue

                frame = self.frames[frame_idx]
                mask = self.masks[frame_idx]

                # Fauna í˜•ì‹: {index:07d}_rgb.png, {index:07d}_mask.png
                rgb_path = output_dir / f"{idx:07d}_rgb.png"
                mask_path = output_dir / f"{idx:07d}_mask.png"

                # RGB ì €ì¥ (BGR â†’ RGB)
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                cv2.imwrite(str(rgb_path), cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR))

                # Mask ì €ì¥ (0-255 í˜•ì‹)
                mask_uint8 = (mask * 255).astype(np.uint8)
                cv2.imwrite(str(mask_path), mask_uint8)

                saved_count += 1
                progress(0.1 + 0.8 * (idx + 1) / len(selected_indices),
                        desc=f"ì €ì¥ ì¤‘... {idx+1}/{len(selected_indices)}")

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "animal_name": animal_name,
                "sequence": sequence_name,
                "split": "train",
                "total_frames": saved_count,
                "original_video_frames": total_frames,
                "sampling_strategy": "uniform" if total_frames > target_frames else "all",
                "annotations": {
                    "foreground_points": len(self.annotations['foreground']),
                    "background_points": len(self.annotations['background'])
                },
                "export_date": datetime.now().isoformat(),
                "source_video": str(self.video_path) if self.video_path else None
            }

            metadata_path = output_dir / "metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            progress(1.0, desc="Fauna ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")

            return f"""
### Fauna ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ âœ…

**ì €ì¥ ìœ„ì¹˜**: `{output_dir}`
**ì‹œí€€ìŠ¤**: `{sequence_name}` (ìë™ í• ë‹¹ - ê¸°ì¡´ ë°ì´í„° ë³´ì¡´)

**ë°ì´í„°ì…‹ êµ¬ì¡°**:
```
{animal_name}/train/{sequence_name}/
â”œâ”€â”€ 0000000_rgb.png
â”œâ”€â”€ 0000000_mask.png
â”œâ”€â”€ 0000001_rgb.png
â”œâ”€â”€ 0000001_mask.png
...
â”œâ”€â”€ {saved_count-1:07d}_rgb.png
â”œâ”€â”€ {saved_count-1:07d}_mask.png
â””â”€â”€ metadata.json
```

**í†µê³„**:
- ì €ì¥ëœ í”„ë ˆì„: {saved_count}ê°œ
- ì›ë³¸ ë¹„ë””ì˜¤: {total_frames} í”„ë ˆì„
- ìƒ˜í”Œë§: {"ê· ë“± ê°„ê²© " + str(target_frames) + "ê°œ" if total_frames > target_frames else "ì „ì²´ ì‚¬ìš©"}

**ë‹¤ìŒ ë‹¨ê³„**:
1. ë°ì´í„° ê²€ì¦: `ls {output_dir} | head -20`
2. 3DAnimals í•™ìŠµ ì‹¤í–‰
3. ê²°ê³¼ í™•ì¸ ë° ì‹œê°í™”

**Config ì„¤ì • ì˜ˆì‹œ**:
```yaml
dataset:
  name: {animal_name}
  path: data/fauna/Fauna_dataset/large_scale/{animal_name}
  split: train
```
"""

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âŒ Fauna ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {str(e)}\n\n```\n{error_detail}\n```"

    def export_frames_and_masks(self, output_dir: str = None, progress=gr.Progress()) -> str:
        """
        í”„ë ˆì„ë³„ë¡œ ì›ë³¸ ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ë¥¼ ë³„ë„ í´ë”ì— ì €ì¥

        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ìë™ ìƒì„±)

        Returns:
            ìƒíƒœ ë©”ì‹œì§€
        """
        if len(self.frames) == 0:
            return "âŒ ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        if all(m is None for m in self.masks):
            return "âŒ ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Segment ë˜ëŠ” Propagateë¥¼ ì‹¤í–‰í•˜ì„¸ìš”"

        try:
            progress(0, desc="ì €ì¥ ì¤€ë¹„ ì¤‘...")

            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            if output_dir is None:
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_dir = Path(self.config.output_dir if self.config else "outputs") / f"frames_export_{timestamp}"
            else:
                output_dir = Path(output_dir)

            output_dir.mkdir(parents=True, exist_ok=True)

            # ì„œë¸Œë””ë ‰í† ë¦¬ ìƒì„±
            images_dir = output_dir / "images"
            masks_dir = output_dir / "masks"
            overlays_dir = output_dir / "overlays"

            images_dir.mkdir(exist_ok=True)
            masks_dir.mkdir(exist_ok=True)
            overlays_dir.mkdir(exist_ok=True)

            progress(0.1, desc="í”„ë ˆì„ ì €ì¥ ì¤‘...")

            saved_count = 0
            for i, frame in enumerate(self.frames):
                # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
                image_path = images_dir / f"frame_{i:05d}.png"
                cv2.imwrite(str(image_path), frame)

                # ë§ˆìŠ¤í¬ ì €ì¥ (ìˆì„ ê²½ìš°)
                if self.masks[i] is not None:
                    mask = self.masks[i]
                    mask_path = masks_dir / f"frame_{i:05d}.png"
                    mask_uint8 = (mask * 255).astype(np.uint8)
                    cv2.imwrite(str(mask_path), mask_uint8)

                    # ì˜¤ë²„ë ˆì´ ì €ì¥ (ì‹œê°í™”)
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    overlay = frame_rgb.copy()
                    overlay[mask > 0] = [0, 255, 0]
                    result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)
                    result_bgr = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)

                    overlay_path = overlays_dir / f"frame_{i:05d}.png"
                    cv2.imwrite(str(overlay_path), result_bgr)

                    saved_count += 1

                progress((i + 1) / len(self.frames), desc=f"ì €ì¥ ì¤‘... {i+1}/{len(self.frames)}")

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "video_path": str(self.video_path) if self.video_path else None,
                "total_frames": len(self.frames),
                "frames_with_masks": saved_count,
                "annotations": {
                    "foreground": self.annotations['foreground'],
                    "background": self.annotations['background']
                },
                "export_date": datetime.now().isoformat()
            }

            metadata_path = output_dir / "metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            progress(1.0, desc="ì €ì¥ ì™„ë£Œ!")

            return f"""
### í”„ë ˆì„/ë§ˆìŠ¤í¬ ì €ì¥ ì™„ë£Œ âœ…

**ì €ì¥ ìœ„ì¹˜**: `{output_dir}`

**ì €ì¥ëœ íŒŒì¼**:
- ğŸ“ **images/**: ì›ë³¸ í”„ë ˆì„ {len(self.frames)}ê°œ
- ğŸ“ **masks/**: ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ {saved_count}ê°œ
- ğŸ“ **overlays/**: ì‹œê°í™” ì´ë¯¸ì§€ {saved_count}ê°œ
- ğŸ“„ **metadata.json**: ë©”íƒ€ë°ì´í„°

**íŒŒì¼ í˜•ì‹**: PNG (ë¬´ì†ì‹¤)

**ë‹¤ìŒ ë‹¨ê³„**:
- ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©
- í•™ìŠµ ë°ì´í„°ì…‹ìœ¼ë¡œ í™œìš©
- ì™¸ë¶€ ë„êµ¬ë¡œ ì¶”ê°€ ë¶„ì„
"""

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}\n\n```\n{error_detail}\n```"

    # ===== Lite Annotator Event Handlers =====

    def _lite_load_source(
        self,
        input_path: str,
        input_type: str,
        pattern: str
    ) -> Tuple[str, gr.Slider, Dict]:
        """Load video or image folder"""
        if self.lite_annotator is None:
            return "âŒ Lite Annotator not initialized", gr.Slider(maximum=100), {}

        success, msg, total_frames = self.lite_annotator.change_input_source(
            input_path, input_type, pattern
        )

        if success:
            # Update slider maximum
            new_slider = gr.Slider(minimum=0, maximum=max(0, total_frames - 1), value=0, step=1)
            info = self.lite_annotator.get_info()
            return msg, new_slider, info
        else:
            return msg, gr.Slider(maximum=100), {}

    def _lite_load_model(self, model_name: str) -> str:
        """Load SAM 2.1 model"""
        if self.lite_annotator is None:
            return "âŒ Lite Annotator not initialized"

        msg = self.lite_annotator.load_model(model_name)
        return msg

    def _lite_load_frame(self, frame_idx: int) -> Tuple[Optional[np.ndarray], str, Dict]:
        """Load frame at index"""
        if self.lite_annotator is None:
            return None, "âŒ Lite Annotator not initialized", {}

        frame, msg = self.lite_annotator.load_frame(int(frame_idx))
        info = self.lite_annotator.get_info()

        return frame, msg, info

    def _lite_add_point(self, evt: gr.SelectData, point_type: str) -> Tuple[np.ndarray, str]:
        """Add point from click event"""
        if self.lite_annotator is None:
            return None, "âŒ Lite Annotator not initialized"

        x, y = evt.index[0], evt.index[1]
        frame, msg = self.lite_annotator.add_point(x, y, point_type)

        return frame, msg

    def _lite_generate_mask(self) -> Tuple[np.ndarray, Optional[np.ndarray], str]:
        """Generate mask from points"""
        if self.lite_annotator is None:
            return None, None, "âŒ Lite Annotator not initialized"

        frame_vis, mask_binary, msg = self.lite_annotator.generate_mask()

        return frame_vis, mask_binary, msg

    def _lite_save_annotation(self) -> str:
        """Save current annotation"""
        if self.lite_annotator is None:
            return "âŒ Lite Annotator not initialized"

        msg = self.lite_annotator.save_annotation()
        return msg

    def _lite_clear_points(self) -> Tuple[np.ndarray, str]:
        """Clear all points"""
        if self.lite_annotator is None:
            return None, "âŒ Lite Annotator not initialized"

        frame, msg = self.lite_annotator.clear_points()
        return frame, msg

    def create_interface(self):
        """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± - í†µí•© ë²„ì „"""

        with gr.Blocks(title="SAM 3D GUI - Unified Interface") as demo:
            gr.Markdown("""
            # ğŸ¬ SAM 3D GUI - í†µí•© ì›¹ ì¸í„°í˜ì´ìŠ¤

            **ë‘ ê°€ì§€ ì‘ì—… ëª¨ë“œ:**
            - ğŸš€ **Quick Mode**: ìë™ ì„¸ê·¸ë©˜í…Œì´ì…˜ & ëª¨ì…˜ ê°ì§€ (ë¹ ë¦„)
            - ğŸ¨ **Interactive Mode**: ìˆ˜ë™ annotation & propagation (ì •í™•í•¨)
            """)

            # ë¹„ë””ì˜¤ ìë™ ìŠ¤ìº” (Interactive Modeìš©)
            initial_videos = self.scan_videos(self.default_data_dir)
            initial_video = initial_videos[0] if initial_videos else None

            # ì„¸ì…˜ ìë™ ìŠ¤ìº”
            initial_sessions = self.get_session_ids()

            with gr.Tabs():
                # ===== Tab 1: Interactive Mode (ê¸°ë³¸) =====
                with gr.Tab("ğŸ¨ Interactive Mode"):
                    gr.Markdown("### ëŒ€í™”í˜• Annotation & Propagation")

                    with gr.Row():
                        with gr.Column(scale=1):
                            gr.Markdown("### ğŸ“ ë¹„ë””ì˜¤ ë¡œë“œ")

                            data_dir = gr.Textbox(
                                label="ë°ì´í„° ë””ë ‰í† ë¦¬",
                                value=self.default_data_dir
                            )

                            scan_video_btn = gr.Button("ğŸ“‚ ë¹„ë””ì˜¤ ìŠ¤ìº”")

                            video_file = gr.Dropdown(
                                label="ë¹„ë””ì˜¤ íŒŒì¼",
                                choices=initial_videos,
                                value=initial_video,
                                interactive=True
                            )

                            with gr.Row():
                                start_time = gr.Number(label="ì‹œì‘ (ì´ˆ)", value=0.0, minimum=0)
                                duration = gr.Number(label="ê¸¸ì´ (ì´ˆ)", value=3.0, minimum=0.1)

                            load_btn = gr.Button("ğŸ“¹ ë¹„ë””ì˜¤ ë¡œë“œ", variant="primary")

                            gr.Markdown("### ğŸ¯ Annotation")

                            annotation_mode = gr.Radio(
                                label="Point íƒ€ì…",
                                choices=["foreground", "background"],
                                value="foreground"
                            )

                            clear_btn = gr.Button("ğŸ—‘ï¸ Points ì´ˆê¸°í™”")
                            clear_all_btn = gr.Button("ğŸ”„ All Annotations ì´ˆê¸°í™”", variant="stop")
                            segment_btn = gr.Button("âœ‚ï¸ Segment Current Frame", variant="secondary")

                            gr.Markdown("### ğŸ¬ Propagation")

                            with gr.Row():
                                target_frames = gr.Number(
                                    label="ëª©í‘œ í”„ë ˆì„ ìˆ˜",
                                    value=300,
                                    minimum=10,
                                    maximum=1000,
                                    step=10,
                                    info="ì²˜ë¦¬í•  ì´ í”„ë ˆì„ ìˆ˜ (Stride ìë™ ê³„ì‚°)"
                                )
                                auto_stride = gr.Number(
                                    label="ìë™ Stride",
                                    value=10,
                                    interactive=False,
                                    info="ëª©í‘œ í”„ë ˆì„ ìˆ˜ ê¸°ë°˜ ìë™ ê³„ì‚°"
                                )

                            propagate_btn = gr.Button("ğŸ”„ Propagate to All Frames", variant="primary")

                            gr.Markdown("### ğŸï¸ í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜")

                            # í”„ë ˆì„ í”„ë¡œê·¸ë ˆìŠ¤ ë°” (ì§ì ‘ ì´ë™ ê°€ëŠ¥)
                            frame_slider = gr.Slider(
                                label="í”„ë ˆì„ ìœ„ì¹˜",
                                minimum=0,
                                maximum=100,
                                value=0,
                                step=1,
                                interactive=True,
                                info="ìŠ¬ë¼ì´ë”ë¥¼ ë“œë˜ê·¸í•˜ì—¬ í”„ë ˆì„ ì´ë™"
                            )

                            with gr.Row():
                                first_btn = gr.Button("â®ï¸ ì²˜ìŒ", size="sm")
                                prev_btn = gr.Button("â—€ï¸ ì´ì „", size="sm")
                                next_btn = gr.Button("â–¶ï¸ ë‹¤ìŒ", size="sm")
                                last_btn = gr.Button("â­ï¸ ë§ˆì§€ë§‰", size="sm")

                            with gr.Row():
                                frame_step = gr.Slider(
                                    label="ì´ë™ ê°„ê²© (Stride)",
                                    minimum=1,
                                    maximum=100,
                                    value=1,
                                    step=1,
                                    info="Propagate ì‹œì—ë„ ì´ ê°„ê²©ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤"
                                )

                            with gr.Row():
                                goto_frame = gr.Number(
                                    label="í”„ë ˆì„ ë²ˆí˜¸",
                                    value=1,
                                    minimum=1,
                                    step=1,
                                    scale=2
                                )
                                goto_btn = gr.Button("ì´ë™", scale=1)

                            gr.Markdown("### ğŸ’¾ ì„¸ì…˜ ê´€ë¦¬")

                            save_session_btn = gr.Button("ğŸ’¾ Save Session", variant="primary")

                            gr.Markdown("**ì„¸ì…˜ ë¡œë“œ**")
                            with gr.Row():
                                session_refresh_btn = gr.Button("ğŸ”„ ëª©ë¡ ìƒˆë¡œê³ ì¹¨", size="sm")

                            session_id_dropdown = gr.Dropdown(
                                label="ì„¸ì…˜ ì„ íƒ",
                                choices=initial_sessions,
                                value=initial_sessions[0] if initial_sessions else None,
                                interactive=True,
                                scale=2
                            )
                            load_session_btn = gr.Button("ğŸ“‚ Load Session")

                            gr.Markdown("### ğŸ’¾ ì„¸ì…˜ ì €ì¥")

                            session_name_input = gr.Textbox(
                                label="ì„¸ì…˜ ì´ë¦„ (ì„ íƒì‚¬í•­)",
                                placeholder="ì˜ˆ: mouse_experiment_1",
                                info="ë¹„ì–´ìˆìœ¼ë©´ timestampë§Œ ì‚¬ìš©"
                            )

                            save_session_btn = gr.Button("ğŸ’¾ Save Session", variant="secondary")

                            gr.Markdown("### ğŸ² 3D & ì¶œë ¥")

                            mesh_btn = gr.Button("ğŸ² Generate 3D Mesh")
                            save_masks_btn = gr.Button("ğŸ’¾ Save Masks Only")
                            export_frames_btn = gr.Button("ğŸ“¤ Export Frames & Masks")

                            gr.Markdown("### ğŸ¦ Fauna ë°ì´í„°ì…‹ ì €ì¥")

                            with gr.Row():
                                fauna_animal_name = gr.Textbox(
                                    label="ë™ë¬¼ ì´ë¦„",
                                    value="mouse",
                                    placeholder="ì˜ˆ: mouse, cat, dog"
                                )
                                fauna_target_frames = gr.Number(
                                    label="ëª©í‘œ í”„ë ˆì„ ìˆ˜",
                                    value=50,
                                    minimum=10,
                                    maximum=500,
                                    step=10
                                )

                            export_fauna_btn = gr.Button("ğŸ¾ Fauna í˜•ì‹ìœ¼ë¡œ ì €ì¥", variant="primary")

                        # ìš°ì¸¡: ì´ë¯¸ì§€ & ê²°ê³¼
                        with gr.Column(scale=2):
                            gr.Markdown("### ğŸ–¼ï¸ Annotation & Results")

                            image_display = gr.Image(
                                label="ì´ë¯¸ì§€ (í´ë¦­í•˜ì—¬ point ì¶”ê°€)",
                                type="numpy",
                                height=500,
                                interactive=True
                            )

                            status_text = gr.Markdown("ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”")

                            mesh_file = gr.File(label="3D Mesh íŒŒì¼")

                    # Interactive Mode ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
                    # ë¹„ë””ì˜¤ íŒŒì¼ ì„ íƒ ì‹œ duration ìë™ ì—…ë°ì´íŠ¸
                    video_file.change(
                        fn=self.get_video_duration,
                        inputs=[data_dir, video_file],
                        outputs=[duration]
                    )

                    load_btn.click(
                        fn=self.load_video,
                        inputs=[data_dir, video_file, start_time, duration],
                        outputs=[image_display, status_text, frame_slider]
                    )

                    # ìŠ¬ë¼ì´ë”ë¡œ í”„ë ˆì„ ì´ë™
                    frame_slider.change(
                        fn=lambda frame_idx: self.navigate_frame("goto", int(frame_idx)),
                        inputs=[frame_slider],
                        outputs=[image_display, status_text]
                    )

                    # ì´ë¯¸ì§€ í´ë¦­ ì‹œ point ì¶”ê°€
                    def handle_click(mode, evt: gr.SelectData):
                        """ì´ë¯¸ì§€ í´ë¦­ í•¸ë“¤ëŸ¬ - img íŒŒë¼ë¯¸í„° ì œê±°"""
                        if len(self.frames) == 0:
                            return None, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

                        # í´ë¦­ ì¢Œí‘œ
                        x, y = evt.index[0], evt.index[1]

                        # Point ì¶”ê°€
                        self.annotations[mode].append((x, y))

                        # í˜„ì¬ í”„ë ˆì„ì— point í‘œì‹œ
                        frame = self.frames[self.current_frame_idx].copy()
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                        # Foreground points (ë…¹ìƒ‰)
                        for px, py in self.annotations['foreground']:
                            cv2.circle(frame_rgb, (px, py), 5, (0, 255, 0), -1)
                            cv2.circle(frame_rgb, (px, py), 7, (255, 255, 255), 2)

                        # Background points (ë¹¨ê°„ìƒ‰)
                        for px, py in self.annotations['background']:
                            cv2.circle(frame_rgb, (px, py), 5, (255, 0, 0), -1)
                            cv2.circle(frame_rgb, (px, py), 7, (255, 255, 255), 2)

                        status = f"""
**Annotations:**
- Foreground: {len(self.annotations['foreground'])} points
- Background: {len(self.annotations['background'])} points

í´ë¦­í•œ ìœ„ì¹˜: ({x}, {y}) - {mode}
"""

                        return frame_rgb, status

                    image_display.select(
                        fn=handle_click,
                        inputs=[annotation_mode],
                        outputs=[image_display, status_text]
                    )

                    segment_btn.click(
                        fn=self.segment_current_frame,
                        outputs=[image_display, status_text]
                    )

                    # ëª©í‘œ í”„ë ˆì„ ìˆ˜ ë³€ê²½ ì‹œ auto_stride ìë™ ê³„ì‚°
                    target_frames.change(
                        fn=self.calculate_stride_from_target,
                        inputs=[target_frames],
                        outputs=[auto_stride]
                    )

                    propagate_btn.click(
                        fn=self.propagate_to_all_frames,
                        inputs=[auto_stride],  # frame_step ëŒ€ì‹  auto_stride ì‚¬ìš©
                        outputs=[image_display, status_text]
                    )

                    save_session_btn.click(
                        fn=self.save_annotation_session,
                        inputs=[session_name_input],
                        outputs=[status_text]
                    )

                    mesh_btn.click(
                        fn=self.generate_3d_mesh,
                        outputs=[mesh_file, status_text]
                    )

                    save_masks_btn.click(
                        fn=self.save_masks,
                        outputs=[status_text]
                    )

                    export_frames_btn.click(
                        fn=self.export_frames_and_masks,
                        outputs=[status_text]
                    )

                    export_fauna_btn.click(
                        fn=self.export_fauna_dataset,
                        inputs=[fauna_animal_name, fauna_target_frames],
                        outputs=[status_text]
                    )

                    clear_all_btn.click(
                        fn=self.clear_annotations,
                        outputs=[image_display, status_text]
                    )

                    # ì„¸ì…˜ ê´€ë¦¬ ì´ë²¤íŠ¸
                    save_session_btn.click(
                        fn=self.save_annotation_session,
                        outputs=[status_text]
                    )

                    session_refresh_btn.click(
                        fn=lambda: gr.Dropdown(choices=self.get_session_ids()),
                        outputs=[session_id_dropdown]
                    )

                    load_session_btn.click(
                        fn=self.load_annotation_session,
                        inputs=[session_id_dropdown],
                        outputs=[image_display, status_text]
                    )

                    def clear_points():
                        self.annotations = {'foreground': [], 'background': []}
                        if len(self.frames) > 0:
                            frame_rgb = cv2.cvtColor(self.frames[self.current_frame_idx], cv2.COLOR_BGR2RGB)
                            return frame_rgb, "Points ì´ˆê¸°í™”ë¨"
                        return None, "Points ì´ˆê¸°í™”ë¨"

                    clear_btn.click(
                        fn=clear_points,
                        outputs=[image_display, status_text]
                    )

                    # ë¹„ë””ì˜¤ ìŠ¤ìº” ë²„íŠ¼
                    scan_video_btn.click(
                        fn=lambda d: gr.Dropdown(choices=self.scan_videos(d)),
                        inputs=[data_dir],
                        outputs=[video_file]
                    )

                    # í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜ ì´ë²¤íŠ¸
                    first_btn.click(
                        fn=lambda: self.navigate_frame("first"),
                        outputs=[image_display, status_text]
                    )

                    prev_btn.click(
                        fn=lambda step: self.navigate_frame("prev", int(step)),
                        inputs=[frame_step],
                        outputs=[image_display, status_text]
                    )

                    next_btn.click(
                        fn=lambda step: self.navigate_frame("next", int(step)),
                        inputs=[frame_step],
                        outputs=[image_display, status_text]
                    )

                    last_btn.click(
                        fn=lambda: self.navigate_frame("last"),
                        outputs=[image_display, status_text]
                    )

                    goto_btn.click(
                        fn=lambda frame_num: self.navigate_frame("goto", int(frame_num) - 1),  # 1-indexed to 0-indexed
                        inputs=[goto_frame],
                        outputs=[image_display, status_text]
                    )

                # ===== Tab 2: Batch Mode =====
                with gr.Tab("ğŸ“¦ Batch Mode"):
                    gr.Markdown("### ì—¬ëŸ¬ ë¹„ë””ì˜¤ ì¼ê´„ ì²˜ë¦¬")

                    with gr.Row():
                        with gr.Column(scale=1):
                            gr.Markdown("### ğŸ“‚ ë¹„ë””ì˜¤ ìŠ¤ìº”")

                            batch_data_dir = gr.Textbox(
                                label="ë¹„ë””ì˜¤ í´ë”",
                                value=self.default_data_dir
                            )

                            batch_pattern = gr.Textbox(
                                label="íŒŒì¼ íŒ¨í„´",
                                value="*.mp4",
                                info="ì˜ˆ: *.mp4, *.avi, video_*.mp4"
                            )

                            batch_scan_btn = gr.Button("ğŸ“‚ ë¹„ë””ì˜¤ ìŠ¤ìº”", variant="primary")

                            batch_info = gr.Markdown("ë¹„ë””ì˜¤ë¥¼ ìŠ¤ìº”í•˜ì„¸ìš”")

                            # ë¹„ë””ì˜¤ ì„ íƒ UI (Accordionìœ¼ë¡œ ê°ì‹¸ê¸°)
                            with gr.Accordion("ğŸ¬ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ì„ íƒ", open=True):
                                # ì „ì²´ ì„ íƒ/í•´ì œ ë²„íŠ¼ (ìƒë‹¨)
                                with gr.Row():
                                    batch_select_all_btn = gr.Button("âœ… ì „ì²´ ì„ íƒ", size="sm")
                                    batch_deselect_all_btn = gr.Button("âŒ ì „ì²´ í•´ì œ", size="sm")

                                batch_video_selection = gr.CheckboxGroup(
                                    label="ë¹„ë””ì˜¤ ëª©ë¡",
                                    choices=[],
                                    value=[],
                                    interactive=True,
                                    info="ì„ íƒëœ ë¹„ë””ì˜¤ë§Œ ì²˜ë¦¬ë©ë‹ˆë‹¤"
                                )

                            gr.Markdown("### ğŸ¯ Reference Annotation")

                            gr.Markdown("""
ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ì˜ ëŒ€í‘œ í”„ë ˆì„ì— annotationì„ ì¶”ê°€í•˜ì„¸ìš”.
ëª¨ë“  ë¹„ë””ì˜¤ì— ë™ì¼í•œ annotationì´ ì ìš©ë©ë‹ˆë‹¤.
                            """)

                            # Interactive Modeì—ì„œ ì‚¬ìš©í•˜ëŠ” annotation UI ì¬ì‚¬ìš©
                            batch_load_ref_btn = gr.Button("ğŸ“¹ Reference í”„ë ˆì„ ë¡œë“œ")

                            batch_annotation_mode = gr.Radio(
                                label="Point íƒ€ì…",
                                choices=["foreground", "background"],
                                value="foreground"
                            )

                            with gr.Row():
                                batch_segment_btn = gr.Button("ğŸ¯ Segment (ë¯¸ë¦¬ë³´ê¸°)", variant="secondary", size="sm")
                                batch_clear_btn = gr.Button("ğŸ—‘ï¸ Points ì´ˆê¸°í™”", size="sm")

                            gr.Markdown("### âš™ï¸ Batch ì„¤ì •")

                            with gr.Row():
                                batch_target_frames = gr.Number(
                                    label="ë¹„ë””ì˜¤ë‹¹ ëª©í‘œ í”„ë ˆì„ ìˆ˜",
                                    value=100,
                                    minimum=10,
                                    maximum=500,
                                    step=10,
                                    info="ê° ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œí•  ëª©í‘œ í”„ë ˆì„ ìˆ˜ (ì‹¤ì œ stride ìë™ ê³„ì‚°)"
                                )

                            batch_propagate_btn = gr.Button("ğŸ”„ Batch Propagate", variant="primary", size="lg")

                            gr.Markdown("### ğŸ’¾ ì„¸ì…˜ ê´€ë¦¬")

                            # ì„¸ì…˜ ë¡œë“œ
                            with gr.Accordion("ğŸ“‚ ì„¸ì…˜ ë¶ˆëŸ¬ì˜¤ê¸°", open=False):
                                batch_load_session_path = gr.Textbox(
                                    label="ì„¸ì…˜ ê²½ë¡œ",
                                    placeholder="ì˜ˆ: outputs/sessions/mouse_batch_20251125_123456",
                                    info="ì„¸ì…˜ í´ë” ê²½ë¡œ ë˜ëŠ” session_metadata.json ê²½ë¡œ"
                                )
                                batch_load_session_btn = gr.Button("ğŸ“‚ ì„¸ì…˜ ë¡œë“œ", variant="secondary")

                            # ì„¸ì…˜ ì €ì¥ ë° Export
                            batch_session_name = gr.Textbox(
                                label="ì„¸ì…˜/ë°ì´í„°ì…‹ ì´ë¦„",
                                value="mouse_batch",
                                placeholder="ì˜ˆ: mouse_experiment_batch",
                                info="ì„¸ì…˜ ì €ì¥ ë° Fauna export ì‹œ ì‚¬ìš©"
                            )

                            batch_file_structure = gr.Radio(
                                choices=[
                                    ("ğŸ“ ë¹„ë””ì˜¤ë³„ í´ë” (video001/frame_0000_rgb.png)", "video_folders"),
                                    ("ğŸ“„ ì™„ì „ í‰ë©´ (video001_frame_0000_rgb.png)", "flat")
                                ],
                                value="video_folders",
                                label="ğŸ“ íŒŒì¼ êµ¬ì¡° (Export ì‹œ)",
                                info="ë¹„ë””ì˜¤ë³„ë¡œ í´ë” êµ¬ì¡° vs ëª¨ë“  íŒŒì¼ í‰ë©´ êµ¬ì¡°"
                            )

                            with gr.Row():
                                batch_save_session_btn = gr.Button("ğŸ’¾ Save Session", variant="secondary")
                                batch_export_btn = gr.Button("ğŸ“¦ Export to Fauna", variant="secondary")

                        with gr.Column(scale=2):
                            batch_image_display = gr.Image(
                                label="Reference Frame",
                                type="numpy"
                            )

                            batch_status_text = gr.Markdown("### ìƒíƒœ: ëŒ€ê¸° ì¤‘")

                            batch_output_path = gr.Textbox(
                                label="ì¶œë ¥ ê²½ë¡œ",
                                interactive=False
                            )

                    # Event handlers
                    batch_scan_btn.click(
                        fn=self.scan_batch_videos,
                        inputs=[batch_data_dir, batch_pattern],
                        outputs=[gr.State(), batch_info, batch_video_selection]
                    )

                    # ì „ì²´ ì„ íƒ/í•´ì œ ë²„íŠ¼
                    def select_all_videos():
                        if hasattr(self, 'batch_video_label_map'):
                            all_labels = list(self.batch_video_label_map.keys())
                            return gr.CheckboxGroup(value=all_labels)
                        return gr.CheckboxGroup(value=[])

                    def deselect_all_videos():
                        return gr.CheckboxGroup(value=[])

                    batch_select_all_btn.click(
                        fn=select_all_videos,
                        outputs=[batch_video_selection]
                    )

                    batch_deselect_all_btn.click(
                        fn=deselect_all_videos,
                        outputs=[batch_video_selection]
                    )

                    # Reference frame ë¡œë“œ
                    batch_load_ref_btn.click(
                        fn=self.batch_load_reference_frame,
                        inputs=[batch_video_selection],
                        outputs=[batch_image_display, batch_status_text]
                    )

                    # Batch ëª¨ë“œ point annotation í´ë¦­ ì´ë²¤íŠ¸
                    batch_image_display.select(
                        fn=self.add_point,
                        inputs=[batch_image_display, batch_annotation_mode],
                        outputs=[batch_image_display, batch_status_text]
                    )

                    # Batch segment (ë¯¸ë¦¬ë³´ê¸°)
                    batch_segment_btn.click(
                        fn=self.segment_current_frame,
                        outputs=[batch_image_display, batch_status_text]
                    )

                    # Batch clear points
                    def batch_clear_points():
                        self.annotations = {'foreground': [], 'background': []}
                        if len(self.frames) > 0:
                            frame_rgb = cv2.cvtColor(self.frames[self.current_frame_idx], cv2.COLOR_BGR2RGB)
                            return frame_rgb, "Points ì´ˆê¸°í™”ë¨"
                        return None, "Points ì´ˆê¸°í™”ë¨"

                    batch_clear_btn.click(
                        fn=batch_clear_points,
                        outputs=[batch_image_display, batch_status_text]
                    )

                    batch_propagate_btn.click(
                        fn=self.batch_propagate_videos,
                        inputs=[batch_target_frames, batch_video_selection],
                        outputs=[batch_status_text, gr.State()]
                    )

                    # ì„¸ì…˜ ë¡œë“œ
                    batch_load_session_btn.click(
                        fn=self.load_batch_session,
                        inputs=[batch_load_session_path],
                        outputs=[batch_status_text, batch_output_path]
                    )

                    # ì„¸ì…˜ ì €ì¥
                    batch_save_session_btn.click(
                        fn=self.save_batch_session,
                        inputs=[batch_session_name],
                        outputs=[batch_output_path, batch_status_text]
                    )

                    # Fauna export
                    batch_export_btn.click(
                        fn=self.export_batch_to_fauna,
                        inputs=[batch_session_name, batch_file_structure],
                        outputs=[batch_output_path, batch_status_text]
                    )

                # ===== Tab 3: Quick Mode =====
                with gr.Tab("ğŸš€ Quick Mode"):
                    gr.Markdown("### ë¹ ë¥¸ ìë™ ì²˜ë¦¬")

                    with gr.Row():
                        with gr.Column(scale=1):
                            quick_data_dir = gr.Textbox(
                                label="ë°ì´í„° ë””ë ‰í† ë¦¬",
                                value=self.default_data_dir
                            )

                            quick_scan_btn = gr.Button("ğŸ“‚ ë¹„ë””ì˜¤ ìŠ¤ìº”")

                            quick_video_list = gr.Dropdown(
                                label="ë¹„ë””ì˜¤ íŒŒì¼",
                                choices=initial_videos,
                                value=initial_video,
                                interactive=True
                            )

                            with gr.Row():
                                quick_start = gr.Number(label="ì‹œì‘(ì´ˆ)", value=0.0)
                                quick_duration = gr.Number(label="ê¸¸ì´(ì´ˆ)", value=3.0)

                            quick_threshold = gr.Slider(
                                label="ëª¨ì…˜ ì„ê³„ê°’",
                                minimum=0, maximum=200, value=50.0, step=1
                            )

                            quick_method = gr.Radio(
                                label="ì„¸ê·¸ë©˜í…Œì´ì…˜",
                                choices=["contour", "simple_threshold", "grabcut"],
                                value="contour"
                            )

                            quick_process_btn = gr.Button("ğŸš€ ìë™ ì²˜ë¦¬", variant="primary", size="lg")

                        with gr.Column(scale=2):
                            quick_image = gr.Image(label="ê²°ê³¼", type="numpy", height=500)
                            quick_status = gr.Markdown("ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ê³  ì²˜ë¦¬í•˜ì„¸ìš”")

                    # Quick Mode ì´ë²¤íŠ¸
                    quick_scan_btn.click(
                        fn=lambda d: gr.Dropdown(choices=self.scan_videos(d)),
                        inputs=[quick_data_dir],
                        outputs=[quick_video_list]
                    )

                    quick_process_btn.click(
                        fn=self.quick_process,
                        inputs=[quick_data_dir, quick_video_list, quick_start,
                               quick_duration, quick_threshold, quick_method],
                        outputs=[quick_image, quick_status]
                    )

                # ===== Tab 3: Lite Annotator =====
                with gr.Tab("ğŸ“ Lite Annotator"):
                    gr.Markdown("### íš¨ìœ¨ì  Annotation ëª¨ë“œ")
                    gr.Markdown("Direct video/image loading, multi-model selection, auto-restore")

                    with gr.Row():
                        # Left column: Input & Frame Display
                        with gr.Column(scale=2):
                            # Input source section
                            gr.Markdown("#### ğŸ“‚ Input Source")
                            with gr.Row():
                                lite_input_path = gr.Textbox(
                                    label="Video/Image Folder Path",
                                    placeholder="/path/to/video.mp4 or /path/to/images/",
                                    scale=3
                                )
                                lite_input_type = gr.Radio(
                                    choices=["video", "images"],
                                    value="video",
                                    label="Type",
                                    scale=1
                                )

                            with gr.Row():
                                lite_pattern = gr.Textbox(
                                    label="Image Pattern (for images type)",
                                    value="*.png",
                                    scale=2
                                )
                                lite_load_btn = gr.Button("ğŸ“¥ Load Source", variant="primary", scale=1)

                            lite_load_status = gr.Markdown("No input loaded")

                            # Frame display
                            gr.Markdown("#### ğŸ–¼ï¸ Frame")
                            lite_frame_display = gr.Image(
                                label="Current Frame",
                                type="numpy",
                                height=500,
                                interactive=True  # Enable click events
                            )

                            # Frame navigation
                            with gr.Row():
                                lite_frame_slider = gr.Slider(
                                    label="Frame Index",
                                    minimum=0,
                                    maximum=100,
                                    value=0,
                                    step=1,
                                    interactive=True
                                )

                        # Right column: Controls & Mask Display
                        with gr.Column(scale=1):
                            # Model selection
                            gr.Markdown("#### ğŸ¤– Model Selection")
                            lite_model_dropdown = gr.Dropdown(
                                choices=["tiny", "small", "base_plus", "large"],
                                value="large",
                                label="SAM 2.1 Model",
                                info="tiny: fastest, large: best quality"
                            )
                            lite_load_model_btn = gr.Button("Load Model", size="sm")

                            # Point annotation
                            gr.Markdown("#### ğŸ¨ Annotation")
                            lite_point_type = gr.Radio(
                                choices=["foreground", "background"],
                                value="foreground",
                                label="Point Type"
                            )

                            # Action buttons
                            with gr.Column():
                                lite_generate_btn = gr.Button("ğŸ¯ Generate Mask", variant="primary")
                                lite_save_btn = gr.Button("ğŸ’¾ Save Annotation", variant="secondary")
                                lite_clear_btn = gr.Button("ğŸ”„ Clear Points", variant="stop")

                            # Mask display
                            gr.Markdown("#### ğŸ­ Mask")
                            lite_mask_display = gr.Image(
                                label="Generated Mask",
                                type="numpy",
                                height=200
                            )

                            # Status
                            lite_status = gr.Markdown("**Status:** Load source to start")

                            # Info panel
                            gr.Markdown("#### â„¹ï¸ Info")
                            lite_info = gr.JSON(label="Current State", value={})

                    # Event handlers for Lite Annotator

                    # Load source
                    lite_load_btn.click(
                        fn=self._lite_load_source,
                        inputs=[lite_input_path, lite_input_type, lite_pattern],
                        outputs=[lite_load_status, lite_frame_slider, lite_info]
                    )

                    # Load model
                    lite_load_model_btn.click(
                        fn=self._lite_load_model,
                        inputs=[lite_model_dropdown],
                        outputs=[lite_status]
                    )

                    # Frame slider change
                    lite_frame_slider.change(
                        fn=self._lite_load_frame,
                        inputs=[lite_frame_slider],
                        outputs=[lite_frame_display, lite_status, lite_info]
                    )

                    # Click on frame to add point
                    lite_frame_display.select(
                        fn=self._lite_add_point,
                        inputs=[lite_point_type],
                        outputs=[lite_frame_display, lite_status]
                    )

                    # Generate mask
                    lite_generate_btn.click(
                        fn=self._lite_generate_mask,
                        outputs=[lite_frame_display, lite_mask_display, lite_status]
                    )

                    # Save annotation
                    lite_save_btn.click(
                        fn=self._lite_save_annotation,
                        outputs=[lite_status]
                    )

                    # Clear points
                    lite_clear_btn.click(
                        fn=self._lite_clear_points,
                        outputs=[lite_frame_display, lite_status]
                    )

                # ===== Tab 4: Data Augmentation =====
                with gr.Tab("ğŸ² Data Augmentation"):
                    gr.Markdown("### ë°ì´í„° ì¦ê°•")
                    gr.Markdown("RGB ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ë¥¼ í•¨ê»˜ ì¦ê°•í•©ë‹ˆë‹¤. ê¸°í•˜í•™ì  ë³€í™˜ì€ ë™ì¼í•˜ê²Œ, ìƒ‰ìƒ ë³€í™˜ì€ RGBë§Œ ì ìš©ë©ë‹ˆë‹¤.")

                    with gr.Row():
                        # Left column: Input & Controls
                        with gr.Column(scale=1):
                            gr.Markdown("#### ğŸ“‚ Input Source")

                            # Session selection
                            aug_session_dir = gr.Textbox(
                                label="Session Directory",
                                value=str(Path(self.default_output_dir) / "sessions"),
                                placeholder="Path to saved annotation sessions"
                            )

                            aug_scan_btn = gr.Button("ğŸ“‚ Scan Sessions", size="sm")

                            aug_session_list = gr.Dropdown(
                                label="Select Session",
                                choices=[],
                                interactive=True
                            )

                            aug_load_session_btn = gr.Button("ğŸ“¥ Load Session", variant="primary")

                            aug_session_info = gr.Markdown("No session loaded")

                            # Augmentation parameters
                            gr.Markdown("#### âš™ï¸ Augmentation Parameters")

                            with gr.Accordion("ğŸ”„ Geometric Transforms (RGB + Mask)", open=True):
                                aug_scale_enable = gr.Checkbox(label="Enable Scale", value=True)
                                with gr.Row():
                                    aug_scale_min = gr.Slider(
                                        label="Scale Min",
                                        minimum=0.3, maximum=1.0, value=0.5, step=0.05
                                    )
                                    aug_scale_max = gr.Slider(
                                        label="Scale Max",
                                        minimum=1.0, maximum=3.0, value=2.0, step=0.1
                                    )

                                aug_fill_color = gr.Dropdown(
                                    label="Fill Color (for shrinking)",
                                    choices=["white", "black", "nearest"],
                                    value="white"
                                )

                                aug_rotation_enable = gr.Checkbox(label="Enable Rotation", value=True)
                                with gr.Row():
                                    aug_rotation_min = gr.Slider(
                                        label="Rotation Min (deg)",
                                        minimum=-180, maximum=0, value=-30, step=5
                                    )
                                    aug_rotation_max = gr.Slider(
                                        label="Rotation Max (deg)",
                                        minimum=0, maximum=180, value=30, step=5
                                    )

                                aug_flip_enable = gr.Checkbox(label="Enable Random Flip", value=True)

                            with gr.Accordion("ğŸ¨ Photometric Transforms (RGB only)", open=False):
                                aug_noise_enable = gr.Checkbox(label="Enable Gaussian Noise", value=True)
                                aug_noise_std = gr.Slider(
                                    label="Noise Std",
                                    minimum=0, maximum=30, value=10, step=1
                                )

                                aug_brightness_enable = gr.Checkbox(label="Enable Brightness", value=True)
                                with gr.Row():
                                    aug_brightness_min = gr.Slider(
                                        label="Brightness Min",
                                        minimum=0.5, maximum=1.0, value=0.7, step=0.05
                                    )
                                    aug_brightness_max = gr.Slider(
                                        label="Brightness Max",
                                        minimum=1.0, maximum=1.5, value=1.3, step=0.05
                                    )

                                aug_contrast_enable = gr.Checkbox(label="Enable Contrast", value=False)
                                aug_color_jitter_enable = gr.Checkbox(label="Enable Color Jitter", value=False)
                                aug_blur_enable = gr.Checkbox(label="Enable Gaussian Blur", value=False)

                            # Preview settings
                            gr.Markdown("#### ğŸ‘€ Preview Settings")
                            with gr.Row():
                                aug_preview_rows = gr.Slider(
                                    label="Grid Rows",
                                    minimum=1, maximum=5, value=3, step=1
                                )
                                aug_preview_cols = gr.Slider(
                                    label="Grid Cols",
                                    minimum=1, maximum=5, value=3, step=1
                                )

                            aug_preview_btn = gr.Button("ğŸ” Generate Preview", variant="secondary", size="lg")

                            # Batch augmentation settings
                            gr.Markdown("#### ğŸš€ Batch Augmentation")
                            aug_multiplier = gr.Number(
                                label="Augmentation Multiplier",
                                value=5,
                                minimum=1,
                                maximum=20,
                                step=1,
                                info="Number of augmented versions per sample"
                            )

                            aug_output_dir = gr.Textbox(
                                label="Output Directory",
                                value=str(Path(self.default_output_dir) / "augmented"),
                                placeholder="Output path for augmented data"
                            )

                            aug_apply_btn = gr.Button("âœ¨ Apply Augmentation", variant="primary", size="lg")

                        # Right column: Preview & Results
                        with gr.Column(scale=2):
                            gr.Markdown("#### ğŸ–¼ï¸ Preview Grid")
                            aug_preview_display = gr.Image(
                                label="Augmentation Preview",
                                type="numpy",
                                height=600
                            )

                            aug_status = gr.Markdown("Load a session to start")

                            aug_progress = gr.Markdown("")

                    # Event handlers for Data Augmentation

                    # Scan sessions
                    def scan_aug_sessions(session_dir):
                        """Scan for available annotation sessions"""
                        try:
                            session_path = Path(session_dir)
                            if not session_path.exists():
                                return gr.Dropdown(choices=[]), "âŒ Session directory not found"

                            # Find all session files (both session.json and session_metadata.json)
                            sessions = []

                            # Search for interactive sessions (session.json)
                            for session_file in session_path.rglob("session.json"):
                                # Verify it's an interactive session by reading the file
                                try:
                                    with open(session_file, 'r') as f:
                                        metadata = json.load(f)
                                        if metadata.get('session_type') != 'batch':
                                            session_name = session_file.parent.name
                                            sessions.append(str(session_file.parent))
                                except:
                                    # If can't read, assume it's valid
                                    session_name = session_file.parent.name
                                    sessions.append(str(session_file.parent))

                            # Also search for batch sessions (session_metadata.json) - they can also be augmented
                            for session_file in session_path.rglob("session_metadata.json"):
                                try:
                                    with open(session_file, 'r') as f:
                                        metadata = json.load(f)
                                        if metadata.get('session_type') == 'batch':
                                            session_name = session_file.parent.name
                                            sessions.append(str(session_file.parent))
                                except:
                                    pass

                            if not sessions:
                                return gr.Dropdown(choices=[]), "âš ï¸ No sessions found"

                            return gr.Dropdown(choices=sessions), f"âœ… Found {len(sessions)} sessions"
                        except Exception as e:
                            return gr.Dropdown(choices=[]), f"âŒ Error: {str(e)}"

                    aug_scan_btn.click(
                        fn=scan_aug_sessions,
                        inputs=[aug_session_dir],
                        outputs=[aug_session_list, aug_session_info]
                    )

                    # Load session
                    def load_aug_session(session_path):
                        """Load annotation session for augmentation"""
                        try:
                            if not session_path:
                                return None, "âš ï¸ Please select a session"

                            session_path = Path(session_path)
                            session_file = session_path / "session.json"

                            if not session_file.exists():
                                return None, f"âŒ Session file not found: {session_file}"

                            # Load session metadata
                            with open(session_file, 'r') as f:
                                metadata = json.load(f)

                            # Store for augmentation
                            self.aug_session_path = session_path
                            self.aug_metadata = metadata

                            info = f"""
âœ… Session loaded successfully

**Session ID:** {metadata.get('session_id', 'N/A')}
**Frames:** {metadata.get('num_frames', 0)} frames
**Created:** {metadata.get('created_at', 'N/A')}
"""
                            return None, info
                        except Exception as e:
                            return None, f"âŒ Error loading session: {str(e)}"

                    aug_load_session_btn.click(
                        fn=load_aug_session,
                        inputs=[aug_session_list],
                        outputs=[aug_preview_display, aug_session_info]
                    )

                    # Generate preview
                    def generate_aug_preview(
                        rows, cols,
                        scale_enable, scale_min, scale_max, fill_color,
                        rotation_enable, rotation_min, rotation_max,
                        flip_enable,
                        noise_enable, noise_std,
                        brightness_enable, brightness_min, brightness_max,
                        contrast_enable, color_jitter_enable, blur_enable
                    ):
                        """Generate augmentation preview grid"""
                        try:
                            if not hasattr(self, 'aug_session_path'):
                                return None, "âŒ Please load a session first"

                            # Load first frame and mask as example
                            rgb_files = sorted((self.aug_session_path / "rgb").glob("*.png"))
                            mask_files = sorted((self.aug_session_path / "masks").glob("*.png"))

                            if not rgb_files or not mask_files:
                                return None, "âŒ No RGB or mask files found in session"

                            # Load first frame
                            rgb = cv2.imread(str(rgb_files[0]))
                            rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)

                            mask = cv2.imread(str(mask_files[0]), cv2.IMREAD_GRAYSCALE)
                            mask = mask > 127  # Convert to boolean

                            # Build base config
                            base_config = {
                                'scale': scale_enable,
                                'rotation': rotation_enable,
                                'flip': flip_enable,
                                'noise': noise_enable,
                                'brightness': brightness_enable,
                                'contrast': contrast_enable,
                                'color_jitter': color_jitter_enable,
                                'blur': blur_enable,
                                'fill_color': fill_color
                            }

                            # Generate random configs
                            num_variations = int(rows * cols)
                            configs = []

                            import random
                            for _ in range(num_variations):
                                config = {'fill_color': fill_color}

                                if scale_enable:
                                    config['scale'] = random.uniform(scale_min, scale_max)

                                if rotation_enable:
                                    config['rotation'] = random.uniform(rotation_min, rotation_max)

                                if flip_enable and random.random() > 0.5:
                                    config['flip'] = random.choice(['horizontal', 'vertical'])

                                if noise_enable:
                                    config['noise'] = random.uniform(5, noise_std)

                                if brightness_enable:
                                    config['brightness'] = random.uniform(brightness_min, brightness_max)

                                if contrast_enable:
                                    config['contrast'] = random.uniform(0.8, 1.2)

                                if color_jitter_enable:
                                    config['color_jitter'] = True

                                if blur_enable:
                                    config['blur'] = random.choice([3, 5, 7])

                                configs.append(config)

                            # Generate preview grid
                            grid = self.augmentor.generate_preview_grid(
                                rgb, mask, configs, grid_size=(int(rows), int(cols))
                            )

                            return grid, f"âœ… Preview generated with {num_variations} variations"

                        except Exception as e:
                            import traceback
                            return None, f"âŒ Error: {str(e)}\n{traceback.format_exc()}"

                    aug_preview_btn.click(
                        fn=generate_aug_preview,
                        inputs=[
                            aug_preview_rows, aug_preview_cols,
                            aug_scale_enable, aug_scale_min, aug_scale_max, aug_fill_color,
                            aug_rotation_enable, aug_rotation_min, aug_rotation_max,
                            aug_flip_enable,
                            aug_noise_enable, aug_noise_std,
                            aug_brightness_enable, aug_brightness_min, aug_brightness_max,
                            aug_contrast_enable, aug_color_jitter_enable, aug_blur_enable
                        ],
                        outputs=[aug_preview_display, aug_status]
                    )

                    # Apply batch augmentation
                    def apply_batch_augmentation(
                        multiplier, output_dir,
                        scale_enable, scale_min, scale_max, fill_color,
                        rotation_enable, rotation_min, rotation_max,
                        flip_enable,
                        noise_enable, noise_std,
                        brightness_enable, brightness_min, brightness_max,
                        contrast_enable, color_jitter_enable, blur_enable
                    ):
                        """Apply augmentation to all frames in session"""
                        try:
                            if not hasattr(self, 'aug_session_path'):
                                return "âŒ Please load a session first", ""

                            output_path = Path(output_dir)
                            output_path.mkdir(parents=True, exist_ok=True)

                            # Create subdirectories
                            rgb_out = output_path / "rgb"
                            mask_out = output_path / "masks"
                            rgb_out.mkdir(exist_ok=True)
                            mask_out.mkdir(exist_ok=True)

                            # Load all frames
                            rgb_files = sorted((self.aug_session_path / "rgb").glob("*.png"))
                            mask_files = sorted((self.aug_session_path / "masks").glob("*.png"))

                            total_frames = len(rgb_files)
                            total_outputs = total_frames * int(multiplier)

                            progress_msg = f"ğŸš€ Processing {total_frames} frames Ã— {int(multiplier)} = {total_outputs} outputs..."

                            import random
                            processed = 0

                            for idx, (rgb_file, mask_file) in enumerate(zip(rgb_files, mask_files)):
                                # Load frame
                                rgb = cv2.imread(str(rgb_file))
                                rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)

                                mask = cv2.imread(str(mask_file), cv2.IMREAD_GRAYSCALE)
                                mask = mask > 127

                                # Generate augmentations
                                for aug_idx in range(int(multiplier)):
                                    # Random config
                                    config = {'fill_color': fill_color}

                                    if scale_enable:
                                        config['scale'] = random.uniform(scale_min, scale_max)

                                    if rotation_enable:
                                        config['rotation'] = random.uniform(rotation_min, rotation_max)

                                    if flip_enable and random.random() > 0.5:
                                        config['flip'] = random.choice(['horizontal', 'vertical'])

                                    if noise_enable:
                                        config['noise'] = random.uniform(5, noise_std)

                                    if brightness_enable:
                                        config['brightness'] = random.uniform(brightness_min, brightness_max)

                                    if contrast_enable:
                                        config['contrast'] = random.uniform(0.8, 1.2)

                                    if color_jitter_enable:
                                        config['color_jitter'] = True

                                    if blur_enable:
                                        config['blur'] = random.choice([3, 5, 7])

                                    # Apply augmentation
                                    aug_rgb, aug_mask, applied = self.augmentor.augment(rgb, mask, config)

                                    # Save with consistent naming: frame{idx:04d}_aug{aug_idx:02d}.png
                                    output_name = f"frame{idx:04d}_aug{aug_idx:02d}.png"

                                    # Save RGB
                                    rgb_bgr = cv2.cvtColor(aug_rgb, cv2.COLOR_RGB2BGR)
                                    cv2.imwrite(str(rgb_out / output_name), rgb_bgr)

                                    # Save mask
                                    mask_img = (aug_mask * 255).astype(np.uint8)
                                    cv2.imwrite(str(mask_out / output_name), mask_img)

                                    processed += 1

                                # Update progress
                                if (idx + 1) % 10 == 0:
                                    progress_msg = f"â³ Processed {idx + 1}/{total_frames} frames ({processed}/{total_outputs} outputs)"

                            # Save metadata
                            metadata = {
                                'source_session': str(self.aug_session_path),
                                'original_frames': total_frames,
                                'multiplier': int(multiplier),
                                'total_augmented': processed,
                                'augmentation_config': {
                                    'scale': {'enabled': scale_enable, 'min': scale_min, 'max': scale_max} if scale_enable else None,
                                    'rotation': {'enabled': rotation_enable, 'min': rotation_min, 'max': rotation_max} if rotation_enable else None,
                                    'flip': flip_enable,
                                    'noise': {'enabled': noise_enable, 'std': noise_std} if noise_enable else None,
                                    'brightness': {'enabled': brightness_enable, 'min': brightness_min, 'max': brightness_max} if brightness_enable else None,
                                    'fill_color': fill_color
                                },
                                'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            }

                            with open(output_path / "augmentation_metadata.json", 'w') as f:
                                json.dump(metadata, f, indent=2)

                            final_msg = f"""
âœ… Augmentation complete!

**Original frames:** {total_frames}
**Multiplier:** {int(multiplier)}Ã—
**Total generated:** {processed} augmented samples

**Output location:**
`{output_path}`
"""
                            return final_msg, f"Saved to: {output_path}"

                        except Exception as e:
                            import traceback
                            return f"âŒ Error: {str(e)}\n{traceback.format_exc()}", ""

                    aug_apply_btn.click(
                        fn=apply_batch_augmentation,
                        inputs=[
                            aug_multiplier, aug_output_dir,
                            aug_scale_enable, aug_scale_min, aug_scale_max, aug_fill_color,
                            aug_rotation_enable, aug_rotation_min, aug_rotation_max,
                            aug_flip_enable,
                            aug_noise_enable, aug_noise_std,
                            aug_brightness_enable, aug_brightness_min, aug_brightness_max,
                            aug_contrast_enable, aug_color_jitter_enable, aug_blur_enable
                        ],
                        outputs=[aug_status, aug_progress]
                    )

        return demo

def main():
    """ì›¹ ì•± ì‹¤í–‰"""
    import os
    import socket

    app = SAMInteractiveWebApp()
    demo = app.create_interface()

    # í¬íŠ¸ ì„¤ì •: í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” 7860-7900 ë²”ìœ„ì—ì„œ ìë™ ì„ íƒ
    start_port = int(os.getenv("GRADIO_SERVER_PORT", "7860"))

    # ì‚¬ìš© ê°€ëŠ¥í•œ í¬íŠ¸ ì°¾ê¸°
    def find_free_port(start, end=None):
        """Find a free port in the range [start, end]"""
        if end is None:
            end = start + 40  # 7860-7900

        for port in range(start, end + 1):
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.bind(('', port))
                    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    return port
            except OSError:
                continue
        return None

    port = find_free_port(start_port)
    if port is None:
        print(f"âŒ Cannot find free port in range {start_port}-{start_port + 40}")
        print("ğŸ’¡ Kill existing processes: pkill -f web_app.py")
        return

    print(f"âœ“ Using port: {port}")

    demo.launch(
        server_name="0.0.0.0",
        server_port=port,
        share=False,
        debug=True,
        max_threads=40  # ë™ì‹œ ì²˜ë¦¬ ì¦ê°€
    )

if __name__ == "__main__":
    main()
