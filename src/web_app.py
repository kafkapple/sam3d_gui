#!/usr/bin/env python3
"""
SAM 3D GUI - Interactive SAM Annotation Web Interface
ëŒ€í™”í˜• SAM annotation: point í´ë¦­ìœ¼ë¡œ fg/bg ì§€ì • + ë¹„ë””ì˜¤ propagation
"""

import sys
import gradio as gr
import numpy as np
import cv2
import torch
from pathlib import Path
from typing import Optional, Tuple, List, Dict
import json

from sam3d_processor import SAM3DProcessor
from config_loader import ModelConfig
from lite_annotator import LiteAnnotator

# Load configuration
try:
    config = ModelConfig()
    print(f"âœ“ Config loaded from: {Path(__file__).parent.parent / 'config' / 'model_config.yaml'}")
except Exception as e:
    print(f"Warning: Failed to load config: {e}")
    config = None

# SAM 2 imports
SAM2_PATH = Path.home() / 'dev/segment-anything-2'
if SAM2_PATH.exists():
    sys.path.insert(0, str(SAM2_PATH))
    from sam2.sam2_image_predictor import SAM2ImagePredictor
    from sam2.sam2_video_predictor import SAM2VideoPredictor
    SAM2_AVAILABLE = True
else:
    SAM2ImagePredictor = None
    SAM2VideoPredictor = None
    SAM2_AVAILABLE = False
    print("Warning: SAM 2 not found. Interactive segmentation will use fallback method.")

class SAMInteractiveWebApp:
    """
    SAM 3D GUI - í†µí•© ì›¹ ì¸í„°í˜ì´ìŠ¤

    ëª¨ë“œ 1: ìë™ ì²˜ë¦¬ (Quick Mode)
    - ë¹„ë””ì˜¤ ì„ íƒ â†’ ìë™ ì„¸ê·¸ë©˜í…Œì´ì…˜ â†’ ëª¨ì…˜ ê°ì§€ â†’ ê²°ê³¼

    ëª¨ë“œ 2: ëŒ€í™”í˜• Annotation (Interactive Mode)
    - Point annotation (foreground/background)
    - ìˆ˜ë™ ì„¸ê·¸ë©˜í…Œì´ì…˜ â†’ Propagation â†’ 3D mesh
    """

    def __init__(self):
        # Config-based initialization
        self.config = config

        # SAM 3D processor ì´ˆê¸°í™”
        if config:
            sam3d_checkpoint = config.sam3d_checkpoint_dir
            self.processor = SAM3DProcessor(sam3d_checkpoint_path=sam3d_checkpoint)
        else:
            self.processor = SAM3DProcessor()

        # SAM2 predictor ì´ˆê¸°í™” (Interactive Modeìš©)
        self.sam2_predictor = None
        self.sam2_video_predictor = None
        self.sam2_device = None
        if SAM2_AVAILABLE and config:
            try:
                print("Loading SAM 2 for interactive segmentation...")
                checkpoint = Path(config.sam2_checkpoint)
                model_cfg = config.sam2_config
                device = config.sam2_device

                # Auto-detect device
                if device == "auto":
                    if torch.cuda.is_available():
                        device = "cuda"
                        gpu_name = torch.cuda.get_device_name(0)
                        print(f"âœ“ CUDA detected: {gpu_name}")
                    else:
                        device = "cpu"
                        print("Warning: CUDA not available, using CPU")
                elif device == "cuda" and not torch.cuda.is_available():
                    device = "cpu"
                    print("Warning: CUDA not available, using CPU")

                self.sam2_device = device

                if checkpoint.exists():
                    from sam2.build_sam import build_sam2, build_sam2_video_predictor

                    # Image predictor for single-frame segmentation
                    sam2_model = build_sam2(model_cfg, str(checkpoint), device=device)
                    self.sam2_predictor = SAM2ImagePredictor(sam2_model)

                    # Video predictor for memory-based tracking
                    self.sam2_video_predictor = build_sam2_video_predictor(model_cfg, str(checkpoint), device=device)

                    print(f"âœ“ SAM 2 loaded: {config.cfg.sam2.name} on {device}")
                    print(f"âœ“ SAM 2 Video Predictor initialized for propagation")
                else:
                    print(f"Warning: SAM 2 checkpoint not found at {checkpoint}")
            except Exception as e:
                print(f"Warning: Failed to load SAM 2: {e}")
                import traceback
                traceback.print_exc()
                self.sam2_predictor = None
                self.sam2_video_predictor = None

        # ìƒíƒœ ê´€ë¦¬
        self.video_path = None
        self.frames = []
        self.current_frame_idx = 0
        self.annotations = {
            'foreground': [],  # [(x, y), ...]
            'background': []   # [(x, y), ...]
        }
        self.masks = []  # ê° í”„ë ˆì„ì˜ ë§ˆìŠ¤í¬
        self.current_mask = None
        self.tracking_result = None

        # Default paths from config
        if config:
            self.default_data_dir = config.default_data_dir
        else:
            self.default_data_dir = "/home/joon/dev/data/markerless_mouse/"

        # LiteAnnotator ì´ˆê¸°í™” (Tab 3: Lite Mode)
        self.lite_annotator = None
        if SAM2_AVAILABLE:
            try:
                print("Initializing Lite Annotator...")
                self.lite_annotator = LiteAnnotator(
                    sam2_base_path=SAM2_PATH,
                    device=self.sam2_device if self.sam2_device else "auto"
                )
                print("âœ“ Lite Annotator initialized")
            except Exception as e:
                print(f"Warning: Failed to initialize Lite Annotator: {e}")
                self.lite_annotator = None

    def quick_process(self, data_dir: str, video_file: str,
                     start_time: float, duration: float,
                     motion_threshold: float, segmentation_method: str,
                     progress=gr.Progress()) -> Tuple[np.ndarray, str]:
        """
        Quick Mode: ìë™ ì²˜ë¦¬ (ê¸°ì¡´ web_app.py ê¸°ëŠ¥ í†µí•©)
        """
        if not video_file:
            return None, "ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”"

        video_path = Path(data_dir) / video_file
        if not video_path.exists():
            return None, f"ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}"

        self.video_path = str(video_path)

        progress(0, desc="ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘...")

        try:
            # ë¹„ë””ì˜¤ ì²˜ë¦¬
            result, reconstruction = self.processor.process_video_segment(
                video_path=self.video_path,
                start_time=start_time,
                duration=duration,
                output_dir="outputs/",
                motion_threshold=motion_threshold,
                segmentation_method=segmentation_method
            )

            self.tracking_result = result

            progress(0.5, desc="ê²°ê³¼ ìƒì„± ì¤‘...")

            # ê²°ê³¼ ì‹œê°í™”
            if result.segments:
                first_frame = result.segments[0]

                # í”„ë ˆì„ ë‹¤ì‹œ ì½ê¸°
                cap = cv2.VideoCapture(self.video_path)
                cap.set(cv2.CAP_PROP_POS_FRAMES, first_frame.frame_idx)
                ret, frame = cap.read()
                cap.release()

                if ret:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    mask_colored = np.zeros_like(frame_rgb)
                    mask_colored[first_frame.mask > 0] = [0, 255, 0]
                    overlay = cv2.addWeighted(frame_rgb, 0.7, mask_colored, 0.3, 0)

                    if first_frame.bbox:
                        x1, y1, x2, y2 = first_frame.bbox
                        cv2.rectangle(overlay, (x1, y1), (x2, y2), (255, 0, 0), 2)

                    visualization = overlay
                else:
                    visualization = None
            else:
                visualization = None

            progress(0.8, desc="í†µê³„ ê³„ì‚° ì¤‘...")

            # ê²°ê³¼ í…ìŠ¤íŠ¸
            result_text = f"""
### ğŸ¯ Quick Process ì™„ë£Œ

**ê¸°ë³¸ ì •ë³´**
- ë¶„ì„ëœ í”„ë ˆì„: {len(result.segments)}
- ì²˜ë¦¬ ì‹œê°„: {start_time}s - {start_time + duration}s

**ëª¨ì…˜ ê°ì§€**
- ê°ì§€ ì—¬ë¶€: {'âœ… ì˜ˆ' if result.motion_detected else 'âŒ ì•„ë‹ˆì˜¤'}
- ì„ê³„ê°’: {motion_threshold} í”½ì…€
"""

            if result.motion_detected and len(result.segments) > 1:
                displacements = []
                for i in range(1, len(result.segments)):
                    prev = result.segments[i-1].center
                    curr = result.segments[i].center
                    dx = curr[0] - prev[0]
                    dy = curr[1] - prev[1]
                    disp = (dx**2 + dy**2)**0.5
                    displacements.append(disp)

                if displacements:
                    result_text += f"""
**ë³€ìœ„ í†µê³„**
- ìµœëŒ€ ë³€ìœ„: {max(displacements):.1f} í”½ì…€
- í‰ê·  ë³€ìœ„: {sum(displacements)/len(displacements):.1f} í”½ì…€
"""

            if result.segments:
                first = result.segments[0]
                result_text += f"""
**ê°ì²´ ì •ë³´**
- ë°”ìš´ë”© ë°•ìŠ¤: {first.bbox}
- ì¤‘ì‹¬ì : {first.center}
- ë©´ì : {first.area:.0f} í”½ì…€Â²

**ì¶œë ¥**
- ì €ì¥ ìœ„ì¹˜: `outputs/`
"""

            progress(1.0, desc="ì™„ë£Œ!")

            return visualization, result_text

        except Exception as e:
            import traceback
            error_msg = f"ì˜¤ë¥˜:\n```\n{str(e)}\n{traceback.format_exc()}\n```"
            return None, error_msg

    def scan_videos(self, data_dir: str) -> List[str]:
        """ë””ë ‰í† ë¦¬ì—ì„œ ë¹„ë””ì˜¤ íŒŒì¼ ìŠ¤ìº”"""
        data_path = Path(data_dir)
        if not data_path.exists():
            return []

        video_extensions = ['.mp4', '.avi', '.mov', '.mkv']
        videos = []

        for ext in video_extensions:
            videos.extend([str(p.relative_to(data_path))
                          for p in data_path.rglob(f'*{ext}')])

        return sorted(videos)

    def load_video(self, data_dir: str, video_file: str,
                   start_time: float, duration: float) -> Tuple[np.ndarray, str, gr.Slider]:
        """ë¹„ë””ì˜¤ ë¡œë“œ ë° í”„ë ˆì„ ì¶”ì¶œ"""
        default_slider = gr.Slider(label="í”„ë ˆì„ ìœ„ì¹˜", minimum=0, maximum=100, value=0, step=1)

        if not video_file:
            return None, "ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”", default_slider

        video_path = Path(data_dir) / video_file

        if not video_path.exists():
            return None, f"ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}", default_slider

        self.video_path = str(video_path)

        try:
            # ë¹„ë””ì˜¤ ì •ë³´
            info = self.processor.get_video_info(self.video_path)
            fps = info['fps']

            # í”„ë ˆì„ ì¶”ì¶œ
            start_frame = int(start_time * fps)
            num_frames = int(duration * fps)

            self.frames = self.processor.extract_frames(
                self.video_path,
                start_frame,
                num_frames,
                stride=1
            )

            if not self.frames:
                return None, "âŒ í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤", default_slider

            # ì´ˆê¸°í™”
            self.current_frame_idx = 0
            self.annotations = {'foreground': [], 'background': []}
            self.masks = [None] * len(self.frames)
            self.current_mask = None

            info_text = f"""
### ë¹„ë””ì˜¤ ë¡œë“œ ì™„ë£Œ âœ…

- **í”„ë ˆì„ ìˆ˜**: {len(self.frames)}
- **í•´ìƒë„**: {info['width']} x {info['height']}
- **FPS**: {info['fps']:.2f}
- **êµ¬ê°„**: {start_time}s - {start_time + duration}s

### ë‹¤ìŒ ë‹¨ê³„:
1. **Foreground Point** í´ë¦­í•˜ì—¬ ê°ì²´ ìœ„ì¹˜ ì§€ì •
2. **Background Point** í´ë¦­í•˜ì—¬ ë°°ê²½ ìœ„ì¹˜ ì§€ì • (ì„ íƒì‚¬í•­)
3. **Segment Current Frame** í´ë¦­í•˜ì—¬ í˜„ì¬ í”„ë ˆì„ ì„¸ê·¸ë©˜í…Œì´ì…˜
4. **Propagate to All Frames** í´ë¦­í•˜ì—¬ ì „ì²´ ë¹„ë””ì˜¤ ì¶”ì 
5. **Generate 3D Mesh** í´ë¦­í•˜ì—¬ 3D ìƒì„±
            """

            # ì²« í”„ë ˆì„ ë°˜í™˜ + ìŠ¬ë¼ì´ë” ì—…ë°ì´íŠ¸
            frame_rgb = cv2.cvtColor(self.frames[0], cv2.COLOR_BGR2RGB)

            # ìŠ¬ë¼ì´ë” ë²”ìœ„ ì—…ë°ì´íŠ¸
            slider_update = gr.Slider(
                label="í”„ë ˆì„ ìœ„ì¹˜",
                minimum=0,
                maximum=len(self.frames) - 1,
                value=0,
                step=1,
                interactive=True,
                info=f"ìŠ¬ë¼ì´ë”ë¥¼ ë“œë˜ê·¸í•˜ì—¬ í”„ë ˆì„ ì´ë™ (ì´ {len(self.frames)}ê°œ)"
            )

            return frame_rgb, info_text, slider_update

        except Exception as e:
            import traceback
            error_msg = f"""
### âŒ ì˜¤ë¥˜ ë°œìƒ

**ì—ëŸ¬ ë©”ì‹œì§€:**
```
{str(e)}
```

**ìƒì„¸ ì •ë³´:**
```
{traceback.format_exc()}
```

**í™•ì¸ì‚¬í•­:**
- ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œê°€ ì •í™•í•œê°€ìš”?
- ë¹„ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ë‚˜ìš”?
- íŒŒì¼ í˜•ì‹ì´ ì§€ì›ë˜ë‚˜ìš”? (MP4, AVI, MOV, MKV)
"""
            print(f"[ERROR] {error_msg}")
            return None, error_msg, default_slider

    def add_point(self, image: np.ndarray, point_type: str, evt: gr.SelectData) -> Tuple[np.ndarray, str]:
        """
        ì´ë¯¸ì§€ í´ë¦­ ì‹œ point ì¶”ê°€

        Args:
            image: í˜„ì¬ ì´ë¯¸ì§€
            point_type: 'foreground' or 'background'
            evt: Gradio í´ë¦­ ì´ë²¤íŠ¸
        """
        if image is None or len(self.frames) == 0:
            return image, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

        # í´ë¦­ ì¢Œí‘œ
        x, y = evt.index[0], evt.index[1]

        # Point ì¶”ê°€
        self.annotations[point_type].append((x, y))

        # í˜„ì¬ í”„ë ˆì„ì— point í‘œì‹œ
        frame = self.frames[self.current_frame_idx].copy()
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Foreground points (ë…¹ìƒ‰)
        for px, py in self.annotations['foreground']:
            cv2.circle(frame_rgb, (px, py), 5, (0, 255, 0), -1)
            cv2.circle(frame_rgb, (px, py), 7, (255, 255, 255), 2)

        # Background points (ë¹¨ê°„ìƒ‰)
        for px, py in self.annotations['background']:
            cv2.circle(frame_rgb, (px, py), 5, (255, 0, 0), -1)
            cv2.circle(frame_rgb, (px, py), 7, (255, 255, 255), 2)

        status = f"""
**Annotations:**
- Foreground: {len(self.annotations['foreground'])} points
- Background: {len(self.annotations['background'])} points

í´ë¦­í•œ ìœ„ì¹˜: ({x}, {y}) - {point_type}
"""

        return frame_rgb, status

    def segment_current_frame(self) -> Tuple[np.ndarray, str]:
        """
        í˜„ì¬ í”„ë ˆì„ì„ SAMìœ¼ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜
        (ê°„ë‹¨í•œ contour ê¸°ë°˜, ì‹¤ì œ SAM ëª¨ë¸ í†µí•©ì€ ë³„ë„ í•„ìš”)
        """
        if len(self.frames) == 0:
            return None, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

        if len(self.annotations['foreground']) == 0:
            return None, "ìµœì†Œ 1ê°œì˜ foreground pointê°€ í•„ìš”í•©ë‹ˆë‹¤"

        try:
            frame = self.frames[self.current_frame_idx]
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # SAM2 ì‚¬ìš© (availableí•˜ë©´)
            if self.sam2_predictor is not None:
                # SAM2 inference
                self.sam2_predictor.set_image(frame_rgb)

                # Pointsì™€ labels ì¤€ë¹„
                point_coords = []
                point_labels = []

                for px, py in self.annotations['foreground']:
                    point_coords.append([px, py])
                    point_labels.append(1)  # foreground

                for px, py in self.annotations['background']:
                    point_coords.append([px, py])
                    point_labels.append(0)  # background

                point_coords = np.array(point_coords, dtype=np.float32)
                point_labels = np.array(point_labels, dtype=np.int32)

                # SAM2 predict
                masks, scores, _ = self.sam2_predictor.predict(
                    point_coords=point_coords,
                    point_labels=point_labels,
                    multimask_output=True
                )

                # Best mask ì„ íƒ
                best_idx = np.argmax(scores)
                mask = masks[best_idx]
                confidence = scores[best_idx]

                status_method = f"SAM2 (confidence: {confidence:.3f})"
            else:
                # Fallback: contour ê¸°ë°˜
                mask = self.processor.segment_object_interactive(frame, method='contour')
                confidence = 0.0
                status_method = "Contour (fallback)"

            # ë§ˆìŠ¤í¬ ì €ì¥
            self.masks[self.current_frame_idx] = mask
            self.current_mask = mask

            # ì‹œê°í™”
            overlay = frame_rgb.copy()
            overlay[mask > 0] = [0, 255, 0]  # ë…¹ìƒ‰ ë§ˆìŠ¤í¬
            result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)

            # Points í‘œì‹œ
            for px, py in self.annotations['foreground']:
                cv2.circle(result, (px, py), 5, (0, 255, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)
            for px, py in self.annotations['background']:
                cv2.circle(result, (px, py), 5, (255, 0, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)

            mask_area = np.sum(mask > 0)
            mask_pct = mask_area / mask.size * 100

            status = f"""
### Segmentation ì™„ë£Œ âœ…

- **Method**: {status_method}
- **í”„ë ˆì„**: {self.current_frame_idx + 1} / {len(self.frames)}
- **ë§ˆìŠ¤í¬ ì˜ì—­**: {mask_area} í”½ì…€ ({mask_pct:.1f}%)
- **Foreground points**: {len(self.annotations['foreground'])}
- **Background points**: {len(self.annotations['background'])}

### ë‹¤ìŒ:
- ë‹¤ë¥¸ í”„ë ˆì„ì—ë„ annotationí•˜ë ¤ë©´ í”„ë ˆì„ ì´ë™
- ë˜ëŠ” **Propagate to All Frames** í´ë¦­
"""

            return result, status

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return None, f"ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹¤íŒ¨: {str(e)}\n\n```\n{error_detail}\n```"

    def propagate_to_all_frames(self, progress=gr.Progress()) -> Tuple[np.ndarray, str]:
        """
        í˜„ì¬ í”„ë ˆì„ì˜ annotationì„ ì „ì²´ ë¹„ë””ì˜¤ì— propagation (tracking)
        SAM 2 Video Predictorë¥¼ ì‚¬ìš©í•œ ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì 

        ì¤‘ìš”: ê³ ì • pointsë¥¼ ëª¨ë“  í”„ë ˆì„ì— ì¬ì ìš©í•˜ì§€ ì•ŠìŒ!
        ëŒ€ì‹  SAM 2ì˜ memory mechanismì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ê°ì²´ ì¶”ì 
        """
        if len(self.frames) == 0:
            return None, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

        if len(self.annotations['foreground']) == 0:
            return None, "Annotation pointsê°€ í•„ìš”í•©ë‹ˆë‹¤ (ìµœì†Œ 1ê°œì˜ foreground point)"

        try:
            progress(0, desc="ë¹„ë””ì˜¤ tracking ì´ˆê¸°í™” (SAM 2 Video Predictor)...")

            # SAM 2 Video Predictor ì‚¬ìš© (ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì )
            if self.sam2_video_predictor is not None:
                # 1. ì„ì‹œ ë””ë ‰í† ë¦¬ì— í”„ë ˆì„ ì €ì¥ (SAM 2 Video PredictorëŠ” ë””ë ‰í† ë¦¬ ì…ë ¥ í•„ìš”)
                import tempfile
                import os
                temp_dir = tempfile.mkdtemp(prefix="sam3d_video_")

                try:
                    progress(0.05, desc="í”„ë ˆì„ ì €ì¥ ì¤‘...")
                    for i, frame in enumerate(self.frames):
                        frame_path = os.path.join(temp_dir, f"{i:05d}.jpg")
                        cv2.imwrite(frame_path, frame)

                    progress(0.1, desc="SAM 2 Video Predictor ì´ˆê¸°í™” ì¤‘...")

                    # 2. Inference state ì´ˆê¸°í™”
                    inference_state = self.sam2_video_predictor.init_state(video_path=temp_dir)

                    # 3. í˜„ì¬ í”„ë ˆì„ì—ë§Œ annotation points ì¶”ê°€ (conditioning frame)
                    point_coords = []
                    point_labels = []

                    for px, py in self.annotations['foreground']:
                        point_coords.append([px, py])
                        point_labels.append(1)

                    for px, py in self.annotations['background']:
                        point_coords.append([px, py])
                        point_labels.append(0)

                    point_coords = np.array(point_coords, dtype=np.float32)
                    point_labels = np.array(point_labels, dtype=np.int32)

                    progress(0.15, desc=f"ì´ˆê¸° í”„ë ˆì„ ({self.current_frame_idx}) annotation ì¤‘...")

                    # í˜„ì¬ í”„ë ˆì„ì„ conditioning frameìœ¼ë¡œ ì„¤ì •
                    _, out_obj_ids, out_mask_logits = self.sam2_video_predictor.add_new_points_or_box(
                        inference_state=inference_state,
                        frame_idx=self.current_frame_idx,
                        obj_id=1,  # Single object tracking
                        points=point_coords,
                        labels=point_labels,
                    )

                    progress(0.2, desc="ë©”ëª¨ë¦¬ ê¸°ë°˜ ì „íŒŒ ì‹œì‘...")

                    # 4. Propagate using memory-based tracking (NO points on other frames!)
                    video_segments = {}
                    for frame_idx, obj_ids, mask_logits in self.sam2_video_predictor.propagate_in_video(
                        inference_state,
                        start_frame_idx=self.current_frame_idx
                    ):
                        # Memory-based tracking - ê° í”„ë ˆì„ì€ ì´ì „ í”„ë ˆì„ì˜ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©
                        # PointsëŠ” ì¬ì ìš©ë˜ì§€ ì•ŠìŒ!
                        video_segments[frame_idx] = (mask_logits[0] > 0.0).cpu().numpy()

                        progress_pct = 0.2 + 0.6 * (frame_idx + 1) / len(self.frames)
                        progress(progress_pct, desc=f"Tracking... {frame_idx+1}/{len(self.frames)}")

                    # 5. ê²°ê³¼ë¥¼ self.masksì— ì €ì¥
                    self.masks = [None] * len(self.frames)
                    for frame_idx, mask in video_segments.items():
                        if frame_idx < len(self.masks):
                            self.masks[frame_idx] = mask.squeeze()

                    progress(0.9, desc="Tracking ì™„ë£Œ, ê²°ê³¼ ì²˜ë¦¬ ì¤‘...")

                finally:
                    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                    import shutil
                    shutil.rmtree(temp_dir, ignore_errors=True)

            else:
                # Fallback: Image predictor ì‚¬ìš© (êµ¬ë²„ì „ ë°©ì‹ - ì •í™•ë„ ë‚®ìŒ)
                progress(0, desc="Fallback: í”„ë ˆì„ë³„ ì„¸ê·¸ë©˜í…Œì´ì…˜...")

                for i, frame in enumerate(self.frames):
                    if self.masks[i] is None:
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                        if self.sam2_predictor is not None:
                            self.sam2_predictor.set_image(frame_rgb)

                            point_coords = []
                            point_labels = []

                            for px, py in self.annotations['foreground']:
                                point_coords.append([px, py])
                                point_labels.append(1)

                            for px, py in self.annotations['background']:
                                point_coords.append([px, py])
                                point_labels.append(0)

                            point_coords = np.array(point_coords, dtype=np.float32)
                            point_labels = np.array(point_labels, dtype=np.int32)

                            masks, scores, _ = self.sam2_predictor.predict(
                                point_coords=point_coords,
                                point_labels=point_labels,
                                multimask_output=True
                            )

                            best_idx = np.argmax(scores)
                            mask = masks[best_idx]
                        else:
                            mask = self.processor.segment_object_interactive(frame, method='contour')

                        self.masks[i] = mask

                    progress((i + 1) / len(self.frames), desc=f"Processing... {i+1}/{len(self.frames)}")

            progress(1.0, desc="ì‹œê°í™” ì¤€ë¹„ ì¤‘...")

            # í˜„ì¬ í”„ë ˆì„ ì‹œê°í™”
            self.current_frame_idx = min(self.current_frame_idx, len(self.frames) - 1)
            current_frame = self.frames[self.current_frame_idx]
            current_mask = self.masks[self.current_frame_idx]

            frame_rgb = cv2.cvtColor(current_frame, cv2.COLOR_BGR2RGB)
            overlay = frame_rgb.copy()
            if current_mask is not None:
                overlay[current_mask > 0] = [0, 255, 0]
            result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)

            # í†µê³„
            tracked_frames = sum(1 for m in self.masks if m is not None)

            method_used = "SAM 2 Video Predictor (Memory-based)" if self.sam2_video_predictor else "SAM 2 Image (Fallback)"

            status = f"""
### Propagation ì™„ë£Œ âœ…

- **Method**: {method_used}
- **Tracked í”„ë ˆì„**: {tracked_frames} / {len(self.frames)}
- **í˜„ì¬ í”„ë ˆì„**: {self.current_frame_idx + 1} / {len(self.frames)}
- **Conditioning Frame**: {self.current_frame_idx} (Pointsë§Œ ì—¬ê¸° ì ìš©)

### ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì :
- í˜„ì¬ í”„ë ˆì„ì˜ pointsë§Œ ì‚¬ìš©
- ë‹¤ë¥¸ í”„ë ˆì„ì€ ë©”ëª¨ë¦¬ë¡œ ìë™ ì¶”ì 
- ê°ì²´ ì´ë™ì—ë„ ì •í™•í•œ ë§ˆìŠ¤í¬ ìƒì„±

### ë‹¤ìŒ:
- **í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜**ìœ¼ë¡œ ê²°ê³¼ í™•ì¸
- **Generate 3D Mesh** í´ë¦­í•˜ì—¬ 3D ìƒì„±
- ë˜ëŠ” **Save Masks** í´ë¦­í•˜ì—¬ ë§ˆìŠ¤í¬ ì €ì¥
"""

            return result, status

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return None, f"Propagation ì‹¤íŒ¨: {str(e)}\n\n```\n{error_detail}\n```"

    def download_sam3d_checkpoint(self, progress=gr.Progress()) -> bool:
        """
        SAM 3D ì²´í¬í¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
        """
        import subprocess
        import os
        from dotenv import load_dotenv

        progress(0, desc="SAM 3D ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘...")

        # .env íŒŒì¼ ë¡œë“œ
        env_path = Path(__file__).parent.parent / ".env"
        if env_path.exists():
            load_dotenv(env_path)
            print(f"âœ“ .env íŒŒì¼ ë¡œë“œë¨: {env_path}")
        else:
            print(f"âš ï¸ .env íŒŒì¼ ì—†ìŒ: {env_path}")

        # HuggingFace í† í° í™•ì¸
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            print("âš ï¸ HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ê°€ëŠ¥.")

        # Configì—ì„œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        if self.config:
            checkpoint_dir = Path(self.config.sam3d_checkpoint_dir).expanduser()
        else:
            checkpoint_dir = Path("~/dev/sam-3d-objects/checkpoints/hf").expanduser()

        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        progress(0.1, desc="Git LFS í™•ì¸ ì¤‘...")

        # Git LFS í™•ì¸ ë° ì„¤ì¹˜
        try:
            subprocess.run(["git", "lfs", "version"], check=True, capture_output=True)
        except:
            progress(0.2, desc="Git LFS ì„¤ì¹˜ ì¤‘...")
            try:
                subprocess.run(["sudo", "apt-get", "update"], check=True)
                subprocess.run(["sudo", "apt-get", "install", "-y", "git-lfs"], check=True)
                subprocess.run(["git", "lfs", "install"], check=True)
            except Exception as e:
                print(f"Git LFS ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
                return False

        progress(0.3, desc="SAM 3D ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (5-10GB, ì‹œê°„ ì†Œìš”)")

        # HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ (í† í° ì¸ì¦ ì‚¬ìš©)
        try:
            # í† í°ì´ ìˆìœ¼ë©´ ì¸ì¦ URL ì‚¬ìš©
            if hf_token:
                clone_url = f"https://oauth2:{hf_token}@huggingface.co/facebook/sam-3d-objects"
                pull_url = f"https://oauth2:{hf_token}@huggingface.co/facebook/sam-3d-objects"
            else:
                clone_url = "https://huggingface.co/facebook/sam-3d-objects"
                pull_url = "origin"

            if not (checkpoint_dir / "pipeline.yaml").exists():
                # ì²˜ìŒ ë‹¤ìš´ë¡œë“œ
                subprocess.run([
                    "git", "clone",
                    clone_url,
                    str(checkpoint_dir)
                ], check=True, cwd=checkpoint_dir.parent)
                progress(0.9, desc="ë‹¤ìš´ë¡œë“œ ì™„ë£Œ, ê²€ì¦ ì¤‘...")
            else:
                # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì—…ë°ì´íŠ¸
                if hf_token:
                    subprocess.run(["git", "pull", pull_url], check=True, cwd=checkpoint_dir)
                else:
                    subprocess.run(["git", "pull"], check=True, cwd=checkpoint_dir)
                progress(0.9, desc="ì—…ë°ì´íŠ¸ ì™„ë£Œ, ê²€ì¦ ì¤‘...")

            # ë‹¤ìš´ë¡œë“œ í™•ì¸
            if (checkpoint_dir / "pipeline.yaml").exists():
                progress(1.0, desc="SAM 3D ì²´í¬í¬ì¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ!")
                return True
            else:
                return False

        except Exception as e:
            print(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def generate_3d_mesh(self, progress=gr.Progress()) -> Tuple[str, str]:
        """
        ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ë¡œ 3D mesh ìƒì„±
        """
        if len(self.frames) == 0 or all(m is None for m in self.masks):
            return None, "ë¨¼ì € ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ì™„ë£Œí•˜ì„¸ìš”"

        try:
            progress(0, desc="3D mesh ìƒì„± ì¤€ë¹„ ì¤‘...")

            # SAM 3D ì²´í¬í¬ì¸íŠ¸ í™•ì¸
            if self.config:
                checkpoint_dir = Path(self.config.sam3d_checkpoint_dir).expanduser()
            else:
                checkpoint_dir = Path("~/dev/sam-3d-objects/checkpoints/hf").expanduser()

            if not (checkpoint_dir / "pipeline.yaml").exists():
                progress(0.1, desc="SAM 3D ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ, ë‹¤ìš´ë¡œë“œ ì‹œì‘...")

                download_success = self.download_sam3d_checkpoint(progress)

                if not download_success:
                    return None, """
### âŒ SAM 3D ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

**ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°©ë²•:**
```bash
cd /home/joon/dev/sam3d_gui
./download_sam3d.sh
```

ë˜ëŠ” ë‹¤ìŒ ëª…ë ¹ì–´:
```bash
cd ~/dev/sam-3d-objects
git clone https://huggingface.co/facebook/sam-3d-objects checkpoints/hf
```
"""

            # ëŒ€í‘œ í”„ë ˆì„ ì„ íƒ (ì¤‘ê°„ í”„ë ˆì„)
            mid_idx = len(self.frames) // 2
            frame = self.frames[mid_idx]
            mask = self.masks[mid_idx]

            if mask is None:
                return None, "ì¤‘ê°„ í”„ë ˆì„ì— ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤"

            # 3D ì¬êµ¬ì„± ì‹œë„
            progress(0.5, desc="SAM 3D ì¬êµ¬ì„± ì¤‘...")

            try:
                reconstruction = self.processor.reconstruct_3d(frame, mask)

                if reconstruction:
                    # PLY ì €ì¥
                    output_path = "outputs/interactive_reconstruction.ply"
                    self.processor.export_mesh(reconstruction, output_path, format='ply')

                    progress(1.0, desc="ì™„ë£Œ!")

                    status = f"""
### 3D Mesh ìƒì„± ì™„ë£Œ âœ…

- **í”„ë ˆì„**: {mid_idx + 1} / {len(self.frames)}
- **ì €ì¥ ìœ„ì¹˜**: `{output_path}`

### 3D ë·°ì–´ë¡œ í™•ì¸:
```bash
meshlab {output_path}
```

ë˜ëŠ” ì˜¨ë¼ì¸: https://3dviewer.net/
"""
                    return output_path, status
                else:
                    return None, "3D ì¬êµ¬ì„± ì‹¤íŒ¨ (SAM 3D ì²´í¬í¬ì¸íŠ¸ í•„ìš”)"

            except Exception as e:
                # SAM 3D ì—†ìœ¼ë©´ ê°„ë‹¨í•œ point cloudë§Œ ìƒì„±
                return None, f"3D ì¬êµ¬ì„± ì‹¤íŒ¨: {str(e)}\n\nSAM 3D ì²´í¬í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤."

        except Exception as e:
            import traceback
            return None, f"ì˜¤ë¥˜:\n{str(e)}\n{traceback.format_exc()}"

    def save_annotation_session(self) -> str:
        """
        Annotation ì„¸ì…˜ ì „ì²´ ì €ì¥ (annotation points + masks + metadata)
        """
        if len(self.frames) == 0:
            return "ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"

        try:
            # ì„¸ì…˜ ID ìƒì„± (timestamp)
            from datetime import datetime
            session_id = datetime.now().strftime("%Y%m%d_%H%M%S")

            output_dir = Path(f"outputs/sessions/{session_id}")
            output_dir.mkdir(parents=True, exist_ok=True)

            # 1. Annotation ë©”íƒ€ë°ì´í„° ì €ì¥ (JSON)
            metadata = {
                "session_id": session_id,
                "video_path": self.video_path,
                "num_frames": len(self.frames),
                "current_frame_idx": self.current_frame_idx,
                "annotations": {
                    "foreground": self.annotations['foreground'],
                    "background": self.annotations['background']
                },
                "frame_info": []
            }

            # 2. ê° í”„ë ˆì„ ì €ì¥
            saved_masks = 0
            for i, (frame, mask) in enumerate(zip(self.frames, self.masks)):
                frame_dir = output_dir / f"frame_{i:04d}"
                frame_dir.mkdir(exist_ok=True)

                # ì›ë³¸ í”„ë ˆì„ ì €ì¥
                frame_path = frame_dir / "original.png"
                cv2.imwrite(str(frame_path), frame)

                # ë§ˆìŠ¤í¬ ì €ì¥ (ìˆìœ¼ë©´)
                if mask is not None:
                    mask_path = frame_dir / "mask.png"
                    cv2.imwrite(str(mask_path), mask.astype(np.uint8) * 255)

                    # ì‹œê°í™” (ë§ˆìŠ¤í¬ ì˜¤ë²„ë ˆì´) ì €ì¥
                    vis_path = frame_dir / "visualization.png"
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    overlay = frame_rgb.copy()
                    overlay[mask > 0] = [0, 255, 0]
                    result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)

                    # Annotation points í‘œì‹œ
                    for px, py in self.annotations['foreground']:
                        cv2.circle(result, (px, py), 5, (0, 255, 0), -1)
                        cv2.circle(result, (px, py), 7, (255, 255, 255), 2)
                    for px, py in self.annotations['background']:
                        cv2.circle(result, (px, py), 5, (255, 0, 0), -1)
                        cv2.circle(result, (px, py), 7, (255, 255, 255), 2)

                    result_bgr = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)
                    cv2.imwrite(str(vis_path), result_bgr)

                    saved_masks += 1

                    # í”„ë ˆì„ ë©”íƒ€ë°ì´í„°
                    mask_area = np.sum(mask > 0)
                    metadata["frame_info"].append({
                        "frame_idx": i,
                        "has_mask": True,
                        "mask_area": int(mask_area),
                        "mask_percentage": float(mask_area / mask.size * 100)
                    })
                else:
                    metadata["frame_info"].append({
                        "frame_idx": i,
                        "has_mask": False
                    })

            # 3. Metadata JSON ì €ì¥
            metadata_path = output_dir / "session_metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            return f"""
### Annotation ì„¸ì…˜ ì €ì¥ ì™„ë£Œ âœ…

**ì„¸ì…˜ ID**: `{session_id}`

**ì €ì¥ ë‚´ìš©**:
- ğŸ“ ì›ë³¸ í”„ë ˆì„: {len(self.frames)}ê°œ
- ğŸ­ ë§ˆìŠ¤í¬: {saved_masks}ê°œ
- ğŸ“ Annotation points: {len(self.annotations['foreground'])} foreground, {len(self.annotations['background'])} background
- ğŸ“‹ ë©”íƒ€ë°ì´í„°: session_metadata.json

**ì €ì¥ ìœ„ì¹˜**: `{output_dir}/`

**ë””ë ‰í† ë¦¬ êµ¬ì¡°**:
```
{session_id}/
â”œâ”€â”€ session_metadata.json
â”œâ”€â”€ frame_0000/
â”‚   â”œâ”€â”€ original.png
â”‚   â”œâ”€â”€ mask.png
â”‚   â””â”€â”€ visualization.png
â”œâ”€â”€ frame_0001/
â”‚   â””â”€â”€ ...
```

**ì„¸ì…˜ ì¬ë¡œë“œ**: ì´ session_idë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚˜ì¤‘ì— ì¬ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

        except Exception as e:
            import traceback
            return f"ì €ì¥ ì˜¤ë¥˜: {str(e)}\n\n```\n{traceback.format_exc()}\n```"

    def load_annotation_session(self, session_id: str) -> Tuple[np.ndarray, str]:
        """
        ì €ì¥ëœ annotation ì„¸ì…˜ ë¡œë“œ
        """
        try:
            session_dir = Path(f"outputs/sessions/{session_id}")
            if not session_dir.exists():
                return None, f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}"

            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = session_dir / "session_metadata.json"
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)

            # í”„ë ˆì„ ë° ë§ˆìŠ¤í¬ ë¡œë“œ
            num_frames = metadata["num_frames"]
            self.frames = []
            self.masks = []

            for i in range(num_frames):
                frame_dir = session_dir / f"frame_{i:04d}"

                # ì›ë³¸ í”„ë ˆì„ ë¡œë“œ
                frame_path = frame_dir / "original.png"
                frame = cv2.imread(str(frame_path))
                self.frames.append(frame)

                # ë§ˆìŠ¤í¬ ë¡œë“œ (ìˆìœ¼ë©´)
                mask_path = frame_dir / "mask.png"
                if mask_path.exists():
                    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)
                    self.masks.append((mask > 0).astype(bool))
                else:
                    self.masks.append(None)

            # Annotation points ë³µì›
            self.annotations = {
                'foreground': metadata["annotations"]["foreground"],
                'background': metadata["annotations"]["background"]
            }

            # ë¹„ë””ì˜¤ ê²½ë¡œ ë° í˜„ì¬ í”„ë ˆì„ ì¸ë±ìŠ¤ ë³µì›
            self.video_path = metadata["video_path"]
            self.current_frame_idx = metadata["current_frame_idx"]

            # í˜„ì¬ í”„ë ˆì„ ì‹œê°í™”
            current_frame = self.frames[self.current_frame_idx]
            current_mask = self.masks[self.current_frame_idx]

            frame_rgb = cv2.cvtColor(current_frame, cv2.COLOR_BGR2RGB)
            if current_mask is not None:
                overlay = frame_rgb.copy()
                overlay[current_mask > 0] = [0, 255, 0]
                result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)
            else:
                result = frame_rgb

            # Annotation points í‘œì‹œ
            for px, py in self.annotations['foreground']:
                cv2.circle(result, (px, py), 5, (0, 255, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)
            for px, py in self.annotations['background']:
                cv2.circle(result, (px, py), 5, (255, 0, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)

            masks_loaded = sum(1 for m in self.masks if m is not None)

            status = f"""
### ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ âœ…

**ì„¸ì…˜ ID**: `{session_id}`

**ë¡œë“œëœ ë°ì´í„°**:
- ğŸ“ í”„ë ˆì„: {len(self.frames)}ê°œ
- ğŸ­ ë§ˆìŠ¤í¬: {masks_loaded}ê°œ
- ğŸ“ Foreground points: {len(self.annotations['foreground'])}ê°œ
- ğŸ“ Background points: {len(self.annotations['background'])}ê°œ
- ğŸ“¹ ë¹„ë””ì˜¤: {self.video_path}

**í˜„ì¬ í”„ë ˆì„**: {self.current_frame_idx + 1} / {len(self.frames)}

ì´ì œ í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜, ì¶”ê°€ annotation, propagation ë“±ì„ ê³„ì†í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

            return result, status

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return None, f"ë¡œë“œ ì˜¤ë¥˜: {str(e)}\n\n```\n{error_detail}\n```"

    def get_session_ids(self) -> List[str]:
        """ì €ì¥ëœ ì„¸ì…˜ ID ëª©ë¡ ë°˜í™˜ (Dropdownìš©)"""
        try:
            sessions_dir = Path("outputs/sessions")
            if not sessions_dir.exists():
                return []

            sessions = sorted([d.name for d in sessions_dir.iterdir() if d.is_dir()], reverse=True)
            return sessions

        except Exception as e:
            print(f"ì„¸ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return []

    def list_saved_sessions(self) -> str:
        """ì €ì¥ëœ ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ"""
        try:
            sessions_dir = Path("outputs/sessions")
            if not sessions_dir.exists():
                return "ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤"

            sessions = sorted([d.name for d in sessions_dir.iterdir() if d.is_dir()])

            if not sessions:
                return "ì €ì¥ëœ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤"

            result = "### ì €ì¥ëœ Annotation ì„¸ì…˜ ëª©ë¡\n\n"
            for session_id in sessions:
                metadata_path = sessions_dir / session_id / "session_metadata.json"
                if metadata_path.exists():
                    with open(metadata_path, 'r') as f:
                        metadata = json.load(f)
                    num_frames = metadata["num_frames"]
                    num_masks = sum(1 for info in metadata["frame_info"] if info.get("has_mask", False))
                    video_path = Path(metadata["video_path"]).name if metadata.get("video_path") else "Unknown"

                    result += f"""
**{session_id}**
- ë¹„ë””ì˜¤: `{video_path}`
- í”„ë ˆì„: {num_frames}ê°œ
- ë§ˆìŠ¤í¬: {num_masks}ê°œ
- ê²½ë¡œ: `outputs/sessions/{session_id}/`

---
"""

            return result

        except Exception as e:
            return f"ì˜¤ë¥˜: {str(e)}"

    def save_masks(self) -> str:
        """ë§ˆìŠ¤í¬ë§Œ ê°„ë‹¨íˆ ì €ì¥ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        if all(m is None for m in self.masks):
            return "ì €ì¥í•  ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤"

        try:
            output_dir = Path("outputs/masks")
            output_dir.mkdir(parents=True, exist_ok=True)

            saved_count = 0
            for i, mask in enumerate(self.masks):
                if mask is not None:
                    output_path = output_dir / f"mask_{i:04d}.png"
                    cv2.imwrite(str(output_path), mask.astype(np.uint8) * 255)
                    saved_count += 1

            return f"""
### ë§ˆìŠ¤í¬ ì €ì¥ ì™„ë£Œ âœ…

- **ì €ì¥ëœ ë§ˆìŠ¤í¬**: {saved_count} / {len(self.masks)}
- **ì €ì¥ ìœ„ì¹˜**: `{output_dir}/`

**ì°¸ê³ **: ì „ì²´ ì„¸ì…˜(annotation + masks + metadata)ì„ ì €ì¥í•˜ë ¤ë©´ **"ğŸ’¾ Save Session"** ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.
"""

        except Exception as e:
            return f"ì˜¤ë¥˜: {str(e)}"

    def navigate_frame(self, direction: str, step: int = 1) -> Tuple[np.ndarray, str]:
        """
        í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜

        Args:
            direction: "prev", "next", "first", "last", "goto"
            step: ì´ë™í•  í”„ë ˆì„ ìˆ˜
        """
        if len(self.frames) == 0:
            return None, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

        old_idx = self.current_frame_idx

        if direction == "prev":
            self.current_frame_idx = max(0, self.current_frame_idx - step)
        elif direction == "next":
            self.current_frame_idx = min(len(self.frames) - 1, self.current_frame_idx + step)
        elif direction == "first":
            self.current_frame_idx = 0
        elif direction == "last":
            self.current_frame_idx = len(self.frames) - 1
        elif direction == "goto":
            # stepì€ ì‹¤ì œ í”„ë ˆì„ ë²ˆí˜¸ (0-indexed)
            self.current_frame_idx = max(0, min(len(self.frames) - 1, step))

        # í˜„ì¬ í”„ë ˆì„ ì‹œê°í™”
        frame = self.frames[self.current_frame_idx]
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # ë§ˆìŠ¤í¬ê°€ ìˆìœ¼ë©´ í‘œì‹œ
        mask = self.masks[self.current_frame_idx]
        if mask is not None:
            overlay = frame_rgb.copy()
            overlay[mask > 0] = [0, 255, 0]
            result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)

            mask_area = np.sum(mask > 0)
            mask_pct = mask_area / mask.size * 100
            mask_info = f"ë§ˆìŠ¤í¬: {mask_area} í”½ì…€ ({mask_pct:.1f}%)"
        else:
            result = frame_rgb
            mask_info = "ë§ˆìŠ¤í¬ ì—†ìŒ"

        # Points í‘œì‹œ (annotationì´ ìˆìœ¼ë©´)
        if len(self.annotations['foreground']) > 0 or len(self.annotations['background']) > 0:
            for px, py in self.annotations['foreground']:
                cv2.circle(result, (px, py), 5, (0, 255, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)
            for px, py in self.annotations['background']:
                cv2.circle(result, (px, py), 5, (255, 0, 0), -1)
                cv2.circle(result, (px, py), 7, (255, 255, 255), 2)

        status = f"""
### í”„ë ˆì„ {self.current_frame_idx + 1} / {len(self.frames)}

- **ì´ë™**: {old_idx + 1} â†’ {self.current_frame_idx + 1}
- **{mask_info}**
"""

        return result, status

    def clear_annotations(self) -> Tuple[np.ndarray, str]:
        """
        ëª¨ë“  annotation pointsì™€ masks ì´ˆê¸°í™”

        Returns:
            í˜„ì¬ í”„ë ˆì„ ì´ë¯¸ì§€, ìƒíƒœ ë©”ì‹œì§€
        """
        # Annotations ì´ˆê¸°í™”
        self.annotations['foreground'] = []
        self.annotations['background'] = []

        # Masks ì´ˆê¸°í™”
        self.masks = [None] * len(self.frames) if self.frames else []
        self.current_mask = None

        # í˜„ì¬ í”„ë ˆì„ ì´ë¯¸ì§€ ë°˜í™˜ (annotation ì—†ì´)
        if len(self.frames) > 0:
            current_frame = self.frames[self.current_frame_idx]
            frame_rgb = cv2.cvtColor(current_frame, cv2.COLOR_BGR2RGB)

            status = """
### Annotations ì´ˆê¸°í™” ì™„ë£Œ âœ…

- ëª¨ë“  foreground/background points ì œê±°
- ëª¨ë“  ë§ˆìŠ¤í¬ ì´ˆê¸°í™”
- ìƒˆë¡œ annotation ì‹œì‘ ê°€ëŠ¥

**ë‹¤ìŒ ë‹¨ê³„**: ì´ë¯¸ì§€ í´ë¦­í•˜ì—¬ ìƒˆë¡œìš´ annotation ì‹œì‘
"""
            return frame_rgb, status
        else:
            return None, "ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

    def export_fauna_dataset(
        self,
        animal_name: str = "mouse",
        target_frames: int = 50,
        progress=gr.Progress()
    ) -> str:
        """
        Fauna ë°ì´í„°ì…‹ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§: ì „ì²´ ë¹„ë””ì˜¤ì—ì„œ target_frames ê°œë§Œ ê· ë“± ê°„ê²©ìœ¼ë¡œ ì„ íƒ

        Args:
            animal_name: ë™ë¬¼ ì´ë¦„ (í´ë”ëª…)
            target_frames: ì €ì¥í•  í”„ë ˆì„ ìˆ˜ (ê¸°ë³¸ 50ê°œ)

        Returns:
            ìƒíƒœ ë©”ì‹œì§€
        """
        if len(self.frames) == 0:
            return "âŒ ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        if all(m is None for m in self.masks):
            return "âŒ ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Propagateë¥¼ ì‹¤í–‰í•˜ì„¸ìš”"

        try:
            from datetime import datetime

            progress(0, desc="Fauna ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")

            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            fauna_root = Path.home() / "dev/3DAnimals/data/fauna/Fauna_dataset/large_scale"
            output_dir = fauna_root / animal_name / "train" / "seq_000"
            output_dir.mkdir(parents=True, exist_ok=True)

            # ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§: target_framesê°œë¥¼ ê· ë“± ê°„ê²©ìœ¼ë¡œ ì„ íƒ
            total_frames = len(self.frames)
            if total_frames <= target_frames:
                # í”„ë ˆì„ ìˆ˜ê°€ ì ìœ¼ë©´ ì „ë¶€ ì‚¬ìš©
                selected_indices = list(range(total_frames))
            else:
                # ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                step = total_frames / target_frames
                selected_indices = [int(i * step) for i in range(target_frames)]

            progress(0.1, desc=f"{len(selected_indices)}ê°œ í”„ë ˆì„ ì„ íƒë¨ (ì „ì²´ {total_frames}ê°œ ì¤‘)...")

            # í”„ë ˆì„ ë° ë§ˆìŠ¤í¬ ì €ì¥
            saved_count = 0
            for idx, frame_idx in enumerate(selected_indices):
                if self.masks[frame_idx] is None:
                    continue

                frame = self.frames[frame_idx]
                mask = self.masks[frame_idx]

                # Fauna í˜•ì‹: {index:07d}_rgb.png, {index:07d}_mask.png
                rgb_path = output_dir / f"{idx:07d}_rgb.png"
                mask_path = output_dir / f"{idx:07d}_mask.png"

                # RGB ì €ì¥ (BGR â†’ RGB)
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                cv2.imwrite(str(rgb_path), cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR))

                # Mask ì €ì¥ (0-255 í˜•ì‹)
                mask_uint8 = (mask * 255).astype(np.uint8)
                cv2.imwrite(str(mask_path), mask_uint8)

                saved_count += 1
                progress(0.1 + 0.8 * (idx + 1) / len(selected_indices),
                        desc=f"ì €ì¥ ì¤‘... {idx+1}/{len(selected_indices)}")

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "animal_name": animal_name,
                "sequence": "seq_000",
                "split": "train",
                "total_frames": saved_count,
                "original_video_frames": total_frames,
                "sampling_strategy": "uniform" if total_frames > target_frames else "all",
                "annotations": {
                    "foreground_points": len(self.annotations['foreground']),
                    "background_points": len(self.annotations['background'])
                },
                "export_date": datetime.now().isoformat(),
                "source_video": str(self.video_path) if self.video_path else None
            }

            metadata_path = output_dir / "metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            progress(1.0, desc="Fauna ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")

            return f"""
### Fauna ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ âœ…

**ì €ì¥ ìœ„ì¹˜**: `{output_dir}`

**ë°ì´í„°ì…‹ êµ¬ì¡°**:
```
{animal_name}/train/seq_000/
â”œâ”€â”€ 0000000_rgb.png
â”œâ”€â”€ 0000000_mask.png
â”œâ”€â”€ 0000001_rgb.png
â”œâ”€â”€ 0000001_mask.png
...
â”œâ”€â”€ {saved_count-1:07d}_rgb.png
â”œâ”€â”€ {saved_count-1:07d}_mask.png
â””â”€â”€ metadata.json
```

**í†µê³„**:
- ì €ì¥ëœ í”„ë ˆì„: {saved_count}ê°œ
- ì›ë³¸ ë¹„ë””ì˜¤: {total_frames} í”„ë ˆì„
- ìƒ˜í”Œë§: {"ê· ë“± ê°„ê²© " + str(target_frames) + "ê°œ" if total_frames > target_frames else "ì „ì²´ ì‚¬ìš©"}

**ë‹¤ìŒ ë‹¨ê³„**:
1. ë°ì´í„° ê²€ì¦: `ls {output_dir} | head -20`
2. 3DAnimals í•™ìŠµ ì‹¤í–‰
3. ê²°ê³¼ í™•ì¸ ë° ì‹œê°í™”

**Config ì„¤ì • ì˜ˆì‹œ**:
```yaml
dataset:
  name: {animal_name}
  path: data/fauna/Fauna_dataset/large_scale/{animal_name}
  split: train
```
"""

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âŒ Fauna ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {str(e)}\n\n```\n{error_detail}\n```"

    def export_frames_and_masks(self, output_dir: str = None, progress=gr.Progress()) -> str:
        """
        í”„ë ˆì„ë³„ë¡œ ì›ë³¸ ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ë¥¼ ë³„ë„ í´ë”ì— ì €ì¥

        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ìë™ ìƒì„±)

        Returns:
            ìƒíƒœ ë©”ì‹œì§€
        """
        if len(self.frames) == 0:
            return "âŒ ë¹„ë””ì˜¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"

        if all(m is None for m in self.masks):
            return "âŒ ë§ˆìŠ¤í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Segment ë˜ëŠ” Propagateë¥¼ ì‹¤í–‰í•˜ì„¸ìš”"

        try:
            progress(0, desc="ì €ì¥ ì¤€ë¹„ ì¤‘...")

            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            if output_dir is None:
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_dir = Path(self.config.output_dir if self.config else "outputs") / f"frames_export_{timestamp}"
            else:
                output_dir = Path(output_dir)

            output_dir.mkdir(parents=True, exist_ok=True)

            # ì„œë¸Œë””ë ‰í† ë¦¬ ìƒì„±
            images_dir = output_dir / "images"
            masks_dir = output_dir / "masks"
            overlays_dir = output_dir / "overlays"

            images_dir.mkdir(exist_ok=True)
            masks_dir.mkdir(exist_ok=True)
            overlays_dir.mkdir(exist_ok=True)

            progress(0.1, desc="í”„ë ˆì„ ì €ì¥ ì¤‘...")

            saved_count = 0
            for i, frame in enumerate(self.frames):
                # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
                image_path = images_dir / f"frame_{i:05d}.png"
                cv2.imwrite(str(image_path), frame)

                # ë§ˆìŠ¤í¬ ì €ì¥ (ìˆì„ ê²½ìš°)
                if self.masks[i] is not None:
                    mask = self.masks[i]
                    mask_path = masks_dir / f"frame_{i:05d}.png"
                    mask_uint8 = (mask * 255).astype(np.uint8)
                    cv2.imwrite(str(mask_path), mask_uint8)

                    # ì˜¤ë²„ë ˆì´ ì €ì¥ (ì‹œê°í™”)
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    overlay = frame_rgb.copy()
                    overlay[mask > 0] = [0, 255, 0]
                    result = cv2.addWeighted(frame_rgb, 0.6, overlay, 0.4, 0)
                    result_bgr = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)

                    overlay_path = overlays_dir / f"frame_{i:05d}.png"
                    cv2.imwrite(str(overlay_path), result_bgr)

                    saved_count += 1

                progress((i + 1) / len(self.frames), desc=f"ì €ì¥ ì¤‘... {i+1}/{len(self.frames)}")

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "video_path": str(self.video_path) if self.video_path else None,
                "total_frames": len(self.frames),
                "frames_with_masks": saved_count,
                "annotations": {
                    "foreground": self.annotations['foreground'],
                    "background": self.annotations['background']
                },
                "export_date": datetime.now().isoformat()
            }

            metadata_path = output_dir / "metadata.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            progress(1.0, desc="ì €ì¥ ì™„ë£Œ!")

            return f"""
### í”„ë ˆì„/ë§ˆìŠ¤í¬ ì €ì¥ ì™„ë£Œ âœ…

**ì €ì¥ ìœ„ì¹˜**: `{output_dir}`

**ì €ì¥ëœ íŒŒì¼**:
- ğŸ“ **images/**: ì›ë³¸ í”„ë ˆì„ {len(self.frames)}ê°œ
- ğŸ“ **masks/**: ë§ˆìŠ¤í¬ ì´ë¯¸ì§€ {saved_count}ê°œ
- ğŸ“ **overlays/**: ì‹œê°í™” ì´ë¯¸ì§€ {saved_count}ê°œ
- ğŸ“„ **metadata.json**: ë©”íƒ€ë°ì´í„°

**íŒŒì¼ í˜•ì‹**: PNG (ë¬´ì†ì‹¤)

**ë‹¤ìŒ ë‹¨ê³„**:
- ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©
- í•™ìŠµ ë°ì´í„°ì…‹ìœ¼ë¡œ í™œìš©
- ì™¸ë¶€ ë„êµ¬ë¡œ ì¶”ê°€ ë¶„ì„
"""

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}\n\n```\n{error_detail}\n```"

    # ===== Lite Annotator Event Handlers =====

    def _lite_load_source(
        self,
        input_path: str,
        input_type: str,
        pattern: str
    ) -> Tuple[str, gr.Slider, Dict]:
        """Load video or image folder"""
        if self.lite_annotator is None:
            return "âŒ Lite Annotator not initialized", gr.Slider(maximum=100), {}

        success, msg, total_frames = self.lite_annotator.change_input_source(
            input_path, input_type, pattern
        )

        if success:
            # Update slider maximum
            new_slider = gr.Slider(minimum=0, maximum=max(0, total_frames - 1), value=0, step=1)
            info = self.lite_annotator.get_info()
            return msg, new_slider, info
        else:
            return msg, gr.Slider(maximum=100), {}

    def _lite_load_model(self, model_name: str) -> str:
        """Load SAM 2.1 model"""
        if self.lite_annotator is None:
            return "âŒ Lite Annotator not initialized"

        msg = self.lite_annotator.load_model(model_name)
        return msg

    def _lite_load_frame(self, frame_idx: int) -> Tuple[Optional[np.ndarray], str, Dict]:
        """Load frame at index"""
        if self.lite_annotator is None:
            return None, "âŒ Lite Annotator not initialized", {}

        frame, msg = self.lite_annotator.load_frame(int(frame_idx))
        info = self.lite_annotator.get_info()

        return frame, msg, info

    def _lite_add_point(self, evt: gr.SelectData, point_type: str) -> Tuple[np.ndarray, str]:
        """Add point from click event"""
        if self.lite_annotator is None:
            return None, "âŒ Lite Annotator not initialized"

        x, y = evt.index[0], evt.index[1]
        frame, msg = self.lite_annotator.add_point(x, y, point_type)

        return frame, msg

    def _lite_generate_mask(self) -> Tuple[np.ndarray, Optional[np.ndarray], str]:
        """Generate mask from points"""
        if self.lite_annotator is None:
            return None, None, "âŒ Lite Annotator not initialized"

        frame_vis, mask_binary, msg = self.lite_annotator.generate_mask()

        return frame_vis, mask_binary, msg

    def _lite_save_annotation(self) -> str:
        """Save current annotation"""
        if self.lite_annotator is None:
            return "âŒ Lite Annotator not initialized"

        msg = self.lite_annotator.save_annotation()
        return msg

    def _lite_clear_points(self) -> Tuple[np.ndarray, str]:
        """Clear all points"""
        if self.lite_annotator is None:
            return None, "âŒ Lite Annotator not initialized"

        frame, msg = self.lite_annotator.clear_points()
        return frame, msg

    def create_interface(self):
        """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± - í†µí•© ë²„ì „"""

        with gr.Blocks(title="SAM 3D GUI - Unified Interface") as demo:
            gr.Markdown("""
            # ğŸ¬ SAM 3D GUI - í†µí•© ì›¹ ì¸í„°í˜ì´ìŠ¤

            **ë‘ ê°€ì§€ ì‘ì—… ëª¨ë“œ:**
            - ğŸš€ **Quick Mode**: ìë™ ì„¸ê·¸ë©˜í…Œì´ì…˜ & ëª¨ì…˜ ê°ì§€ (ë¹ ë¦„)
            - ğŸ¨ **Interactive Mode**: ìˆ˜ë™ annotation & propagation (ì •í™•í•¨)
            """)

            # ë¹„ë””ì˜¤ ìë™ ìŠ¤ìº” (Interactive Modeìš©)
            initial_videos = self.scan_videos(self.default_data_dir)
            initial_video = initial_videos[0] if initial_videos else None

            # ì„¸ì…˜ ìë™ ìŠ¤ìº”
            initial_sessions = self.get_session_ids()

            with gr.Tabs():
                # ===== Tab 1: Interactive Mode (ê¸°ë³¸) =====
                with gr.Tab("ğŸ¨ Interactive Mode"):
                    gr.Markdown("### ëŒ€í™”í˜• Annotation & Propagation")

                    with gr.Row():
                        with gr.Column(scale=1):
                            gr.Markdown("### ğŸ“ ë¹„ë””ì˜¤ ë¡œë“œ")

                            data_dir = gr.Textbox(
                                label="ë°ì´í„° ë””ë ‰í† ë¦¬",
                                value=self.default_data_dir
                            )

                            scan_video_btn = gr.Button("ğŸ“‚ ë¹„ë””ì˜¤ ìŠ¤ìº”")

                            video_file = gr.Dropdown(
                                label="ë¹„ë””ì˜¤ íŒŒì¼",
                                choices=initial_videos,
                                value=initial_video,
                                interactive=True
                            )

                            with gr.Row():
                                start_time = gr.Number(label="ì‹œì‘ (ì´ˆ)", value=0.0, minimum=0)
                                duration = gr.Number(label="ê¸¸ì´ (ì´ˆ)", value=3.0, minimum=0.1)

                            load_btn = gr.Button("ğŸ“¹ ë¹„ë””ì˜¤ ë¡œë“œ", variant="primary")

                            gr.Markdown("### ğŸ¯ Annotation")

                            annotation_mode = gr.Radio(
                                label="Point íƒ€ì…",
                                choices=["foreground", "background"],
                                value="foreground"
                            )

                            clear_btn = gr.Button("ğŸ—‘ï¸ Points ì´ˆê¸°í™”")
                            clear_all_btn = gr.Button("ğŸ”„ All Annotations ì´ˆê¸°í™”", variant="stop")
                            segment_btn = gr.Button("âœ‚ï¸ Segment Current Frame", variant="secondary")

                            gr.Markdown("### ğŸ¬ Propagation")

                            propagate_btn = gr.Button("ğŸ”„ Propagate to All Frames", variant="primary")

                            gr.Markdown("### ğŸï¸ í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜")

                            # í”„ë ˆì„ í”„ë¡œê·¸ë ˆìŠ¤ ë°” (ì§ì ‘ ì´ë™ ê°€ëŠ¥)
                            frame_slider = gr.Slider(
                                label="í”„ë ˆì„ ìœ„ì¹˜",
                                minimum=0,
                                maximum=100,
                                value=0,
                                step=1,
                                interactive=True,
                                info="ìŠ¬ë¼ì´ë”ë¥¼ ë“œë˜ê·¸í•˜ì—¬ í”„ë ˆì„ ì´ë™"
                            )

                            with gr.Row():
                                first_btn = gr.Button("â®ï¸ ì²˜ìŒ", size="sm")
                                prev_btn = gr.Button("â—€ï¸ ì´ì „", size="sm")
                                next_btn = gr.Button("â–¶ï¸ ë‹¤ìŒ", size="sm")
                                last_btn = gr.Button("â­ï¸ ë§ˆì§€ë§‰", size="sm")

                            with gr.Row():
                                frame_step = gr.Slider(
                                    label="ì´ë™ ê°„ê²©",
                                    minimum=1,
                                    maximum=10,
                                    value=1,
                                    step=1
                                )

                            with gr.Row():
                                goto_frame = gr.Number(
                                    label="í”„ë ˆì„ ë²ˆí˜¸",
                                    value=1,
                                    minimum=1,
                                    step=1,
                                    scale=2
                                )
                                goto_btn = gr.Button("ì´ë™", scale=1)

                            gr.Markdown("### ğŸ’¾ ì„¸ì…˜ ê´€ë¦¬")

                            save_session_btn = gr.Button("ğŸ’¾ Save Session", variant="primary")

                            gr.Markdown("**ì„¸ì…˜ ë¡œë“œ**")
                            with gr.Row():
                                session_refresh_btn = gr.Button("ğŸ”„ ëª©ë¡ ìƒˆë¡œê³ ì¹¨", size="sm")

                            session_id_dropdown = gr.Dropdown(
                                label="ì„¸ì…˜ ì„ íƒ",
                                choices=initial_sessions,
                                value=initial_sessions[0] if initial_sessions else None,
                                interactive=True,
                                scale=2
                            )
                            load_session_btn = gr.Button("ğŸ“‚ Load Session")

                            gr.Markdown("### ğŸ² 3D & ì¶œë ¥")

                            mesh_btn = gr.Button("ğŸ² Generate 3D Mesh")
                            save_masks_btn = gr.Button("ğŸ’¾ Save Masks Only")
                            export_frames_btn = gr.Button("ğŸ“¤ Export Frames & Masks")

                            gr.Markdown("### ğŸ¦ Fauna ë°ì´í„°ì…‹ ì €ì¥")

                            with gr.Row():
                                fauna_animal_name = gr.Textbox(
                                    label="ë™ë¬¼ ì´ë¦„",
                                    value="mouse",
                                    placeholder="ì˜ˆ: mouse, cat, dog"
                                )
                                fauna_target_frames = gr.Number(
                                    label="ëª©í‘œ í”„ë ˆì„ ìˆ˜",
                                    value=50,
                                    minimum=10,
                                    maximum=500,
                                    step=10
                                )

                            export_fauna_btn = gr.Button("ğŸ¾ Fauna í˜•ì‹ìœ¼ë¡œ ì €ì¥", variant="primary")

                        # ìš°ì¸¡: ì´ë¯¸ì§€ & ê²°ê³¼
                        with gr.Column(scale=2):
                            gr.Markdown("### ğŸ–¼ï¸ Annotation & Results")

                            image_display = gr.Image(
                                label="ì´ë¯¸ì§€ (í´ë¦­í•˜ì—¬ point ì¶”ê°€)",
                                type="numpy",
                                height=500,
                                interactive=True
                            )

                            status_text = gr.Markdown("ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”")

                            mesh_file = gr.File(label="3D Mesh íŒŒì¼")

                    # Interactive Mode ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
                    load_btn.click(
                        fn=self.load_video,
                        inputs=[data_dir, video_file, start_time, duration],
                        outputs=[image_display, status_text, frame_slider]
                    )

                    # ìŠ¬ë¼ì´ë”ë¡œ í”„ë ˆì„ ì´ë™
                    frame_slider.change(
                        fn=lambda frame_idx: self.navigate_frame("goto", int(frame_idx)),
                        inputs=[frame_slider],
                        outputs=[image_display, status_text]
                    )

                    # ì´ë¯¸ì§€ í´ë¦­ ì‹œ point ì¶”ê°€
                    def handle_click(mode, evt: gr.SelectData):
                        """ì´ë¯¸ì§€ í´ë¦­ í•¸ë“¤ëŸ¬ - img íŒŒë¼ë¯¸í„° ì œê±°"""
                        if len(self.frames) == 0:
                            return None, "ë¨¼ì € ë¹„ë””ì˜¤ë¥¼ ë¡œë“œí•˜ì„¸ìš”"

                        # í´ë¦­ ì¢Œí‘œ
                        x, y = evt.index[0], evt.index[1]

                        # Point ì¶”ê°€
                        self.annotations[mode].append((x, y))

                        # í˜„ì¬ í”„ë ˆì„ì— point í‘œì‹œ
                        frame = self.frames[self.current_frame_idx].copy()
                        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                        # Foreground points (ë…¹ìƒ‰)
                        for px, py in self.annotations['foreground']:
                            cv2.circle(frame_rgb, (px, py), 5, (0, 255, 0), -1)
                            cv2.circle(frame_rgb, (px, py), 7, (255, 255, 255), 2)

                        # Background points (ë¹¨ê°„ìƒ‰)
                        for px, py in self.annotations['background']:
                            cv2.circle(frame_rgb, (px, py), 5, (255, 0, 0), -1)
                            cv2.circle(frame_rgb, (px, py), 7, (255, 255, 255), 2)

                        status = f"""
**Annotations:**
- Foreground: {len(self.annotations['foreground'])} points
- Background: {len(self.annotations['background'])} points

í´ë¦­í•œ ìœ„ì¹˜: ({x}, {y}) - {mode}
"""

                        return frame_rgb, status

                    image_display.select(
                        fn=handle_click,
                        inputs=[annotation_mode],
                        outputs=[image_display, status_text]
                    )

                    segment_btn.click(
                        fn=self.segment_current_frame,
                        outputs=[image_display, status_text]
                    )

                    propagate_btn.click(
                        fn=self.propagate_to_all_frames,
                        outputs=[image_display, status_text]
                    )

                    mesh_btn.click(
                        fn=self.generate_3d_mesh,
                        outputs=[mesh_file, status_text]
                    )

                    save_masks_btn.click(
                        fn=self.save_masks,
                        outputs=[status_text]
                    )

                    export_frames_btn.click(
                        fn=self.export_frames_and_masks,
                        outputs=[status_text]
                    )

                    export_fauna_btn.click(
                        fn=self.export_fauna_dataset,
                        inputs=[fauna_animal_name, fauna_target_frames],
                        outputs=[status_text]
                    )

                    clear_all_btn.click(
                        fn=self.clear_annotations,
                        outputs=[image_display, status_text]
                    )

                    # ì„¸ì…˜ ê´€ë¦¬ ì´ë²¤íŠ¸
                    save_session_btn.click(
                        fn=self.save_annotation_session,
                        outputs=[status_text]
                    )

                    session_refresh_btn.click(
                        fn=lambda: gr.Dropdown(choices=self.get_session_ids()),
                        outputs=[session_id_dropdown]
                    )

                    load_session_btn.click(
                        fn=self.load_annotation_session,
                        inputs=[session_id_dropdown],
                        outputs=[image_display, status_text]
                    )

                    def clear_points():
                        self.annotations = {'foreground': [], 'background': []}
                        if len(self.frames) > 0:
                            frame_rgb = cv2.cvtColor(self.frames[self.current_frame_idx], cv2.COLOR_BGR2RGB)
                            return frame_rgb, "Points ì´ˆê¸°í™”ë¨"
                        return None, "Points ì´ˆê¸°í™”ë¨"

                    clear_btn.click(
                        fn=clear_points,
                        outputs=[image_display, status_text]
                    )

                    # ë¹„ë””ì˜¤ ìŠ¤ìº” ë²„íŠ¼
                    scan_video_btn.click(
                        fn=lambda d: gr.Dropdown(choices=self.scan_videos(d)),
                        inputs=[data_dir],
                        outputs=[video_file]
                    )

                    # í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜ ì´ë²¤íŠ¸
                    first_btn.click(
                        fn=lambda: self.navigate_frame("first"),
                        outputs=[image_display, status_text]
                    )

                    prev_btn.click(
                        fn=lambda step: self.navigate_frame("prev", int(step)),
                        inputs=[frame_step],
                        outputs=[image_display, status_text]
                    )

                    next_btn.click(
                        fn=lambda step: self.navigate_frame("next", int(step)),
                        inputs=[frame_step],
                        outputs=[image_display, status_text]
                    )

                    last_btn.click(
                        fn=lambda: self.navigate_frame("last"),
                        outputs=[image_display, status_text]
                    )

                    goto_btn.click(
                        fn=lambda frame_num: self.navigate_frame("goto", int(frame_num) - 1),  # 1-indexed to 0-indexed
                        inputs=[goto_frame],
                        outputs=[image_display, status_text]
                    )

                # ===== Tab 2: Quick Mode =====
                with gr.Tab("ğŸš€ Quick Mode"):
                    gr.Markdown("### ë¹ ë¥¸ ìë™ ì²˜ë¦¬")

                    with gr.Row():
                        with gr.Column(scale=1):
                            quick_data_dir = gr.Textbox(
                                label="ë°ì´í„° ë””ë ‰í† ë¦¬",
                                value=self.default_data_dir
                            )

                            quick_scan_btn = gr.Button("ğŸ“‚ ë¹„ë””ì˜¤ ìŠ¤ìº”")

                            quick_video_list = gr.Dropdown(
                                label="ë¹„ë””ì˜¤ íŒŒì¼",
                                choices=initial_videos,
                                value=initial_video,
                                interactive=True
                            )

                            with gr.Row():
                                quick_start = gr.Number(label="ì‹œì‘(ì´ˆ)", value=0.0)
                                quick_duration = gr.Number(label="ê¸¸ì´(ì´ˆ)", value=3.0)

                            quick_threshold = gr.Slider(
                                label="ëª¨ì…˜ ì„ê³„ê°’",
                                minimum=0, maximum=200, value=50.0, step=1
                            )

                            quick_method = gr.Radio(
                                label="ì„¸ê·¸ë©˜í…Œì´ì…˜",
                                choices=["contour", "simple_threshold", "grabcut"],
                                value="contour"
                            )

                            quick_process_btn = gr.Button("ğŸš€ ìë™ ì²˜ë¦¬", variant="primary", size="lg")

                        with gr.Column(scale=2):
                            quick_image = gr.Image(label="ê²°ê³¼", type="numpy", height=500)
                            quick_status = gr.Markdown("ë¹„ë””ì˜¤ë¥¼ ì„ íƒí•˜ê³  ì²˜ë¦¬í•˜ì„¸ìš”")

                    # Quick Mode ì´ë²¤íŠ¸
                    quick_scan_btn.click(
                        fn=lambda d: gr.Dropdown(choices=self.scan_videos(d)),
                        inputs=[quick_data_dir],
                        outputs=[quick_video_list]
                    )

                    quick_process_btn.click(
                        fn=self.quick_process,
                        inputs=[quick_data_dir, quick_video_list, quick_start,
                               quick_duration, quick_threshold, quick_method],
                        outputs=[quick_image, quick_status]
                    )

                # ===== Tab 3: Lite Annotator =====
                with gr.Tab("ğŸ“ Lite Annotator"):
                    gr.Markdown("### íš¨ìœ¨ì  Annotation ëª¨ë“œ")
                    gr.Markdown("Direct video/image loading, multi-model selection, auto-restore")

                    with gr.Row():
                        # Left column: Input & Frame Display
                        with gr.Column(scale=2):
                            # Input source section
                            gr.Markdown("#### ğŸ“‚ Input Source")
                            with gr.Row():
                                lite_input_path = gr.Textbox(
                                    label="Video/Image Folder Path",
                                    placeholder="/path/to/video.mp4 or /path/to/images/",
                                    scale=3
                                )
                                lite_input_type = gr.Radio(
                                    choices=["video", "images"],
                                    value="video",
                                    label="Type",
                                    scale=1
                                )

                            with gr.Row():
                                lite_pattern = gr.Textbox(
                                    label="Image Pattern (for images type)",
                                    value="*.png",
                                    scale=2
                                )
                                lite_load_btn = gr.Button("ğŸ“¥ Load Source", variant="primary", scale=1)

                            lite_load_status = gr.Markdown("No input loaded")

                            # Frame display
                            gr.Markdown("#### ğŸ–¼ï¸ Frame")
                            lite_frame_display = gr.Image(
                                label="Current Frame",
                                type="numpy",
                                height=500,
                                interactive=True  # Enable click events
                            )

                            # Frame navigation
                            with gr.Row():
                                lite_frame_slider = gr.Slider(
                                    label="Frame Index",
                                    minimum=0,
                                    maximum=100,
                                    value=0,
                                    step=1,
                                    interactive=True
                                )

                        # Right column: Controls & Mask Display
                        with gr.Column(scale=1):
                            # Model selection
                            gr.Markdown("#### ğŸ¤– Model Selection")
                            lite_model_dropdown = gr.Dropdown(
                                choices=["tiny", "small", "base_plus", "large"],
                                value="large",
                                label="SAM 2.1 Model",
                                info="tiny: fastest, large: best quality"
                            )
                            lite_load_model_btn = gr.Button("Load Model", size="sm")

                            # Point annotation
                            gr.Markdown("#### ğŸ¨ Annotation")
                            lite_point_type = gr.Radio(
                                choices=["foreground", "background"],
                                value="foreground",
                                label="Point Type"
                            )

                            # Action buttons
                            with gr.Column():
                                lite_generate_btn = gr.Button("ğŸ¯ Generate Mask", variant="primary")
                                lite_save_btn = gr.Button("ğŸ’¾ Save Annotation", variant="secondary")
                                lite_clear_btn = gr.Button("ğŸ”„ Clear Points", variant="stop")

                            # Mask display
                            gr.Markdown("#### ğŸ­ Mask")
                            lite_mask_display = gr.Image(
                                label="Generated Mask",
                                type="numpy",
                                height=200
                            )

                            # Status
                            lite_status = gr.Markdown("**Status:** Load source to start")

                            # Info panel
                            gr.Markdown("#### â„¹ï¸ Info")
                            lite_info = gr.JSON(label="Current State", value={})

                    # Event handlers for Lite Annotator

                    # Load source
                    lite_load_btn.click(
                        fn=self._lite_load_source,
                        inputs=[lite_input_path, lite_input_type, lite_pattern],
                        outputs=[lite_load_status, lite_frame_slider, lite_info]
                    )

                    # Load model
                    lite_load_model_btn.click(
                        fn=self._lite_load_model,
                        inputs=[lite_model_dropdown],
                        outputs=[lite_status]
                    )

                    # Frame slider change
                    lite_frame_slider.change(
                        fn=self._lite_load_frame,
                        inputs=[lite_frame_slider],
                        outputs=[lite_frame_display, lite_status, lite_info]
                    )

                    # Click on frame to add point
                    lite_frame_display.select(
                        fn=self._lite_add_point,
                        inputs=[lite_point_type],
                        outputs=[lite_frame_display, lite_status]
                    )

                    # Generate mask
                    lite_generate_btn.click(
                        fn=self._lite_generate_mask,
                        outputs=[lite_frame_display, lite_mask_display, lite_status]
                    )

                    # Save annotation
                    lite_save_btn.click(
                        fn=self._lite_save_annotation,
                        outputs=[lite_status]
                    )

                    # Clear points
                    lite_clear_btn.click(
                        fn=self._lite_clear_points,
                        outputs=[lite_frame_display, lite_status]
                    )

        return demo

def main():
    """ì›¹ ì•± ì‹¤í–‰"""
    import os

    app = SAMInteractiveWebApp()
    demo = app.create_interface()

    # í¬íŠ¸ ì„¤ì •: í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” 7860-7870 ë²”ìœ„ì—ì„œ ìë™ ì„ íƒ
    port = int(os.getenv("GRADIO_SERVER_PORT", "7860"))

    demo.launch(
        server_name="0.0.0.0",
        server_port=port,  # í¬íŠ¸ ì‚¬ìš© ì¤‘ì´ë©´ ìë™ìœ¼ë¡œ ë‹¤ìŒ í¬íŠ¸ ì‹œë„
        share=False,
        debug=True,
        max_threads=40  # ë™ì‹œ ì²˜ë¦¬ ì¦ê°€
    )

if __name__ == "__main__":
    main()
