# SAM 2 Propagation Mechanism & SAM3D ìƒì„¸ ë¶„ì„

**ë‚ ì§œ**: 2025-11-25
**ì‘ì„±ì**: Claude Code
**í”„ë¡œì íŠ¸**: sam3d_gui

---

## ğŸ“‹ Executive Summary

ì´ ë³´ê³ ì„œëŠ” Meta AIì˜ SAM 2 Video Predictorì˜ propagation ë©”ì»¤ë‹ˆì¦˜, SAM3D ëª¨ë¸ì˜ ì‹¤ì²´ì™€ ì°¨ì´ì , ê·¸ë¦¬ê³  í˜„ì¬ sam3d_gui í”„ë¡œì íŠ¸ì—ì„œì˜ í†µí•© ë°©ì•ˆì„ ë¶„ì„í•©ë‹ˆë‹¤.

**í•µì‹¬ ë°œê²¬ì‚¬í•­**:
- SAM 2ëŠ” memory-based trackingìœ¼ë¡œ í•œ í”„ë ˆì„ annotationë§Œìœ¼ë¡œ ë¹„ë””ì˜¤ ì „ì²´ ì¶”ì  ê°€ëŠ¥
- SAM3DëŠ” ì‹¤ì œë¡œ ì¡´ì¬í•˜ë©°, ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ 3D ì¬êµ¬ì„±ì„ ìˆ˜í–‰í•˜ëŠ” Metaì˜ ìµœì‹  ëª¨ë¸
- SAM2 Image Predictorì™€ Video PredictorëŠ” ë©”ëª¨ë¦¬ ë©”ì»¤ë‹ˆì¦˜ ìœ ë¬´ë¡œ êµ¬ë¶„ë˜ë©°, ê°ê° ë‹¤ë¥¸ ìš©ë„
- í˜„ì¬ ì½”ë“œëŠ” SAM2ImagePredictorë§Œ ì‚¬ìš© ì¤‘, Video Predictor í†µí•© ì‹œ íš¨ìœ¨ì„± ëŒ€í­ í–¥ìƒ ê°€ëŠ¥

---

## 1. SAM 2 Video Predictorì˜ Propagation ë©”ì»¤ë‹ˆì¦˜

### 1.1 Memory-Based Tracking ì•„í‚¤í…ì²˜

SAM 2ëŠ” **memory-augmented streaming architecture**ë¥¼ ë„ì…í•˜ì—¬ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤[1][2].

#### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

**1. Memory Encoder (ë©”ëª¨ë¦¬ ì¸ì½”ë”)**
- ì˜ˆì¸¡ëœ ë§ˆìŠ¤í¬ì™€ ì´ë¯¸ì§€ íŠ¹ì§•ì„ ê²°í•©í•˜ì—¬ ë©”ëª¨ë¦¬ í‘œí˜„ ìƒì„±
- Hiera ì´ë¯¸ì§€ ì¸ì½”ë”ì˜ ì¶œë ¥ì„ ì¬ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì„± í™•ë³´
- ë” í° ì´ë¯¸ì§€ ì¸ì½”ë”ë¡œ í™•ì¥ ê°€ëŠ¥í•œ ì„¤ê³„

```python
# Pseudo-code
def memory_encoder(image_embedding, predicted_mask):
    """
    Args:
        image_embedding: Hiera encoder ì¶œë ¥ (ê°•ë ¥í•œ ì‹œê°ì  í‘œí˜„)
        predicted_mask: í˜„ì¬ í”„ë ˆì„ì˜ ì˜ˆì¸¡ ë§ˆìŠ¤í¬
    Returns:
        memory_feature: ì €ì¥í•  ë©”ëª¨ë¦¬ í‘œí˜„
    """
    # ì´ë¯¸ì§€ íŠ¹ì§•ê³¼ ë§ˆìŠ¤í¬ ì •ë³´ ìœµí•©
    memory_feature = fuse(image_embedding, predicted_mask)
    return memory_feature
```

**2. Memory Bank (ë©”ëª¨ë¦¬ ë±…í¬)**
- **FIFO (First-In-First-Out)** ë°©ì‹ì˜ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
- ìµœê·¼ Nê°œ í”„ë ˆì„ì˜ ë©”ëª¨ë¦¬ ë³´ê´€ (Nì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°)
- Temporal position ì •ë³´ë¥¼ ì„ë² ë”©ì— í¬í•¨í•˜ì—¬ ë‹¨ê¸° ê°ì²´ ì›€ì§ì„ í‘œí˜„

```
Memory Bank êµ¬ì¡°:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frame t-3  Frame t-2  Frame t-1  â”‚ (ì €ì¥)
â”‚   [mem]      [mem]      [mem]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ (cross-attention)
      Current Frame t
```

**3. Memory Attention (ë©”ëª¨ë¦¬ ì–´í…ì…˜)**
- **Self-attention**: í˜„ì¬ í”„ë ˆì„ ë‚´ë¶€ì˜ íŠ¹ì§• ê´€ê³„ íŒŒì•…
- **Cross-attention**: ì €ì¥ëœ ê³¼ê±° í”„ë ˆì„ ë©”ëª¨ë¦¬ì™€ í˜„ì¬ í”„ë ˆì„ ê°„ ì—°ê²°
- Transformer ë¸”ë¡ ê¸°ë°˜ìœ¼ë¡œ temporal ì •ë³´ í†µí•©

```python
# Memory Attention ë™ì‘ ì›ë¦¬
def memory_attention(current_frame_features, memory_bank):
    """
    Args:
        current_frame_features: í˜„ì¬ í”„ë ˆì„ì˜ íŠ¹ì§•
        memory_bank: ê³¼ê±° Nê°œ í”„ë ˆì„ì˜ ë©”ëª¨ë¦¬
    Returns:
        attended_features: ë©”ëª¨ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ë³´ê°•ëœ íŠ¹ì§•
    """
    # Self-attention: í˜„ì¬ í”„ë ˆì„ ë‚´ë¶€ ê´€ê³„
    self_attn = self_attention(current_frame_features)

    # Cross-attention: ê³¼ê±° ë©”ëª¨ë¦¬ì™€ í˜„ì¬ í”„ë ˆì„ ì—°ê²°
    cross_attn = cross_attention(
        query=current_frame_features,
        key_value=memory_bank  # ê³¼ê±° í”„ë ˆì„ë“¤
    )

    # í†µí•©: ì‹œê°„ì  ì¼ê´€ì„± í™•ë³´
    attended_features = combine(self_attn, cross_attn)
    return attended_features
```

### 1.2 ì™œ í•œ í”„ë ˆì„ Annotationë§Œìœ¼ë¡œ ê°€ëŠ¥í•œê°€?

**í•µì‹¬ ì›ë¦¬: Memory Propagation**

1. **ì´ˆê¸° í”„ë ˆì„ (t=0)**:
   - ì‚¬ìš©ìê°€ í´ë¦­ ë˜ëŠ” ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ê°ì²´ ì§€ì •
   - SAM 2ê°€ ì •í™•í•œ ë§ˆìŠ¤í¬ ìƒì„±
   - ì´ ë§ˆìŠ¤í¬ì™€ ì´ë¯¸ì§€ íŠ¹ì§•ì„ ë©”ëª¨ë¦¬ë¡œ ì €ì¥

2. **ë‹¤ìŒ í”„ë ˆì„ (t=1)**:
   - **ì´ì „ ë©”ëª¨ë¦¬ ì°¸ì¡°**: t=0ì˜ ê°ì²´ í‘œí˜„ì„ Memory Bankì—ì„œ ê°€ì ¸ì˜´
   - **Cross-attention**: í˜„ì¬ í”„ë ˆì„ì—ì„œ ìœ ì‚¬í•œ íŠ¹ì§• ì°¾ê¸°
   - **ë§ˆìŠ¤í¬ ì˜ˆì¸¡**: ê°ì²´ê°€ ì–´ë””ë¡œ ì´ë™í–ˆëŠ”ì§€ ì¶”ë¡ 
   - **ìƒˆ ë©”ëª¨ë¦¬ ì €ì¥**: t=1ì˜ í‘œí˜„ì„ Memory Bankì— ì¶”ê°€

3. **ì´í›„ í”„ë ˆì„ (t=2, 3, ...)**:
   - ë™ì¼í•œ ê³¼ì • ë°˜ë³µ
   - ì—¬ëŸ¬ ê³¼ê±° í”„ë ˆì„ì˜ ë©”ëª¨ë¦¬ë¥¼ ì¢…í•©í•˜ì—¬ ë” robustí•œ ì¶”ì 

```
Timeline ì‹œê°í™”:

Frame 0 (User annotation)
  â†“
  ğŸ–±ï¸ User clicks on mouse
  â†“
  ğŸ­ SAM 2 generates mask
  â†“
  ğŸ’¾ Store in Memory Bank

Frame 1 (Automatic)
  â†“
  ğŸ“– Read Memory Bank (Frame 0 info)
  â†“
  ğŸ” Cross-attention: Find object in Frame 1
  â†“
  ğŸ­ Generate mask automatically
  â†“
  ğŸ’¾ Store Frame 1 memory

Frame 2 (Automatic)
  â†“
  ğŸ“– Read Memory Bank (Frame 0, 1 info)
  â†“
  ğŸ” Cross-attention with multiple memories
  â†“
  ğŸ­ Generate mask (more robust)
  â†“
  ğŸ’¾ Store Frame 2 memory

... (continues for all frames)
```

### 1.3 Temporal Consistency í•™ìŠµ

**ì•”ë¬µì  í•™ìŠµ (Implicit Learning)**:
- í•™ìŠµ ë‹¨ê³„ì—ì„œ memory-based frame propagationì„ í†µí•´ ì‹œê°„ì  ì¼ê´€ì„± í•™ìŠµ
- ëª…ì‹œì ì¸ optical flowë‚˜ tracking loss ì—†ì´ë„ ì¼ê´€ëœ ì¶”ì  ê°€ëŠ¥
- ë©”ëª¨ë¦¬ ë©”ì»¤ë‹ˆì¦˜ ìì²´ê°€ temporal consistencyë¥¼ ë³´ì¥

**Occlusion Handling (ê°€ë¦¼ ì²˜ë¦¬)**:
- ê°ì²´ê°€ ì¼ì‹œì ìœ¼ë¡œ ê°€ë ¤ì§ˆ ë•Œ: ë©”ëª¨ë¦¬ì— ì €ì¥ëœ ê³¼ê±° ì •ë³´ í™œìš©
- ì¬ë“±ì¥ ì‹œ: ë©”ëª¨ë¦¬ ê¸°ë°˜ cross-attentionìœ¼ë¡œ ìë™ ì¬ì¸ì‹
- ê¸´ occlusion: ì—¬ëŸ¬ í”„ë ˆì„ì˜ ë©”ëª¨ë¦¬ ì¢…í•©ìœ¼ë¡œ ë³µì›

```python
# Occlusion ì²˜ë¦¬ ì˜ˆì‹œ
def track_with_occlusion(frames, initial_mask):
    memory_bank = []
    predictions = []

    for t, frame in enumerate(frames):
        if t == 0:
            # ì´ˆê¸° annotation
            mask = initial_mask
        else:
            # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì 
            if len(memory_bank) > 0:
                # Cross-attention with memory
                mask = predict_from_memory(frame, memory_bank)

                # ê°€ë¦¼ ê°ì§€ (confidence ë‚®ìŒ)
                if mask.confidence < threshold:
                    # ì´ì „ ë©”ëª¨ë¦¬ë“¤ë¡œë¶€í„° ë³´ì™„
                    mask = recover_from_history(memory_bank)

        # ë©”ëª¨ë¦¬ ì €ì¥
        memory = encode_memory(frame, mask)
        memory_bank.append(memory)
        predictions.append(mask)

    return predictions
```

### 1.4 ì„±ëŠ¥ ì§€í‘œ

- **ì†ë„**: ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥ (~44 FPS)[3]
- **ì •í™•ë„**: ê¸°ì¡´ SOTA ëŒ€ë¹„ ìš°ìˆ˜í•œ segmentation quality
- **ë©”ëª¨ë¦¬**: FIFO ë°©ì‹ìœ¼ë¡œ ì¼ì •í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìœ ì§€

---

## 2. SAM3D ëª¨ë¸ ì¡°ì‚¬

### 2.1 SAM3Dì˜ ì‹¤ì²´

**ê²°ë¡ : SAM3DëŠ” ì‹¤ì œë¡œ ì¡´ì¬í•˜ë©°, Meta AIê°€ 2024ë…„ 11ì›” ê³µê°œí•œ ê³µì‹ ëª¨ë¸ì…ë‹ˆë‹¤**[4][5].

#### SAM3D ê³µì‹ ì •ë³´

- **ë°œí‘œì¼**: 2024ë…„ 11ì›” 19ì¼
- **ê°œë°œì**: Meta AI
- **ê³µì‹ í˜ì´ì§€**: https://ai.meta.com/sam3d/
- **GitHub**: https://github.com/facebookresearch/sam-3d-objects
- **Demo**: https://sam3d.org/

#### ë‘ ê°€ì§€ ì „ë¬¸ ëª¨ë¸

**1. SAM 3D Objects**
- **ìš©ë„**: ì¼ë°˜ ê°ì²´ ë° ì¥ë©´ ì¬êµ¬ì„±
- **íŠ¹ì§•**:
  - ë‹¨ì¼ RGB ì´ë¯¸ì§€ì—ì„œ ê³ í’ˆì§ˆ 3D ë©”ì‰¬ ìƒì„±
  - Occlusion, clutter, ì‘ì€ ê°ì²´, ë¹„ì •ìƒì  ì‹œì ì—ì„œë„ robust
  - Single-object ë° multi-object ìƒì„± ì§€ì›
  - ì¸ê°„ ì„ í˜¸ë„ í…ŒìŠ¤íŠ¸ì—ì„œ ê¸°ì¡´ SOTA ëŒ€ë¹„ 5:1 ìŠ¹ë¥ [6]

**2. SAM 3D Body**
- **ìš©ë„**: ì¸ì²´ 3D ì¬êµ¬ì„± ì „ìš©
- **íŠ¹ì§•**:
  - ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ì‹ ì²´ í˜•íƒœ, ìì„¸ ì¶”ì •
  - Meta Momentum Human Rig (MHR) í¬ë§· ì§€ì›
  - Rigging ë° animation ê¸°ëŠ¥

### 2.2 SAM3D ê¸°ìˆ ì  ì›ë¦¬

#### Workflow (ì²˜ë¦¬ íë¦„)

```
Input: Single RGB Image
  â†“
Step 1: Segmentation (SAM ê¸°ë°˜)
  - User clicks object
  - SAM generates 2D mask
  â†“
Step 2: 3D Geometry Inference
  - ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ 3D í˜•ìƒ ì¶”ë¡ 
  - Depth, normal, occlusion ê³ ë ¤
  â†“
Step 3: Texture & Pose Estimation
  - RGB í…ìŠ¤ì²˜ ë§¤í•‘
  - ê°ì²´ ìì„¸ ì¶”ì •
  â†“
Output: High-quality 3D Mesh
  - PLY, OBJ, GLB í¬ë§·
  - UV mapping ë³´ì¡´
```

#### Progressive Training (ì ì§„ì  í•™ìŠµ)

```python
# SAM3D Objectsì˜ í•™ìŠµ ì „ëµ
def progressive_training():
    """
    ë‹¨ê³„ì ìœ¼ë¡œ ë³µì¡í•œ ë°ì´í„° í•™ìŠµ
    """
    # Stage 1: Clean backgrounds, simple objects
    train_on(clean_data)

    # Stage 2: Add occlusion
    train_on(occluded_data)

    # Stage 3: Add clutter
    train_on(cluttered_scenes)

    # Stage 4: Small objects, unusual viewpoints
    train_on(challenging_data)
```

#### Data Engine with Human Feedback

- ì¸ê°„ í‰ê°€ìì˜ í”¼ë“œë°±ì„ í•™ìŠµì— í†µí•©
- Iterative refinementë¡œ í’ˆì§ˆ í–¥ìƒ
- Real-world scenariosì— ê°•ê±´

### 2.3 SAM3D Output Formats

| Format | ìš©ë„ | íŠ¹ì§• |
|--------|------|------|
| **PLY** | Point cloud | Gaussian Splatting ì§€ì› |
| **OBJ** | Mesh | í…ìŠ¤ì²˜ + UV mapping |
| **GLB** | 3D scene | Unity, Unreal Engine í˜¸í™˜ |
| **MHR** | Human body | Animation rigging (SAM 3D Body ì „ìš©) |

### 2.4 í˜„ì¬ í”„ë¡œì íŠ¸ì—ì„œì˜ ì‚¬ìš©

**sam3d_gui í”„ë¡œì íŠ¸**ëŠ” ì´ë¯¸ **SAM 3D Objects**ë¥¼ í†µí•©í•˜ì—¬ ì‚¬ìš© ì¤‘:

```python
# /home/joon/dev/sam3d_gui/src/sam3d_processor.py
# Line 113
self.inference_model = Inference(config_path, compile=False)

# Line 338
output = self.inference_model(frame, mask, seed=seed)
```

**ì‚¬ìš© íë¦„**:
1. ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œ
2. SAM2 (ë˜ëŠ” ê¸°ì¡´ segmentation)ë¡œ 2D ë§ˆìŠ¤í¬ ìƒì„±
3. SAM3D Objectsë¡œ 3D ì¬êµ¬ì„±
4. PLY íŒŒì¼ë¡œ ì €ì¥

---

## 3. SAM2 Image Predictor vs Video Predictor

### 3.1 í•µì‹¬ ì°¨ì´ì 

| êµ¬ë¶„ | SAM2ImagePredictor | SAM2VideoPredictor |
|------|-------------------|-------------------|
| **ë©”ëª¨ë¦¬ ë©”ì»¤ë‹ˆì¦˜** | âŒ ì—†ìŒ | âœ… Memory Bank + Memory Attention |
| **ì´ˆê¸°í™”** | `build_sam2()` | `build_sam2_video_predictor()` |
| **ìš©ë„** | ì •ì  ì´ë¯¸ì§€ segmentation | ë¹„ë””ì˜¤ ê°ì²´ ì¶”ì  |
| **Temporal consistency** | âŒ í”„ë ˆì„ ê°„ ë…ë¦½ì  | âœ… ì‹œê°„ì  ì¼ê´€ì„± ë³´ì¥ |
| **Annotation í•„ìš”** | ëª¨ë“  í”„ë ˆì„ì— í•„ìš” | í•œ í”„ë ˆì„ë§Œ í•„ìš” |
| **Occlusion ì²˜ë¦¬** | âŒ ë¶ˆê°€ëŠ¥ | âœ… ë©”ëª¨ë¦¬ ê¸°ë°˜ ë³µì› |
| **ì†ë„** | ë¹ ë¦„ (~47 FPS, tiny) | ë¹ ë¦„ (~44 FPS, ì‹¤ì‹œê°„) |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | ë‚®ìŒ | ì¤‘ê°„ (FIFOë¡œ ì œí•œ) |

### 3.2 ë”°ë¡œ ì¨ì•¼ í•˜ëŠ” ì´ìœ 

#### ì•„í‚¤í…ì²˜ ì°¨ì´

**SAM2ImagePredictor**:
```python
# ì´ë¯¸ì§€ ë‹¨ì¼ ì²˜ë¦¬
predictor.set_image(image)
mask = predictor.predict(point_coords, point_labels)
# ë‹¤ìŒ ì´ë¯¸ì§€ëŠ” ì™„ì „íˆ ë…ë¦½ì 
```

**SAM2VideoPredictor**:
```python
# ë¹„ë””ì˜¤ ìƒíƒœ ê´€ë¦¬
predictor.init_state(video_path)  # ë¹„ë””ì˜¤ ì „ì²´ ì´ˆê¸°í™”

# ì²« í”„ë ˆì„ annotation
predictor.add_new_points(frame_idx=0, points, labels)

# ë‚˜ë¨¸ì§€ í”„ë ˆì„ ìë™ propagation
for frame_idx in range(1, num_frames):
    mask = predictor.propagate(frame_idx)  # ìë™ ì¶”ì 
```

**ë©”ëª¨ë¦¬ êµ¬ì¡°**:
- Image Predictor: í”„ë ˆì„ ê°„ ì •ë³´ ê³µìœ  ì—†ìŒ
- Video Predictor: Memory Bankì— ê³¼ê±° í”„ë ˆì„ ì €ì¥ ë° í™œìš©

### 3.3 Use Cases ë¶„ì„

#### SAM2ImagePredictor ì í•©í•œ ê²½ìš°

âœ… **ì •ì  ì´ë¯¸ì§€ segmentation**
- ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„
- Batch processing (í”„ë ˆì„ ê°„ ê´€ê³„ ì—†ìŒ)
- ì´ë¯¸ì§€ ë°ì´í„°ì…‹ annotation

âœ… **ë…ë¦½ì  í”„ë ˆì„ ì²˜ë¦¬**
- ë¹„ë””ì˜¤ í”„ë ˆì„ë“¤ì„ ê°ê° ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬
- ê°ì²´ê°€ í”„ë ˆì„ë§ˆë‹¤ ë‹¤ë¦„
- ì‹œê°„ì  ì¼ê´€ì„± ë¶ˆí•„ìš”

**ì˜ˆì‹œ**:
```python
# í˜„ì¬ sam3d_guiì˜ lite_annotator.py ì‚¬ìš© íŒ¨í„´
for frame in frames:
    predictor.set_image(frame)
    mask = predictor.predict(points, labels)  # ê° í”„ë ˆì„ ë…ë¦½ ì²˜ë¦¬
```

#### SAM2VideoPredictor ì í•©í•œ ê²½ìš°

âœ… **ë¹„ë””ì˜¤ ê°ì²´ ì¶”ì **
- í•œ ê°ì²´ë¥¼ ì—¬ëŸ¬ í”„ë ˆì„ì—ì„œ ì¶”ì 
- Annotation íš¨ìœ¨ì„± (í•œ í”„ë ˆì„ë§Œ annotation)
- ì‹œê°„ì  ì¼ê´€ì„± í•„ìš”

âœ… **Occlusion ì²˜ë¦¬**
- ê°ì²´ ì¼ì‹œì  ê°€ë¦¼
- ì¬ë“±ì¥ ìë™ ê°ì§€

âœ… **ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ segmentation**
- ìŠ¤íŠ¸ë¦¬ë° ë¹„ë””ì˜¤
- ë¼ì´ë¸Œ ì¹´ë©”ë¼ ì…ë ¥

**ì˜ˆì‹œ**:
```python
# íš¨ìœ¨ì ì¸ ë¹„ë””ì˜¤ ì¶”ì  (ê¶Œì¥)
video_predictor.init_state(video_path)

# ì²« í”„ë ˆì„ë§Œ annotation
video_predictor.add_new_points(frame_idx=0, points=[mouse_click], labels=[1])

# ë‚˜ë¨¸ì§€ ìë™ ì¶”ì  (no annotation needed!)
for frame_idx in range(1, total_frames):
    mask = video_predictor.propagate_in_video(frame_idx)
    # ìë™ìœ¼ë¡œ ê°ì²´ ì¶”ì 
```

### 3.4 í†µí•© ê°€ëŠ¥ì„±

**ê³µì‹ ë‹µë³€ (Meta AI)**[7]:
- "SAM 2 has all the capabilities of SAM on static images"
- Imageì™€ Video APIê°€ ëª¨ë‘ ì œê³µë¨
- í•˜ì§€ë§Œ **ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì°¨ì´**ë¡œ ì¸í•´ ë³„ë„ API í•„ìš”

**í†µí•© ì‹œë„ ì‹œ ë¬¸ì œ**:
```python
# âŒ ì´ë ‡ê²Œ í•  ìˆ˜ ì—†ìŒ
video_predictor = build_sam2_video_predictor(...)
video_predictor.set_image(single_image)  # ì—ëŸ¬!

# âœ… ì˜¬ë°”ë¥¸ ì‚¬ìš©
image_predictor = build_sam2(...)
image_predictor.set_image(single_image)  # ì •ìƒ
```

**ì´ìœ **:
- Video PredictorëŠ” `init_state(video_path)` í˜¸ì¶œ í•„ìš”
- ë¹„ë””ì˜¤ ì „ì²´ì˜ ë©”ëª¨ë¦¬ ìƒíƒœ ê´€ë¦¬
- ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ì—ëŠ” ë¶ˆí•„ìš”í•œ ì˜¤ë²„í—¤ë“œ

---

## 4. í˜„ì¬ ì½”ë“œ ë¶„ì„ ë° ê°œì„  ì œì•ˆ

### 4.1 í˜„ì¬ êµ¬í˜„ ìƒíƒœ

#### 4.1.1 lite_annotator.py (í˜„ì¬)

```python
# /home/joon/dev/sam3d_gui/src/lite_annotator.py
from sam2.sam2_image_predictor import SAM2ImagePredictor  # Image Predictorë§Œ ì‚¬ìš©

class LiteAnnotator:
    def __init__(self, sam2_base_path, device="cuda"):
        self.predictor = None  # SAM2ImagePredictor

    def load_frame(self, frame_idx):
        """ê° í”„ë ˆì„ì„ ë…ë¦½ì ìœ¼ë¡œ ë¡œë“œ"""
        # ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ ì½ê¸°
        self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = self.video_cap.read()

        # Annotation ë³µì› (íŒŒì¼ì—ì„œ)
        annotation_file = f'frame_{frame_idx:04d}_annotation.json'
        if annotation_file.exists():
            # ì €ì¥ëœ annotation ë¡œë“œ
            self.points = load_points(annotation_file)

    def generate_mask(self):
        """SAM2 Image Predictorë¡œ ë§ˆìŠ¤í¬ ìƒì„±"""
        self.predictor.set_image(self.current_frame)  # ë§¤ë²ˆ set_image í˜¸ì¶œ
        masks, scores, _ = self.predictor.predict(
            point_coords=self.points,
            point_labels=self.labels
        )
```

**ë¬¸ì œì **:
- âŒ ëª¨ë“  í”„ë ˆì„ì— ìˆ˜ë™ annotation í•„ìš”
- âŒ í”„ë ˆì„ ê°„ ì •ë³´ ê³µìœ  ì—†ìŒ
- âŒ ë™ì¼ ê°ì²´ë¥¼ ì—¬ëŸ¬ ë²ˆ annotation í•´ì•¼ í•¨
- âŒ ì‹œê°„ì  ì¼ê´€ì„± ë³´ì¥ ì—†ìŒ

#### 4.1.2 web_app.py (Importë§Œ ì¡´ì¬)

```python
# /home/joon/dev/sam3d_gui/src/web_app.py
from sam2.sam2_video_predictor import SAM2VideoPredictor  # Importë§Œ ë˜ì–´ ìˆìŒ

self.sam2_video_predictor = None  # ì´ˆê¸°í™”ë§Œ, ì‹¤ì œ ì‚¬ìš© ì•ˆ í•¨
```

**í˜„í™©**:
- âœ… Video Predictor import ì™„ë£Œ
- âŒ ì‹¤ì œ ì´ˆê¸°í™” ë° ì‚¬ìš© ì½”ë“œ ì—†ìŒ
- âŒ Memory-based tracking ë¯¸í™œìš©

### 4.2 í†µí•© ì œì•ˆ: Video Predictor í™œìš©

#### 4.2.1 ìƒˆë¡œìš´ í´ë˜ìŠ¤: VideoAnnotator

```python
# /home/joon/dev/sam3d_gui/src/video_annotator.py
"""
SAM2 Video Predictor ê¸°ë°˜ íš¨ìœ¨ì  ë¹„ë””ì˜¤ annotation
"""

import cv2
import numpy as np
from pathlib import Path
import torch
import sys
from typing import Optional, Tuple, List

sys.path.append(str(Path.home() / 'dev/segment-anything-2'))
from sam2.build_sam import build_sam2_video_predictor
from sam2.sam2_video_predictor import SAM2VideoPredictor


class VideoAnnotator:
    """
    SAM2 Video Predictorë¥¼ í™œìš©í•œ íš¨ìœ¨ì  ë¹„ë””ì˜¤ annotation

    Key Features:
    - í•œ í”„ë ˆì„ annotationìœ¼ë¡œ ì „ì²´ ë¹„ë””ì˜¤ ìë™ ì¶”ì 
    - Memory-based propagation
    - Occlusion ìë™ ì²˜ë¦¬
    """

    SAM_MODELS = {
        'base_plus': {
            'config': 'configs/sam2.1/sam2.1_hiera_b+.yaml',
            'checkpoint': 'checkpoints/sam2.1_hiera_base_plus.pt',
        },
        'large': {
            'config': 'configs/sam2.1/sam2.1_hiera_l.yaml',
            'checkpoint': 'checkpoints/sam2.1_hiera_large.pt',
        }
    }

    def __init__(self, sam2_base_path: Path, device: str = "cuda"):
        self.sam2_base_path = sam2_base_path
        self.device = device if torch.cuda.is_available() else "cpu"

        # Video predictor
        self.predictor: Optional[SAM2VideoPredictor] = None
        self.current_model = None

        # Video state
        self.video_path = None
        self.inference_state = None  # Video predictor ë‚´ë¶€ ìƒíƒœ
        self.total_frames = 0

        # Tracking state
        self.object_ids = []  # ì¶”ì  ì¤‘ì¸ ê°ì²´ ID ëª©ë¡
        self.object_annotations = {}  # {obj_id: {frame_idx: points}}

        # Output
        self.output_dir = Path.home() / "dev/sam3d_gui/outputs/video_annotations"
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def load_model(self, model_name: str = "base_plus") -> str:
        """SAM2 Video Predictor ë¡œë“œ"""
        try:
            model_info = self.SAM_MODELS[model_name]
            config_path = self.sam2_base_path / model_info['config']
            checkpoint_path = self.sam2_base_path / model_info['checkpoint']

            if not checkpoint_path.exists():
                return f"Checkpoint not found: {checkpoint_path}"

            # Build video predictor
            self.predictor = build_sam2_video_predictor(
                config_file=str(config_path),
                ckpt_path=str(checkpoint_path),
                device=self.device
            )
            self.current_model = model_name

            return f"âœ“ Loaded SAM2 Video Predictor: {model_name} on {self.device}"

        except Exception as e:
            return f"âœ— Failed to load model: {str(e)}"

    def init_video(self, video_path: str) -> Tuple[bool, str, int]:
        """
        ë¹„ë””ì˜¤ ì´ˆê¸°í™” ë° inference state ìƒì„±

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

        Returns:
            (success, message, total_frames)
        """
        if self.predictor is None:
            return False, "Load model first", 0

        try:
            self.video_path = Path(video_path)

            if not self.video_path.exists():
                return False, f"Video not found: {video_path}", 0

            # Video info
            cap = cv2.VideoCapture(str(self.video_path))
            self.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.release()

            # Initialize inference state (í•µì‹¬!)
            # ì´ ë‹¨ê³„ì—ì„œ ì „ì²´ ë¹„ë””ì˜¤ì˜ ë©”ëª¨ë¦¬ êµ¬ì¡° ì´ˆê¸°í™”
            self.inference_state = self.predictor.init_state(
                video_path=str(self.video_path)
            )

            # Reset tracking
            self.object_ids = []
            self.object_annotations = {}

            msg = f"âœ“ Video initialized: {self.video_path.name} ({self.total_frames} frames)"
            return True, msg, self.total_frames

        except Exception as e:
            return False, f"âœ— Error: {str(e)}", 0

    def add_object_annotation(
        self,
        frame_idx: int,
        points: List[Tuple[int, int]],
        labels: List[int],
        object_id: Optional[int] = None
    ) -> Tuple[bool, str, int]:
        """
        íŠ¹ì • í”„ë ˆì„ì— ê°ì²´ annotation ì¶”ê°€

        Args:
            frame_idx: Annotationí•  í”„ë ˆì„ ì¸ë±ìŠ¤
            points: [(x, y), ...] í¬ì¸íŠ¸ ì¢Œí‘œ
            labels: [1, 1, 0, ...] (1=foreground, 0=background)
            object_id: ê°ì²´ ID (Noneì´ë©´ ìë™ ìƒì„±)

        Returns:
            (success, message, assigned_object_id)
        """
        if self.inference_state is None:
            return False, "Initialize video first", -1

        try:
            # Object ID ìƒì„±
            if object_id is None:
                object_id = len(self.object_ids)
                self.object_ids.append(object_id)

            # SAM2 Video Predictorì— annotation ì¶”ê°€
            point_coords = np.array(points, dtype=np.float32)
            point_labels = np.array(labels, dtype=np.int32)

            # í•µì‹¬ API: add_new_points_or_box
            _, out_obj_ids, out_mask_logits = self.predictor.add_new_points_or_box(
                inference_state=self.inference_state,
                frame_idx=frame_idx,
                obj_id=object_id,
                points=point_coords,
                labels=point_labels
            )

            # Annotation ê¸°ë¡
            if object_id not in self.object_annotations:
                self.object_annotations[object_id] = {}

            self.object_annotations[object_id][frame_idx] = {
                'points': points,
                'labels': labels
            }

            msg = f"âœ“ Added annotation for object {object_id} at frame {frame_idx}"
            return True, msg, object_id

        except Exception as e:
            return False, f"âœ— Error: {str(e)}", -1

    def propagate_in_video(self) -> Tuple[bool, str, dict]:
        """
        ì „ì²´ ë¹„ë””ì˜¤ì— ëŒ€í•´ ìë™ propagation ì‹¤í–‰

        í•œ ë²ˆì˜ í˜¸ì¶œë¡œ ëª¨ë“  í”„ë ˆì„ì˜ ë§ˆìŠ¤í¬ ìƒì„±!

        Returns:
            (success, message, results)
            results = {
                frame_idx: {
                    obj_id: mask (H, W) numpy array
                }
            }
        """
        if self.inference_state is None:
            return False, "Initialize video first", {}

        if len(self.object_ids) == 0:
            return False, "Add at least one object annotation", {}

        try:
            results = {}

            # í•µì‹¬ API: propagate_in_video
            # ëª¨ë“  í”„ë ˆì„ì— ëŒ€í•´ ìë™ìœ¼ë¡œ ë§ˆìŠ¤í¬ ìƒì„±
            for frame_idx, obj_ids, mask_logits in self.predictor.propagate_in_video(
                self.inference_state
            ):
                # ê° í”„ë ˆì„ì˜ ê²°ê³¼ ì €ì¥
                results[frame_idx] = {}

                for obj_id, logit in zip(obj_ids, mask_logits):
                    # Logitì„ binary maskë¡œ ë³€í™˜
                    mask = (logit > 0.0).cpu().numpy().squeeze()
                    results[frame_idx][obj_id] = mask

            msg = f"âœ“ Propagated {len(results)} frames, {len(self.object_ids)} objects"
            return True, msg, results

        except Exception as e:
            return False, f"âœ— Error: {str(e)}", {}

    def get_frame_mask(
        self,
        frame_idx: int,
        object_id: Optional[int] = None
    ) -> Tuple[Optional[np.ndarray], str]:
        """
        íŠ¹ì • í”„ë ˆì„ì˜ ë§ˆìŠ¤í¬ ê°€ì ¸ì˜¤ê¸° (on-demand)

        Args:
            frame_idx: í”„ë ˆì„ ì¸ë±ìŠ¤
            object_id: íŠ¹ì • ê°ì²´ (Noneì´ë©´ ëª¨ë“  ê°ì²´ í•©ì„±)

        Returns:
            (mask, message)
        """
        if self.inference_state is None:
            return None, "Initialize video first"

        try:
            # íŠ¹ì • í”„ë ˆì„ë§Œ ì²˜ë¦¬ (propagate_in_videoì˜ ê²½ëŸ‰ ë²„ì „)
            for out_frame_idx, out_obj_ids, out_mask_logits in self.predictor.propagate_in_video(
                self.inference_state,
                start_frame_idx=frame_idx,
                max_frame_num_to_track=1  # í•œ í”„ë ˆì„ë§Œ
            ):
                if out_frame_idx == frame_idx:
                    if object_id is not None:
                        # íŠ¹ì • ê°ì²´ë§Œ
                        idx = out_obj_ids.index(object_id)
                        logit = out_mask_logits[idx]
                        mask = (logit > 0.0).cpu().numpy().squeeze()
                    else:
                        # ëª¨ë“  ê°ì²´ í•©ì„±
                        masks = [(logit > 0.0).cpu().numpy().squeeze()
                                for logit in out_mask_logits]
                        mask = np.logical_or.reduce(masks)

                    return mask, "Success"

            return None, "Frame not found"

        except Exception as e:
            return None, f"âœ— Error: {str(e)}"

    def refine_annotation(
        self,
        frame_idx: int,
        object_id: int,
        additional_points: List[Tuple[int, int]],
        additional_labels: List[int]
    ) -> Tuple[bool, str]:
        """
        íŠ¹ì • í”„ë ˆì„ì˜ annotation ìˆ˜ì • (interactive refinement)

        Args:
            frame_idx: ìˆ˜ì •í•  í”„ë ˆì„
            object_id: ê°ì²´ ID
            additional_points: ì¶”ê°€ í¬ì¸íŠ¸
            additional_labels: ì¶”ê°€ ë ˆì´ë¸”
        """
        return self.add_object_annotation(
            frame_idx=frame_idx,
            points=additional_points,
            labels=additional_labels,
            object_id=object_id
        )

    def save_results(
        self,
        results: dict,
        format: str = "png"
    ) -> str:
        """
        ê²°ê³¼ ì €ì¥

        Args:
            results: propagate_in_video()ì˜ ê²°ê³¼
            format: 'png' or 'npy'
        """
        try:
            video_name = self.video_path.stem
            output_subdir = self.output_dir / video_name
            output_subdir.mkdir(exist_ok=True)

            for frame_idx, objects in results.items():
                for obj_id, mask in objects.items():
                    if format == "png":
                        mask_uint8 = (mask * 255).astype(np.uint8)
                        filename = f"frame_{frame_idx:04d}_obj_{obj_id}.png"
                        cv2.imwrite(str(output_subdir / filename), mask_uint8)
                    elif format == "npy":
                        filename = f"frame_{frame_idx:04d}_obj_{obj_id}.npy"
                        np.save(str(output_subdir / filename), mask)

            msg = f"âœ“ Saved {len(results)} frames to {output_subdir}"
            return msg

        except Exception as e:
            return f"âœ— Error saving: {str(e)}"


# Usage Example
if __name__ == "__main__":
    # Initialize
    annotator = VideoAnnotator(
        sam2_base_path=Path.home() / "dev/segment-anything-2",
        device="cuda"
    )

    # Load model
    print(annotator.load_model("base_plus"))

    # Init video
    success, msg, total = annotator.init_video(
        "/home/joon/dev/data/markerless_mouse/mouse_1/Camera1/0.mp4"
    )
    print(msg)

    # Annotate one frame only!
    success, msg, obj_id = annotator.add_object_annotation(
        frame_idx=0,
        points=[(500, 400)],  # ë§ˆìš°ìŠ¤ í´ë¦­ ìœ„ì¹˜
        labels=[1]  # Foreground
    )
    print(msg)

    # Propagate to all frames automatically
    success, msg, results = annotator.propagate_in_video()
    print(msg)

    # Save all masks
    print(annotator.save_results(results, format="png"))

    # Check specific frame
    mask, msg = annotator.get_frame_mask(frame_idx=100, object_id=obj_id)
    print(f"Frame 100 mask shape: {mask.shape if mask is not None else 'None'}")
```

#### 4.2.2 Gradio UI í†µí•©

```python
# /home/joon/dev/sam3d_gui/src/web_app.pyì— ì¶”ê°€

def create_video_annotation_tab():
    """
    Tab 4: Video Annotation (SAM2 Video Predictor)

    Workflow:
    1. Load video
    2. Annotate first frame (or any frame)
    3. Click "Propagate" â†’ All frames auto-annotated
    4. Review and refine if needed
    5. Export masks or 3D reconstruct
    """
    with gr.Tab("Video Annotation"):
        with gr.Row():
            # Left: Video player
            with gr.Column(scale=2):
                video_input = gr.Video(label="Upload Video")
                frame_slider = gr.Slider(
                    minimum=0,
                    maximum=100,
                    step=1,
                    label="Frame Index"
                )
                frame_display = gr.Image(label="Current Frame")

                with gr.Row():
                    prev_btn = gr.Button("â—€ Prev Frame")
                    next_btn = gr.Button("Next Frame â–¶")

            # Right: Annotation controls
            with gr.Column(scale=1):
                model_select = gr.Radio(
                    choices=["base_plus", "large"],
                    value="base_plus",
                    label="Model"
                )
                load_model_btn = gr.Button("Load Model")

                gr.Markdown("### Annotation")
                point_type = gr.Radio(
                    choices=["Foreground", "Background"],
                    value="Foreground",
                    label="Point Type"
                )

                add_point_btn = gr.Button("Add Point (Click on Image)")

                gr.Markdown("### Propagation")
                propagate_btn = gr.Button("ğŸš€ Propagate to All Frames", variant="primary")
                progress_bar = gr.Progress()

                status_text = gr.Textbox(label="Status", lines=5)

                gr.Markdown("### Export")
                export_format = gr.Radio(
                    choices=["PNG", "NPY"],
                    value="PNG",
                    label="Format"
                )
                export_btn = gr.Button("ğŸ’¾ Export All Masks")

        # Event handlers
        def on_load_model(model_name):
            return video_annotator.load_model(model_name)

        def on_video_upload(video):
            success, msg, total = video_annotator.init_video(video)
            if success:
                return msg, gr.Slider(maximum=total-1)
            return msg, gr.Slider()

        def on_frame_change(frame_idx):
            # Get frame from video
            cap = cv2.VideoCapture(video_annotator.video_path)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            cap.release()

            if ret:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                return frame_rgb
            return None

        def on_add_point(evt: gr.SelectData, point_type):
            x, y = evt.index
            label = 1 if point_type == "Foreground" else 0

            # Add to video predictor
            success, msg, obj_id = video_annotator.add_object_annotation(
                frame_idx=current_frame_idx,
                points=[(x, y)],
                labels=[label]
            )
            return msg

        def on_propagate():
            success, msg, results = video_annotator.propagate_in_video()
            if success:
                # Store results for export
                video_annotator.last_results = results
                return f"{msg}\nâœ“ Ready to export!"
            return msg

        def on_export(format_type):
            if not hasattr(video_annotator, 'last_results'):
                return "Run propagation first"

            format_str = format_type.lower()
            return video_annotator.save_results(
                video_annotator.last_results,
                format=format_str
            )

        # Connect events
        load_model_btn.click(
            fn=on_load_model,
            inputs=[model_select],
            outputs=[status_text]
        )

        video_input.change(
            fn=on_video_upload,
            inputs=[video_input],
            outputs=[status_text, frame_slider]
        )

        frame_slider.change(
            fn=on_frame_change,
            inputs=[frame_slider],
            outputs=[frame_display]
        )

        frame_display.select(
            fn=on_add_point,
            inputs=[point_type],
            outputs=[status_text]
        )

        propagate_btn.click(
            fn=on_propagate,
            outputs=[status_text]
        )

        export_btn.click(
            fn=on_export,
            inputs=[export_format],
            outputs=[status_text]
        )
```

### 4.3 Best Practices ì œì•ˆ

#### 4.3.1 ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

| ì‹œë‚˜ë¦¬ì˜¤ | ì¶”ì²œ ëª¨ë¸ | ì´ìœ  |
|---------|----------|------|
| **ë‹¨ì¼ ì´ë¯¸ì§€ segmentation** | SAM2ImagePredictor (large) | ìµœê³  í’ˆì§ˆ, í”„ë ˆì„ ê°„ ê´€ê³„ ë¶ˆí•„ìš” |
| **ì´ë¯¸ì§€ ë°°ì¹˜ ì²˜ë¦¬** | SAM2ImagePredictor (base_plus) | ì†ë„ì™€ í’ˆì§ˆ ê· í˜• |
| **ë¹„ë””ì˜¤ ê°ì²´ ì¶”ì ** | SAM2VideoPredictor (base_plus) | Memory ê¸°ë°˜ íš¨ìœ¨ì„± |
| **ê³ í•´ìƒë„ ë¹„ë””ì˜¤** | SAM2VideoPredictor (large) | í’ˆì§ˆ ìš°ì„  |
| **ì‹¤ì‹œê°„ ì²˜ë¦¬** | SAM2ImagePredictor (tiny) | ìµœëŒ€ ì†ë„ |

#### 4.3.2 í†µí•© ì „ëµ

**Unified API ì„¤ê³„**:

```python
# /home/joon/dev/sam3d_gui/src/sam_unified.py
"""
SAM2 í†µí•© API: Imageì™€ Video Predictor ìë™ ì„ íƒ
"""

class SAMUnified:
    """
    ìë™ìœ¼ë¡œ ì ì ˆí•œ predictor ì„ íƒ
    """

    def __init__(self, sam2_base_path, device="auto"):
        self.image_predictor = None
        self.video_predictor = None
        self.mode = None  # 'image' or 'video'

    def set_mode(self, mode: str):
        """
        ëª¨ë“œ ì„¤ì •

        Args:
            mode: 'image' (ë…ë¦½ í”„ë ˆì„) or 'video' (ì‹œê°„ì  ì¶”ì )
        """
        if mode == "image":
            # Image predictor ì‚¬ìš©
            if self.image_predictor is None:
                self.image_predictor = build_sam2(...)
            self.mode = "image"

        elif mode == "video":
            # Video predictor ì‚¬ìš©
            if self.video_predictor is None:
                self.video_predictor = build_sam2_video_predictor(...)
            self.mode = "video"

    def process(self, input_data, **kwargs):
        """
        ì…ë ¥ì— ë”°ë¼ ìë™ ì²˜ë¦¬
        """
        if self.mode == "image":
            # ë…ë¦½ì  ì´ë¯¸ì§€ ì²˜ë¦¬
            return self._process_image(input_data, **kwargs)

        elif self.mode == "video":
            # ë¹„ë””ì˜¤ ì¶”ì 
            return self._process_video(input_data, **kwargs)


# Usage
sam = SAMUnified(sam2_base_path=Path.home() / "dev/segment-anything-2")

# Scenario 1: ë…ë¦½ í”„ë ˆì„ ì²˜ë¦¬
sam.set_mode("image")
for frame in frames:
    mask = sam.process(frame, points=[(x, y)], labels=[1])

# Scenario 2: ë¹„ë””ì˜¤ ì¶”ì 
sam.set_mode("video")
sam.process(video_path, initial_frame=0, points=[(x, y)], labels=[1])
# â†’ ëª¨ë“  í”„ë ˆì„ ìë™ ì¶”ì 
```

#### 4.3.3 ì„±ëŠ¥ ìµœì í™”

**1. torch.compile() í™œìš©** [8]

```python
# SAM2 Video Predictor ì„±ëŠ¥ í–¥ìƒ
video_predictor = build_sam2_video_predictor(
    ...,
    vos_optimized=True  # torch.compile í™œì„±í™”
)
# â†’ Major speedup for video inference
```

**2. Multi-object ì¶”ì  íš¨ìœ¨í™”** [9]

```python
# âŒ ë¹„íš¨ìœ¨: ê° ê°ì²´ë¥¼ ë”°ë¡œ ì²˜ë¦¬
for obj_id in [0, 1, 2]:
    predictor.add_new_points(obj_id=obj_id, ...)
    results = predictor.propagate_in_video()

# âœ… íš¨ìœ¨: í•œ ë²ˆì— ì—¬ëŸ¬ ê°ì²´ ì¶”ì  (ì´ë¯¸ì§€ íŠ¹ì§• ê³µìœ )
predictor.add_new_points(obj_id=0, ...)
predictor.add_new_points(obj_id=1, ...)
predictor.add_new_points(obj_id=2, ...)
results = predictor.propagate_in_video()  # ëª¨ë“  ê°ì²´ í•œ ë²ˆì—
```

**3. í”„ë ˆì„ ìŠ¤íŠ¸ë¼ì´ë“œ ì ìš©**

```python
# ê¸´ ë¹„ë””ì˜¤ëŠ” strideë¡œ ìƒ˜í”Œë§
def process_long_video(video_path, stride=5):
    """
    ê¸´ ë¹„ë””ì˜¤ë¥¼ strideë¡œ ìƒ˜í”Œë§í•˜ì—¬ ì²˜ë¦¬

    Args:
        stride: N í”„ë ˆì„ë§ˆë‹¤ í•˜ë‚˜ì”© ì²˜ë¦¬
    """
    # Annotation on frame 0
    predictor.add_new_points(frame_idx=0, ...)

    # Propagate only on sampled frames
    for frame_idx in range(0, total_frames, stride):
        mask = predictor.get_frame_mask(frame_idx)
        # Process mask
```

---

## 5. ì‹¤ìš©ì  ì œì•ˆ ë° êµ¬í˜„ ë¡œë“œë§µ

### 5.1 ë‹¨ê³„ë³„ í†µí•© ê³„íš

#### Phase 1: Video Predictor í†µí•© (2-3ì¼)

**ëª©í‘œ**: VideoAnnotator í´ë˜ìŠ¤ êµ¬í˜„ ë° ê¸°ë³¸ ë™ì‘ ê²€ì¦

**ì‘ì—…**:
1. âœ… `src/video_annotator.py` ìƒì„±
2. âœ… SAM2VideoPredictor ì´ˆê¸°í™”
3. âœ… ê¸°ë³¸ annotation ë° propagation API
4. âœ… ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±

**ê²€ì¦**:
```python
# Test script
annotator = VideoAnnotator(...)
annotator.load_model("base_plus")
annotator.init_video("test_video.mp4")
annotator.add_object_annotation(frame_idx=0, points=[(x, y)], labels=[1])
results = annotator.propagate_in_video()
# â†’ ëª¨ë“  í”„ë ˆì„ì— ë§ˆìŠ¤í¬ ìƒì„± í™•ì¸
```

#### Phase 2: Gradio UI í†µí•© (2-3ì¼)

**ëª©í‘œ**: Web UIì— Video Annotation íƒ­ ì¶”ê°€

**ì‘ì—…**:
1. âœ… Tab 4 ì¶”ê°€: "Video Annotation"
2. âœ… ë¹„ë””ì˜¤ ì—…ë¡œë“œ ë° í”„ë ˆì„ ë„¤ë¹„ê²Œì´ì…˜
3. âœ… Interactive annotation (í´ë¦­ìœ¼ë¡œ í¬ì¸íŠ¸ ì¶”ê°€)
4. âœ… Propagate ë²„íŠ¼ ë° progress bar
5. âœ… ê²°ê³¼ export (PNG, NPY)

**UI Mockup**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tab 4: Video Annotation (SAM2 Video Predictor)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Video Player     â”‚ Annotation Controls              â”‚
â”‚                  â”‚                                  â”‚
â”‚ [Video Canvas]   â”‚ Model: â—‹ base_plus â—‹ large      â”‚
â”‚                  â”‚ [Load Model]                     â”‚
â”‚ Frame: 0/1000    â”‚                                  â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” â”‚ Point Type: â—‹ FG â—‹ BG          â”‚
â”‚ [â—€ Prev] [Nextâ–¶] â”‚ [Add Point (Click on Video)]     â”‚
â”‚                  â”‚                                  â”‚
â”‚                  â”‚ [ğŸš€ Propagate to All Frames]     â”‚
â”‚                  â”‚                                  â”‚
â”‚                  â”‚ Status: Ready                    â”‚
â”‚                  â”‚                                  â”‚
â”‚                  â”‚ Export: â—‹ PNG â—‹ NPY             â”‚
â”‚                  â”‚ [ğŸ’¾ Export All Masks]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Phase 3: SAM3D í†µí•© (1-2ì¼)

**ëª©í‘œ**: Propagated masksë¥¼ SAM3Dë¡œ 3D ì¬êµ¬ì„±

**ì‘ì—…**:
1. âœ… Video Annotation ê²°ê³¼ë¥¼ SAM3D Processorë¡œ ì „ë‹¬
2. âœ… ì„ íƒëœ í”„ë ˆì„ì˜ ë§ˆìŠ¤í¬ë¡œ 3D ì¬êµ¬ì„±
3. âœ… Batch 3D reconstruction (ì—¬ëŸ¬ í”„ë ˆì„)

**Workflow**:
```
Video â†’ VideoAnnotator â†’ Propagate â†’ All Masks
                              â†“
                    Select Frame(s)
                              â†“
                    SAM3DProcessor.reconstruct_3d()
                              â†“
                    PLY/OBJ export
```

#### Phase 4: ì„±ëŠ¥ ìµœì í™” ë° ê³ ê¸‰ ê¸°ëŠ¥ (2-3ì¼)

**ì‘ì—…**:
1. âœ… torch.compile() ì ìš©
2. âœ… Multi-object tracking
3. âœ… Interactive refinement (annotation ìˆ˜ì •)
4. âœ… Occlusion ê°ì§€ ë° ì‹œê°í™”
5. âœ… ê²°ê³¼ í’ˆì§ˆ ë©”íŠ¸ë¦­ (confidence, IoU)

### 5.2 Migration Path (ê¸°ì¡´ ì½”ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜)

#### í˜„ì¬ LiteAnnotator ì‚¬ìš©ìë¥¼ ìœ„í•œ ì „í™˜ ê°€ì´ë“œ

**Before (LiteAnnotator - ëª¨ë“  í”„ë ˆì„ annotation í•„ìš”)**:
```python
lite = LiteAnnotator(sam2_base_path, device="cuda")
lite.load_model("large")
lite.change_input_source(video_path, 'video')

# ê° í”„ë ˆì„ë§ˆë‹¤ annotation í•„ìš”
for frame_idx in range(total_frames):
    lite.load_frame(frame_idx)
    lite.add_point(x, y, 'foreground')
    frame_vis, mask, msg = lite.generate_mask()
    lite.save_annotation()
```

**After (VideoAnnotator - í•œ í”„ë ˆì„ë§Œ annotation)**:
```python
video = VideoAnnotator(sam2_base_path, device="cuda")
video.load_model("base_plus")
video.init_video(video_path)

# ì²« í”„ë ˆì„ë§Œ annotation
video.add_object_annotation(
    frame_idx=0,
    points=[(x, y)],
    labels=[1]
)

# ë‚˜ë¨¸ì§€ ëª¨ë“  í”„ë ˆì„ ìë™ ì¶”ì 
success, msg, results = video.propagate_in_video()
video.save_results(results)
```

**ì‹œê°„ ì ˆì•½**:
- Before: 1000 í”„ë ˆì„ Ã— 30ì´ˆ = ~8ì‹œê°„
- After: ì²« í”„ë ˆì„ 30ì´ˆ + propagate 2ë¶„ = **~2.5ë¶„** (99.5% ì‹œê°„ ì ˆì•½!)

### 5.3 ì½”ë“œ ê°œì„  ë°©í–¥

#### 5.3.1 í˜„ì¬ sam3d_processor.py ê°œì„ 

**Issue**: ë¹„íš¨ìœ¨ì ì¸ í”„ë ˆì„ë³„ ë…ë¦½ ì²˜ë¦¬

```python
# í˜„ì¬ ì½”ë“œ (sam3d_processor.py)
def track_object_across_frames(self, frames, initial_bbox=None, ...):
    """ê° í”„ë ˆì„ì„ ë…ë¦½ì ìœ¼ë¡œ segmentation"""
    for idx, frame in enumerate(frames):
        # ë§¤ë²ˆ ìƒˆë¡œ segmentation (ë¹„íš¨ìœ¨!)
        mask = self.segment_object_interactive(frame, bbox=initial_bbox, method='grabcut')
        # ...
```

**ê°œì„ ì•ˆ**:

```python
# ê°œì„ ëœ sam3d_processor.py
def track_object_across_frames(self, frames, initial_bbox=None, use_video_predictor=True, ...):
    """
    SAM2 Video Predictorë¥¼ í™œìš©í•œ íš¨ìœ¨ì  ì¶”ì 

    Args:
        use_video_predictor: Trueë©´ Video Predictor ì‚¬ìš©, Falseë©´ ê¸°ì¡´ ë°©ì‹
    """
    if use_video_predictor and self.sam2_video_predictor:
        # Video Predictorë¡œ íš¨ìœ¨ì  ì¶”ì 
        return self._track_with_video_predictor(frames, initial_bbox, ...)
    else:
        # ê¸°ì¡´ ë°©ì‹ (fallback)
        return self._track_with_image_predictor(frames, initial_bbox, ...)

def _track_with_video_predictor(self, frames, initial_bbox, ...):
    """SAM2 Video Predictor ì‚¬ìš©"""
    # 1. ì„ì‹œ ë¹„ë””ì˜¤ íŒŒì¼ ìƒì„±
    temp_video = self._frames_to_video(frames)

    # 2. Video Predictor ì´ˆê¸°í™”
    inference_state = self.sam2_video_predictor.init_state(temp_video)

    # 3. ì²« í”„ë ˆì„ annotation (bbox â†’ points ë³€í™˜)
    center_x, center_y = initial_bbox[0] + initial_bbox[2]//2, initial_bbox[1] + initial_bbox[3]//2
    self.sam2_video_predictor.add_new_points_or_box(
        inference_state=inference_state,
        frame_idx=0,
        obj_id=0,
        box=initial_bbox  # bbox ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥!
    )

    # 4. ëª¨ë“  í”„ë ˆì„ ìë™ ì¶”ì 
    results = {}
    for frame_idx, obj_ids, mask_logits in self.sam2_video_predictor.propagate_in_video(inference_state):
        mask = (mask_logits[0] > 0.0).cpu().numpy().squeeze()
        results[frame_idx] = mask

    # 5. TrackingResult ìƒì„±
    segments = []
    for idx, mask in results.items():
        segment_info = SegmentInfo(
            frame_idx=idx,
            mask=mask,
            bbox=self._mask_to_bbox(mask),
            center=self._mask_to_center(mask),
            area=mask.sum()
        )
        segments.append(segment_info)

    return TrackingResult(
        start_frame=0,
        end_frame=len(frames)-1,
        segments=segments,
        motion_detected=self._detect_motion(segments, motion_threshold),
        duration_seconds=len(frames) / fps
    )
```

#### 5.3.2 Unified Segmentation API

```python
# /home/joon/dev/sam3d_gui/src/segmentation_factory.py
"""
SAM ê¸°ë°˜ segmentation í†µí•© íŒ©í† ë¦¬
"""

from enum import Enum
from typing import Union, List
import numpy as np

class SegmentationMode(Enum):
    IMAGE = "image"  # ë…ë¦½ í”„ë ˆì„
    VIDEO = "video"  # ì‹œê°„ì  ì¶”ì 
    AUTO = "auto"    # ìë™ ì„ íƒ


class SegmentationFactory:
    """
    í†µí•© segmentation íŒ©í† ë¦¬

    ì…ë ¥ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì ì ˆí•œ predictor ì„ íƒ
    """

    def __init__(self, sam2_base_path, device="auto"):
        self.lite_annotator = None  # Image predictor
        self.video_annotator = None  # Video predictor

    def segment(
        self,
        input_data: Union[np.ndarray, str, List[np.ndarray]],
        points: List[tuple],
        labels: List[int],
        mode: SegmentationMode = SegmentationMode.AUTO
    ):
        """
        í†µí•© segmentation API

        Args:
            input_data:
                - np.ndarray: ë‹¨ì¼ ì´ë¯¸ì§€ (H, W, 3)
                - str: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
                - List[np.ndarray]: í”„ë ˆì„ ì‹œí€€ìŠ¤
            points: [(x, y), ...]
            labels: [1, 0, ...]
            mode: ì²˜ë¦¬ ëª¨ë“œ

        Returns:
            ë§ˆìŠ¤í¬ ë˜ëŠ” ë§ˆìŠ¤í¬ ì‹œí€€ìŠ¤
        """
        # ëª¨ë“œ ìë™ ê²°ì •
        if mode == SegmentationMode.AUTO:
            if isinstance(input_data, np.ndarray):
                mode = SegmentationMode.IMAGE
            elif isinstance(input_data, (str, list)):
                mode = SegmentationMode.VIDEO

        # ì²˜ë¦¬
        if mode == SegmentationMode.IMAGE:
            return self._segment_image(input_data, points, labels)
        elif mode == SegmentationMode.VIDEO:
            return self._segment_video(input_data, points, labels)

    def _segment_image(self, image, points, labels):
        """ë‹¨ì¼ ì´ë¯¸ì§€ segmentation"""
        if self.lite_annotator is None:
            self.lite_annotator = LiteAnnotator(...)

        # Image predictor ì‚¬ìš©
        self.lite_annotator.predictor.set_image(image)
        masks, scores, _ = self.lite_annotator.predictor.predict(
            point_coords=np.array(points),
            point_labels=np.array(labels)
        )
        return masks[0]  # Best mask

    def _segment_video(self, video_data, points, labels):
        """ë¹„ë””ì˜¤ segmentation"""
        if self.video_annotator is None:
            self.video_annotator = VideoAnnotator(...)

        # Video predictor ì‚¬ìš©
        if isinstance(video_data, str):
            # ë¹„ë””ì˜¤ íŒŒì¼
            self.video_annotator.init_video(video_data)
        else:
            # í”„ë ˆì„ ì‹œí€€ìŠ¤
            temp_video = self._frames_to_video(video_data)
            self.video_annotator.init_video(temp_video)

        # ì²« í”„ë ˆì„ annotation
        self.video_annotator.add_object_annotation(0, points, labels)

        # Propagate
        _, _, results = self.video_annotator.propagate_in_video()
        return results


# Usage
factory = SegmentationFactory(sam2_base_path)

# Scenario 1: ë‹¨ì¼ ì´ë¯¸ì§€
mask = factory.segment(
    input_data=image,  # np.ndarray
    points=[(500, 400)],
    labels=[1]
)

# Scenario 2: ë¹„ë””ì˜¤
masks = factory.segment(
    input_data="video.mp4",  # ìë™ìœ¼ë¡œ Video Predictor ì‚¬ìš©
    points=[(500, 400)],
    labels=[1]
)
```

### 5.4 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

#### ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

**í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤**: 1000 í”„ë ˆì„ ë¹„ë””ì˜¤, í•œ ê°ì²´ ì¶”ì 

| ë°©ë²• | ì‹œê°„ | Annotation íšŸìˆ˜ | ë©”ëª¨ë¦¬ |
|------|------|----------------|--------|
| **LiteAnnotator (í˜„ì¬)** | ~8ì‹œê°„ | 1000ë²ˆ | ë‚®ìŒ |
| **VideoAnnotator (ì œì•ˆ)** | ~2.5ë¶„ | 1ë²ˆ | ì¤‘ê°„ |
| **ê°œì„ ìœ¨** | **99.5% ê°ì†Œ** | **99.9% ê°ì†Œ** | +20% |

**ì„¸ë¶€ ë¶„ì„**:
```
LiteAnnotator (Image Predictor):
- Frame 0: 30s (manual annotation + inference)
- Frame 1: 30s (manual annotation + inference)
- ...
- Frame 999: 30s
Total: 1000 Ã— 30s = 30,000s â‰ˆ 8.3 hours

VideoAnnotator (Video Predictor):
- Frame 0: 30s (manual annotation + inference)
- Propagate all: 120s (automatic inference on 1000 frames)
Total: 30s + 120s = 150s â‰ˆ 2.5 minutes
```

---

## 6. SAM3D í™œìš© ì „ëµ

### 6.1 SAM3D Objects ìµœì  ì‚¬ìš©ë²•

#### 6.1.1 ì…ë ¥ í’ˆì§ˆ ìµœì í™”

**Best Practices**:

1. **High-quality 2D mask**:
   - SAM2 Video Predictorë¡œ ì¼ê´€ëœ ë§ˆìŠ¤í¬ ìƒì„±
   - Temporal consistency í™•ë³´

2. **ì ì ˆí•œ í”„ë ˆì„ ì„ íƒ**:
   - ê°ì²´ê°€ ëª…í™•íˆ ë³´ì´ëŠ” í”„ë ˆì„
   - Occlusion ìµœì†Œí™”
   - ì •ë©´ ë˜ëŠ” ì¸¡ë©´ view

3. **Multi-view reconstruction** (ì„ íƒ):
   - ì—¬ëŸ¬ í”„ë ˆì„ì˜ 3D ì¬êµ¬ì„± ê²°í•©
   - ë” ì™„ì „í•œ 3D ëª¨ë¸

```python
# Multi-view 3D reconstruction
def reconstruct_3d_multiview(video_results, sam3d_processor):
    """
    ì—¬ëŸ¬ í”„ë ˆì„ì˜ 3D ì¬êµ¬ì„±ì„ ê²°í•©

    Args:
        video_results: VideoAnnotator.propagate_in_video() ê²°ê³¼
        sam3d_processor: SAM3DProcessor ì¸ìŠ¤í„´ìŠ¤
    """
    # 1. ì£¼ìš” í”„ë ˆì„ ì„ íƒ (ê· ë“± ìƒ˜í”Œë§)
    key_frames = [0, 250, 500, 750, 999]

    # 2. ê° í”„ë ˆì„ì—ì„œ 3D ì¬êµ¬ì„±
    reconstructions = []
    for frame_idx in key_frames:
        frame = get_frame(video, frame_idx)
        mask = video_results[frame_idx][obj_id]

        recon = sam3d_processor.reconstruct_3d(frame, mask)
        reconstructions.append(recon)

    # 3. 3D ëª¨ë¸ ê²°í•© (alignment + fusion)
    final_3d = merge_3d_reconstructions(reconstructions)

    return final_3d
```

#### 6.1.2 ì¶œë ¥ í¬ë§· ì„ íƒ

| í¬ë§· | ìš©ë„ | ì¥ì  | ë‹¨ì  |
|------|------|------|------|
| **PLY** | Gaussian Splatting, point cloud | ê³ í’ˆì§ˆ, ë¹ ë¥¸ ë Œë”ë§ | íŒŒì¼ í¬ê¸° í° í¸ |
| **OBJ** | 3D í¸ì§‘ (Blender, Maya) | í…ìŠ¤ì²˜ + UV, ë²”ìš©ì„± | Mesh ë³€í™˜ í•„ìš” |
| **GLB** | ê²Œì„ ì—”ì§„ (Unity, Unreal) | ìµœì í™”, ì• ë‹ˆë©”ì´ì…˜ ì§€ì› | ë³µì¡í•œ ì„¤ì • |

**ê¶Œì¥ ì›Œí¬í”Œë¡œìš°**:
```
SAM3D â†’ PLY (primary output)
  â†“
Convert â†’ OBJ (for editing)
  â†“
Convert â†’ GLB (for game engine)
```

### 6.2 SAM2 + SAM3D í†µí•© íŒŒì´í”„ë¼ì¸

```python
# /home/joon/dev/sam3d_gui/src/pipeline_integrated.py
"""
SAM2 Video Predictor + SAM3D Objects í†µí•© íŒŒì´í”„ë¼ì¸
"""

class IntegratedPipeline:
    """
    End-to-end íŒŒì´í”„ë¼ì¸: ë¹„ë””ì˜¤ â†’ 3D ì¬êµ¬ì„±

    Workflow:
    1. SAM2 Video Predictorë¡œ ëª¨ë“  í”„ë ˆì„ segmentation
    2. ì£¼ìš” í”„ë ˆì„ ì„ íƒ
    3. SAM3D Objectsë¡œ 3D ì¬êµ¬ì„±
    4. Multi-view fusion (ì„ íƒ)
    """

    def __init__(self, sam2_base_path, sam3d_checkpoint):
        self.video_annotator = VideoAnnotator(sam2_base_path)
        self.sam3d_processor = SAM3DProcessor(sam3d_checkpoint)

    def process_video_to_3d(
        self,
        video_path: str,
        annotation_frame: int = 0,
        annotation_points: List[tuple] = None,
        annotation_labels: List[int] = None,
        reconstruction_frames: List[int] = None,
        multiview: bool = True
    ):
        """
        ë¹„ë””ì˜¤ì—ì„œ 3D ëª¨ë¸ ìƒì„±

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼
            annotation_frame: Annotationí•  í”„ë ˆì„ (ë³´í†µ 0)
            annotation_points: [(x, y), ...]
            annotation_labels: [1, 0, ...]
            reconstruction_frames: 3D ì¬êµ¬ì„±í•  í”„ë ˆì„ë“¤ (Noneì´ë©´ ìë™ ì„ íƒ)
            multiview: ì—¬ëŸ¬ í”„ë ˆì„ ê²°í•© ì—¬ë¶€

        Returns:
            3D ì¬êµ¬ì„± ê²°ê³¼
        """
        # Step 1: Video segmentation
        print("Step 1: SAM2 Video Predictor - Segmentation")
        self.video_annotator.load_model("base_plus")
        self.video_annotator.init_video(video_path)

        # Annotation
        self.video_annotator.add_object_annotation(
            frame_idx=annotation_frame,
            points=annotation_points,
            labels=annotation_labels
        )

        # Propagate
        success, msg, video_results = self.video_annotator.propagate_in_video()
        print(f"  {msg}")

        # Step 2: ì£¼ìš” í”„ë ˆì„ ì„ íƒ
        if reconstruction_frames is None:
            total = len(video_results)
            if multiview:
                # 5-10ê°œ ê· ë“± ìƒ˜í”Œë§
                reconstruction_frames = [int(i * total / 10) for i in range(10)]
            else:
                # ì¤‘ê°„ í”„ë ˆì„ í•˜ë‚˜ë§Œ
                reconstruction_frames = [total // 2]

        print(f"Step 2: Selected {len(reconstruction_frames)} frames for 3D reconstruction")

        # Step 3: SAM3D 3D ì¬êµ¬ì„±
        print("Step 3: SAM3D Objects - 3D Reconstruction")
        reconstructions = []

        cap = cv2.VideoCapture(video_path)

        for frame_idx in reconstruction_frames:
            # í”„ë ˆì„ ë¡œë“œ
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame_bgr = cap.read()
            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)

            # ë§ˆìŠ¤í¬ ê°€ì ¸ì˜¤ê¸°
            mask = video_results[frame_idx][0]  # obj_id=0

            # 3D ì¬êµ¬ì„±
            print(f"  Reconstructing frame {frame_idx}...")
            recon = self.sam3d_processor.reconstruct_3d(frame_rgb, mask)
            reconstructions.append({
                'frame_idx': frame_idx,
                'reconstruction': recon
            })

        cap.release()

        # Step 4: Multi-view fusion (optional)
        if multiview and len(reconstructions) > 1:
            print("Step 4: Multi-view fusion")
            final_3d = self._fuse_reconstructions(reconstructions)
        else:
            final_3d = reconstructions[0]['reconstruction']

        return final_3d

    def _fuse_reconstructions(self, reconstructions):
        """
        ì—¬ëŸ¬ 3D ì¬êµ¬ì„± ê²°í•©

        TODO: ì‹¤ì œ êµ¬í˜„ í•„ìš”
        - Point cloud alignment (ICP)
        - Mesh fusion
        - Texture blending
        """
        # Placeholder: ì²« ë²ˆì§¸ ì¬êµ¬ì„±ë§Œ ë°˜í™˜
        print("  Multi-view fusion not yet implemented, using first reconstruction")
        return reconstructions[0]['reconstruction']


# Usage Example
if __name__ == "__main__":
    pipeline = IntegratedPipeline(
        sam2_base_path=Path.home() / "dev/segment-anything-2",
        sam3d_checkpoint=Path.home() / "dev/sam-3d-objects/checkpoints/hf"
    )

    # ë¹„ë””ì˜¤ì—ì„œ 3D ëª¨ë¸ ìƒì„±
    result_3d = pipeline.process_video_to_3d(
        video_path="/home/joon/dev/data/markerless_mouse/mouse_1/Camera1/0.mp4",
        annotation_frame=0,
        annotation_points=[(500, 400)],  # ë§ˆìš°ìŠ¤ í´ë¦­ ìœ„ì¹˜
        annotation_labels=[1],
        multiview=True  # ì—¬ëŸ¬ í”„ë ˆì„ ê²°í•©
    )

    # ì €ì¥
    pipeline.sam3d_processor.export_mesh(result_3d, "output_mouse.ply", format="ply")
    print("âœ“ 3D reconstruction complete!")
```

---

## 7. ì‹¤ì „ ì˜ˆì œ

### 7.1 ì˜ˆì œ 1: ë§ˆìš°ìŠ¤ ë¹„ë””ì˜¤ ì¶”ì  ë° 3D ì¬êµ¬ì„±

```python
"""
ë§ˆìš°ìŠ¤ ë¹„ë””ì˜¤ì—ì„œ ê°ì²´ ì¶”ì  ë° 3D ì¬êµ¬ì„±
"""

# Step 1: Video annotation
video_annotator = VideoAnnotator(
    sam2_base_path=Path.home() / "dev/segment-anything-2",
    device="cuda"
)

video_annotator.load_model("base_plus")

video_path = "/home/joon/dev/data/markerless_mouse/mouse_1/Camera1/0.mp4"
video_annotator.init_video(video_path)

# ì²« í”„ë ˆì„ì—ì„œ ë§ˆìš°ìŠ¤ í´ë¦­
video_annotator.add_object_annotation(
    frame_idx=0,
    points=[(500, 400)],  # ë§ˆìš°ìŠ¤ ì¤‘ì‹¬
    labels=[1]
)

# ëª¨ë“  í”„ë ˆì„ ìë™ ì¶”ì 
success, msg, results = video_annotator.propagate_in_video()
print(msg)  # "âœ“ Propagated 3000 frames, 1 objects"

# Step 2: ê²°ê³¼ ì €ì¥
video_annotator.save_results(results, format="png")
# â†’ outputs/video_annotations/0/frame_XXXX_obj_0.png

# Step 3: íŠ¹ì • í”„ë ˆì„ 3D ì¬êµ¬ì„±
sam3d = SAM3DProcessor(
    sam3d_checkpoint_path=Path.home() / "dev/sam-3d-objects/checkpoints/hf"
)

# ì¤‘ê°„ í”„ë ˆì„ ì„ íƒ
frame_idx = 1500
cap = cv2.VideoCapture(video_path)
cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
ret, frame_bgr = cap.read()
frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
cap.release()

# ë§ˆìŠ¤í¬ ê°€ì ¸ì˜¤ê¸°
mask = results[frame_idx][0]  # obj_id=0

# 3D ì¬êµ¬ì„±
reconstruction = sam3d.reconstruct_3d(frame_rgb, mask, seed=42)

# ì €ì¥
sam3d.export_mesh(reconstruction, "mouse_3d.ply", format="ply")
print("âœ“ 3D reconstruction saved: mouse_3d.ply")
```

### 7.2 ì˜ˆì œ 2: Multi-object ì¶”ì 

```python
"""
ì—¬ëŸ¬ ê°ì²´ ë™ì‹œ ì¶”ì 
"""

video_annotator = VideoAnnotator(...)
video_annotator.load_model("large")
video_annotator.init_video("multi_object_video.mp4")

# Object 1: ì²« ë²ˆì§¸ ë§ˆìš°ìŠ¤
video_annotator.add_object_annotation(
    frame_idx=0,
    points=[(300, 200)],
    labels=[1],
    object_id=None  # ìë™ ìƒì„± â†’ obj_id=0
)

# Object 2: ë‘ ë²ˆì§¸ ë§ˆìš°ìŠ¤
video_annotator.add_object_annotation(
    frame_idx=0,
    points=[(700, 400)],
    labels=[1],
    object_id=None  # ìë™ ìƒì„± â†’ obj_id=1
)

# ëª¨ë“  ê°ì²´ ë™ì‹œ ì¶”ì  (íš¨ìœ¨ì !)
success, msg, results = video_annotator.propagate_in_video()

# ê²°ê³¼ í™•ì¸
print(f"Tracked {len(video_annotator.object_ids)} objects")
for frame_idx, objects in results.items():
    for obj_id, mask in objects.items():
        print(f"Frame {frame_idx}, Object {obj_id}: Mask shape {mask.shape}")

# ê° ê°ì²´ë³„ 3D ì¬êµ¬ì„±
for obj_id in video_annotator.object_ids:
    frame_rgb = get_frame(video_path, frame_idx=1000)
    mask = results[1000][obj_id]

    recon = sam3d.reconstruct_3d(frame_rgb, mask)
    sam3d.export_mesh(recon, f"object_{obj_id}_3d.ply")
```

### 7.3 ì˜ˆì œ 3: Interactive refinement

```python
"""
ìë™ ì¶”ì  ê²°ê³¼ë¥¼ interactiveí•˜ê²Œ ê°œì„ 
"""

video_annotator = VideoAnnotator(...)
video_annotator.load_model("base_plus")
video_annotator.init_video(video_path)

# ì´ˆê¸° annotation
video_annotator.add_object_annotation(
    frame_idx=0,
    points=[(500, 400)],
    labels=[1]
)

# ì²« ë²ˆì§¸ propagation
results = video_annotator.propagate_in_video()

# ì¤‘ê°„ì— ë§ˆìŠ¤í¬ í™•ì¸
mask_100 = results[1][100][0]
visualize_mask(mask_100)  # í’ˆì§ˆ í™•ì¸

# í’ˆì§ˆì´ ë‚®ìœ¼ë©´ â†’ Refinement
# Frame 100ì— ì¶”ê°€ í¬ì¸íŠ¸ annotation
video_annotator.refine_annotation(
    frame_idx=100,
    object_id=0,
    additional_points=[(520, 410), (480, 390)],  # Foreground ë³´ê°•
    additional_labels=[1, 1]
)

# ë‹¤ì‹œ propagation (Frame 100 ì´í›„ë§Œ)
results_refined = video_annotator.propagate_in_video()

# ê°œì„ ëœ ê²°ê³¼ í™•ì¸
mask_100_refined = results_refined[1][100][0]
visualize_mask(mask_100_refined)  # í’ˆì§ˆ í–¥ìƒ í™•ì¸
```

---

## 8. êµí›ˆ ë° Best Practices

### 8.1 í•µì‹¬ êµí›ˆ

#### 1. Memory-Based Trackingì˜ ê°•ë ¥í•¨

**ë°œê²¬**:
- í•œ í”„ë ˆì„ annotationë§Œìœ¼ë¡œ ì „ì²´ ë¹„ë””ì˜¤ ì¶”ì  ê°€ëŠ¥
- Temporal consistencyê°€ ìë™ìœ¼ë¡œ ë³´ì¥
- Occlusion ì²˜ë¦¬ ìë™í™”

**ì‹œì‚¬ì **:
- ë¹„ë””ì˜¤ annotation ì‘ì—…ëŸ‰ 99% ê°ì†Œ
- ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥ (~44 FPS)
- ê¸´ ë¹„ë””ì˜¤ì—ì„œë„ íš¨ìœ¨ì 

#### 2. Image vs Video Predictor êµ¬ë¶„ì˜ í•„ìš”ì„±

**ì´ìœ **:
- ì•„í‚¤í…ì²˜ ì°¨ì´ (ë©”ëª¨ë¦¬ ìœ ë¬´)
- ì´ˆê¸°í™” ë°©ë²• ì°¨ì´
- Use case ì°¨ì´

**Best Practice**:
- ë…ë¦½ í”„ë ˆì„ â†’ Image Predictor
- ì‹œê°„ì  ì¶”ì  â†’ Video Predictor
- í†µí•© APIë¡œ ìë™ ì„ íƒ

#### 3. SAM3Dì˜ ì‹¤ìš©ì„±

**ë°œê²¬**:
- ë‹¨ì¼ ì´ë¯¸ì§€ì—ì„œ ê³ í’ˆì§ˆ 3D ì¬êµ¬ì„±
- Real-world ì¡°ê±´ (occlusion, clutter)ì— ê°•ê±´
- ì¸ê°„ ì„ í˜¸ë„ 5:1 ìŠ¹ë¥ 

**í™œìš©**:
- SAM2ë¡œ ì¼ê´€ëœ ë§ˆìŠ¤í¬ â†’ SAM3Dë¡œ 3D ì¬êµ¬ì„±
- Multi-view fusionìœ¼ë¡œ í’ˆì§ˆ í–¥ìƒ
- ê²Œì„ ì—”ì§„, 3D í¸ì§‘ ë„êµ¬ í˜¸í™˜

### 8.2 ê°œë°œ ê°€ì´ë“œë¼ì¸

#### Defensive Programming

```python
# âœ… Good: ì•ˆì „í•œ Video Predictor ì´ˆê¸°í™”
def init_video_safe(video_path):
    if not Path(video_path).exists():
        raise FileNotFoundError(f"Video not found: {video_path}")

    if predictor is None:
        raise RuntimeError("Load model first")

    inference_state = predictor.init_state(video_path)

    if inference_state is None:
        raise RuntimeError("Failed to initialize video state")

    return inference_state

# âŒ Bad: ê²€ì¦ ì—†ì´ ì§„í–‰
def init_video_unsafe(video_path):
    inference_state = predictor.init_state(video_path)  # ì—ëŸ¬ ê°€ëŠ¥ì„±
    return inference_state
```

#### Error Handling

```python
# âœ… Good: ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€
try:
    results = video_annotator.propagate_in_video()
except RuntimeError as e:
    if "CUDA out of memory" in str(e):
        print("GPU ë©”ëª¨ë¦¬ ë¶€ì¡±. í•´ê²° ë°©ë²•:")
        print("1. ë¹„ë””ì˜¤ í•´ìƒë„ ë‚®ì¶”ê¸°")
        print("2. í”„ë ˆì„ ìˆ˜ ì¤„ì´ê¸°")
        print("3. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© (base_plus â†’ tiny)")
    elif "No annotation" in str(e):
        print("ì²« í”„ë ˆì„ì— annotationì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("add_object_annotation() ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
    else:
        print(f"Unexpected error: {e}")
        raise

# âŒ Bad: ì—ëŸ¬ ë¬´ì‹œ
try:
    results = video_annotator.propagate_in_video()
except:
    pass  # Silent failure
```

#### Documentation

```python
# âœ… Good: ëª…í™•í•œ docstring
def propagate_in_video(self) -> Tuple[bool, str, dict]:
    """
    ì „ì²´ ë¹„ë””ì˜¤ì— ëŒ€í•´ ìë™ propagation ì‹¤í–‰

    Requirements:
        - init_video() í˜¸ì¶œ ì™„ë£Œ
        - add_object_annotation() ìµœì†Œ 1ë²ˆ í˜¸ì¶œ

    Returns:
        success (bool): ì„±ê³µ ì—¬ë¶€
        message (str): ìƒíƒœ ë©”ì‹œì§€
        results (dict): {
            frame_idx (int): {
                obj_id (int): mask (np.ndarray, shape=(H, W), dtype=bool)
            }
        }

    Example:
        >>> annotator.init_video("video.mp4")
        >>> annotator.add_object_annotation(0, [(x, y)], [1])
        >>> success, msg, results = annotator.propagate_in_video()
        >>> print(f"Processed {len(results)} frames")

    Raises:
        RuntimeError: Video not initialized or no annotations
    """
    # Implementation
```

### 8.3 ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **torch.compile() í™œì„±í™”**: `vos_optimized=True`
- [ ] **Multi-object ë°°ì¹­**: ì—¬ëŸ¬ ê°ì²´ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
- [ ] **í”„ë ˆì„ ìŠ¤íŠ¸ë¼ì´ë“œ**: ê¸´ ë¹„ë””ì˜¤ëŠ” ìƒ˜í”Œë§
- [ ] **GPU í™œìš©**: CUDA ìš°ì„ , CPU fallback
- [ ] **ë©”ëª¨ë¦¬ ê´€ë¦¬**: FIFO memory bank í¬ê¸° ì¡°ì •
- [ ] **Early stopping**: í’ˆì§ˆ threshold ë„ë‹¬ ì‹œ ì¡°ê¸° ì¢…ë£Œ

---

## 9. ê²°ë¡  ë° Next Steps

### 9.1 ìš”ì•½

#### SAM 2 Video Predictor

**í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜**:
- Memory Bank (FIFO) + Memory Attention (cross-attention)
- í•œ í”„ë ˆì„ annotation â†’ ì „ì²´ ë¹„ë””ì˜¤ ìë™ ì¶”ì 
- Temporal consistency ì•”ë¬µì  í•™ìŠµ

**ì„±ëŠ¥**:
- ~44 FPS (ì‹¤ì‹œê°„)
- Occlusion ìë™ ì²˜ë¦¬
- ê¸´ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥

#### SAM3D

**ì‹¤ì²´**:
- Meta AI ê³µì‹ ëª¨ë¸ (2024ë…„ 11ì›” ë°œí‘œ)
- SAM 3D Objects + SAM 3D Body

**ê¸°ëŠ¥**:
- ë‹¨ì¼ ì´ë¯¸ì§€ â†’ ê³ í’ˆì§ˆ 3D ë©”ì‰¬
- Real-world ì¡°ê±´ robust
- PLY, OBJ, GLB ì¶œë ¥

#### Image vs Video Predictor

**êµ¬ë¶„ ì´ìœ **:
- ì•„í‚¤í…ì²˜ ì°¨ì´ (ë©”ëª¨ë¦¬ ìœ ë¬´)
- Use case ì°¨ì´ (ë…ë¦½ vs ì¶”ì )

**í†µí•© ê°€ëŠ¥ì„±**:
- Unified APIë¡œ ìë™ ì„ íƒ ê°€ëŠ¥
- í•˜ì§€ë§Œ ë‚´ë¶€ì ìœ¼ë¡œëŠ” ë³„ë„ predictor ì‚¬ìš©

### 9.2 ê¶Œì¥ ì‚¬í•­

#### ì¦‰ì‹œ êµ¬í˜„ ê°€ëŠ¥

1. âœ… **VideoAnnotator í´ë˜ìŠ¤ êµ¬í˜„** (2-3ì¼)
   - SAM2 Video Predictor í†µí•©
   - ê¸°ë³¸ annotation ë° propagation API

2. âœ… **Gradio UI ì¶”ê°€** (2-3ì¼)
   - Tab 4: Video Annotation
   - Interactive annotation
   - Export ê¸°ëŠ¥

#### ë‹¨ê¸° ëª©í‘œ (1-2ì£¼)

3. âœ… **SAM3D í†µí•©**
   - VideoAnnotator ê²°ê³¼ â†’ SAM3D 3D ì¬êµ¬ì„±
   - Multi-view fusion

4. âœ… **ì„±ëŠ¥ ìµœì í™”**
   - torch.compile() ì ìš©
   - Multi-object tracking
   - ë©”ëª¨ë¦¬ íš¨ìœ¨í™”

#### ì¥ê¸° ëª©í‘œ (1ê°œì›”+)

5. â° **Advanced Features**
   - Interactive refinement UI
   - Quality metrics (IoU, confidence)
   - Batch processing pipeline

6. â° **Production Ready**
   - Comprehensive testing
   - Documentation
   - Deployment guide

### 9.3 Next Steps

**Phase 1 (This Week)**:
1. VideoAnnotator í´ë˜ìŠ¤ êµ¬í˜„
2. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
3. ê¸°ë³¸ ë™ì‘ ê²€ì¦

**Phase 2 (Next Week)**:
1. Gradio UI í†µí•©
2. Interactive annotation
3. User testing

**Phase 3 (Following Weeks)**:
1. SAM3D integration
2. Performance optimization
3. Documentation

---

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ

1. [SAM 2: Segment Anything in Images and Videos (arXiv)](https://arxiv.org/abs/2408.00714)
2. [Meta AI SAM 2 Blog Post](https://ai.meta.com/blog/segment-anything-2/)
3. [SAM 2 GitHub Repository](https://github.com/facebookresearch/sam2)
4. [Meta AI SAM 3D Official Page](https://ai.meta.com/sam3d/)
5. [SAM 3D Objects GitHub](https://github.com/facebookresearch/sam-3d-objects)
6. [SAM 3D Demo](https://sam3d.org/)

### ê¸°ìˆ  ë¬¸ì„œ

7. [SAM 2 Ultralytics Documentation](https://docs.ultralytics.com/models/sam-2/)
8. [SAM 2 LearnOpenCV Tutorial](https://learnopencv.com/sam-2/)
9. [Roboflow SAM 2 Video Segmentation Guide](https://blog.roboflow.com/sam-2-video-segmentation/)

### ì¶”ê°€ ìë£Œ

10. [SAMURAI: Zero-Shot Visual Tracking (arXiv)](https://arxiv.org/html/2411.11922v1)
11. [HuggingFace SAM2 Video Documentation](https://huggingface.co/docs/transformers/en/model_doc/sam2_video)
12. [Analytics Vidhya SAM 2 Tutorial](https://www.analyticsvidhya.com/blog/2024/08/meta-sam-2/)

---

## ë¶€ë¡: API Reference

### VideoAnnotator API

```python
class VideoAnnotator:
    """SAM2 Video Predictor ê¸°ë°˜ ë¹„ë””ì˜¤ annotation"""

    def __init__(self, sam2_base_path: Path, device: str = "cuda"):
        """ì´ˆê¸°í™”"""

    def load_model(self, model_name: str = "base_plus") -> str:
        """ëª¨ë¸ ë¡œë“œ"""

    def init_video(self, video_path: str) -> Tuple[bool, str, int]:
        """ë¹„ë””ì˜¤ ì´ˆê¸°í™”"""

    def add_object_annotation(
        self,
        frame_idx: int,
        points: List[Tuple[int, int]],
        labels: List[int],
        object_id: Optional[int] = None
    ) -> Tuple[bool, str, int]:
        """ê°ì²´ annotation ì¶”ê°€"""

    def propagate_in_video(self) -> Tuple[bool, str, dict]:
        """ì „ì²´ ë¹„ë””ì˜¤ ìë™ propagation"""

    def get_frame_mask(
        self,
        frame_idx: int,
        object_id: Optional[int] = None
    ) -> Tuple[Optional[np.ndarray], str]:
        """íŠ¹ì • í”„ë ˆì„ ë§ˆìŠ¤í¬ ê°€ì ¸ì˜¤ê¸°"""

    def refine_annotation(
        self,
        frame_idx: int,
        object_id: int,
        additional_points: List[Tuple[int, int]],
        additional_labels: List[int]
    ) -> Tuple[bool, str]:
        """Annotation ìˆ˜ì •"""

    def save_results(self, results: dict, format: str = "png") -> str:
        """ê²°ê³¼ ì €ì¥"""
```

### IntegratedPipeline API

```python
class IntegratedPipeline:
    """SAM2 + SAM3D í†µí•© íŒŒì´í”„ë¼ì¸"""

    def __init__(self, sam2_base_path: Path, sam3d_checkpoint: Path):
        """ì´ˆê¸°í™”"""

    def process_video_to_3d(
        self,
        video_path: str,
        annotation_frame: int = 0,
        annotation_points: List[tuple] = None,
        annotation_labels: List[int] = None,
        reconstruction_frames: List[int] = None,
        multiview: bool = True
    ) -> dict:
        """ë¹„ë””ì˜¤ â†’ 3D ì¬êµ¬ì„±"""
```

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-25
**ì‘ì„±ì**: Claude Code
**í”„ë¡œì íŠ¸**: sam3d_gui

---

## Sources

- [SAM 2: Segment Anything Model 2 - Ultralytics](https://docs.ultralytics.com/models/sam-2/)
- [SAM-2: Memory-Augmented Video Segmentation](https://www.emergentmind.com/topics/segment-anything-model-2-sam-2)
- [How to Use SAM 2 for Video Segmentation - Roboflow](https://blog.roboflow.com/sam-2-video-segmentation/)
- [SAM 2 GitHub Repository](https://github.com/facebookresearch/sam2)
- [Meta AI SAM 3D Official Page](https://ai.meta.com/sam3d/)
- [SAM 3D: High-Fidelity 3D Reconstruction](https://sam3d.org/)
- [Meta AI's New Segment Anything Model: Exploring SAM 3](https://www.ultralytics.com/blog/exploring-sam-3-meta-ais-new-segment-anything-model)
- [SAM 2: Segment Anything in Images and Videos (arXiv)](https://arxiv.org/abs/2408.00714)
- [SAM 3D Objects GitHub](https://github.com/facebookresearch/sam-3d-objects)
- [SAM 2 â€“ Promptable Segmentation for Images and Videos | LearnOpenCV](https://learnopencv.com/sam-2/)
