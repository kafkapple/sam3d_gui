# SAM 3D Memory Optimization Guide

## ë¬¸ì œ ìƒí™©

**Hardware**: NVIDIA GeForce RTX 3060 (12GB VRAM)
**Issue**: SAM 3D ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì‹œ CUDA OOM (Out Of Memory)
**Error Location**: DINO ViT-L ëª¨ë¸ ë¡œë”© ì¤‘ (9.19 GB ì´ë¯¸ í• ë‹¹ ìƒíƒœì—ì„œ ì¶”ê°€ 20 MB í• ë‹¹ ì‹¤íŒ¨)

## ë©”ëª¨ë¦¬ ì‚¬ìš© ë¶„ì„

### ë¡œë”© ìˆœì„œ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

1. **ScaleShift Generator** (ss_generator.ckpt) - ~1.5 GB
2. **SLAT Generator** (slat_generator.ckpt) - ~4.6 GB
3. **ScaleShift Decoder** (ss_decoder.ckpt) - ~0.1 GB
4. **SLAT Decoder GS** (slat_decoder_gs.ckpt) - ~0.2 GB
5. **SLAT Decoder GS 4** (slat_decoder_gs_4.ckpt) - ~0.2 GB
6. **SLAT Decoder Mesh** (slat_decoder_mesh.ckpt) - ~0.3 GB
7. **MoGe Depth Model** (Ruicheng/moge-vitl) - ~1.4 GB
8. **DINO ViT-L** (dinov2_vitl14_reg4) - ~1.2 GB âŒ **OOM ë°œìƒ ì§€ì **

**í•©ê³„**: ~9.5 GB + PyTorch overhead + CUDA context â‰ˆ **10-11 GB**

### 12GB GPUì—ì„œ ì´ˆê³¼í•˜ëŠ” ì´ìœ 

- PyTorch ìì²´ ë©”ëª¨ë¦¬ overhead: ~0.5-1 GB
- CUDA context: ~0.5 GB
- ë©”ëª¨ë¦¬ fragmentation
- **Result**: 12GBì—ì„œ ì‹¤í–‰ ë¶ˆê°€ëŠ¥

## êµ¬í˜„ëœ ìµœì í™” ë°©ì•ˆ

### 1. Lazy Loading (âœ… ì™„ë£Œ)

**Location**: `sam3d_processor.py:89-155`

```python
def initialize_sam3d(self, force_reload: bool = False):
    """
    ëª¨ë¸ì„ ì²˜ìŒ ì‚¬ìš©í•  ë•Œë§Œ ë¡œë“œ
    """
    if self.inference_model is not None and not force_reload:
        print(f"   âœ“ SAM 3D ëª¨ë¸ ì´ë¯¸ ë¡œë“œë¨ (ì¬ì‚¬ìš©)")
        return

    # Clear GPU cache before loading
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        initial_memory = torch.cuda.memory_allocated() / 1024**3
        print(f"ğŸ”¹ GPU ë©”ëª¨ë¦¬ ìƒíƒœ (ë¡œë”© ì „): {initial_memory:.2f} GB")

    # ... model loading ...
```

**íš¨ê³¼**:
- ë¶ˆí•„ìš”í•œ ëª¨ë¸ ë¡œë“œ ë°©ì§€
- ë©”ëª¨ë¦¬ ì¬ì‚¬ìš© ê°€ëŠ¥

### 2. Model Cleanup (âœ… ì™„ë£Œ)

**Location**: `sam3d_processor.py:157-185`

```python
def cleanup_model(self):
    """
    ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ë° GPU ìºì‹œ ì •ë¦¬
    """
    if self.inference_model is not None:
        del self.inference_model
        self.inference_model = None
        self._model_loaded = False

        import gc
        gc.collect()

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
```

**íš¨ê³¼**:
- Inference í›„ ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ
- ë‹¤ìŒ ì‘ì—…ì„ ìœ„í•œ VRAM í™•ë³´

### 3. Memory Monitoring (âœ… ì™„ë£Œ)

**Location**: `sam3d_processor.py:187-223`

```python
def get_memory_status(self) -> Dict:
    """í˜„ì¬ GPU ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    return {
        'allocated_gb': torch.cuda.memory_allocated() / 1024**3,
        'reserved_gb': torch.cuda.memory_reserved() / 1024**3,
        'max_allocated_gb': torch.cuda.max_memory_allocated() / 1024**3,
        'total_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3,
        'model_loaded': self._model_loaded
    }

def print_memory_status(self):
    """ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥"""
    # ... pretty print ...
```

**íš¨ê³¼**:
- ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- OOM ì „ ê²½ê³ 

### 4. FP16 Mixed Precision (âœ… ì™„ë£Œ)

**Location**: `sam3d_processor.py:64-87`, `sam3d_processor.py:462-469`

```python
def __init__(self, sam3d_checkpoint_path: str = None, enable_fp16: bool = True):
    self.enable_fp16 = enable_fp16 and torch.cuda.is_available()

def reconstruct_3d(..., cleanup_after: bool = False):
    if self.enable_fp16 and torch.cuda.is_available():
        print(f"   Using FP16 mixed precision")
        with torch.cuda.amp.autocast():
            output = self.inference_model(frame, mask, seed=seed)
    else:
        output = self.inference_model(frame, mask, seed=seed)
```

**íš¨ê³¼** (ì´ë¡ ì ):
- ~40-50% ë©”ëª¨ë¦¬ ì ˆê°
- **ì£¼ì˜**: SAM 3D ë‚´ë¶€ ëª¨ë¸ë“¤ì´ FP16ì„ ì§€ì›í•´ì•¼ ì‹¤ì œ íš¨ê³¼ ë°œíœ˜

### 5. Auto Cleanup After Inference (âœ… ì™„ë£Œ)

```python
output = processor.reconstruct_3d(
    frame, mask, seed=42,
    cleanup_after=True  # ìë™ ì •ë¦¬ í™œì„±í™”
)
```

**íš¨ê³¼**:
- í•œ ë²ˆ ì‚¬ìš© í›„ ìë™ìœ¼ë¡œ ë©”ëª¨ë¦¬ í•´ì œ
- Batch ì²˜ë¦¬ ì‹œ ìœ ìš©

## í…ŒìŠ¤íŠ¸ ê²°ê³¼

### Test Script
**Location**: `/home/joon/dev/sam3d_gui/test_sam3d_memory.py`

### ê²°ê³¼

```
============================================================
TEST 1: Memory Tracking
============================================================
ğŸ“Š GPU ë©”ëª¨ë¦¬ ìƒíƒœ:
   í• ë‹¹ë¨: 0.00 GB / 11.75 GB
   ìµœëŒ€ ì‚¬ìš©: 0.00 GB
   ëª¨ë¸ ë¡œë“œ ì—¬ë¶€: No
   ì‚¬ìš© ê°€ëŠ¥: 11.75 GB
âœ… Test 1 passed: Memory tracking working

============================================================
TEST 2: Lazy Loading
============================================================
1. Processor created, but model not loaded yet
   âœ“ Model is None (as expected)

2. Call initialize_sam3d()...
ğŸ”¹ GPU ë©”ëª¨ë¦¬ ìƒíƒœ (ë¡œë”© ì „): 0.00 GB
ğŸ”¹ SAM 3D ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...
   ... (checkpoint loading logs) ...

âŒ OOM Error: CUDA out of memory (9.19 GB allocated, 23.94 MB free)
   Location: DINO ViT-L loading
```

## í˜„ì¬ ì œì•½ì‚¬í•­

### RTX 3060 12GBì—ì„œ ë¶ˆê°€ëŠ¥í•œ ì´ìœ 

1. **ëª¨ë¸ í¬ê¸°**: ì „ì²´ íŒŒì´í”„ë¼ì¸ ~10-11 GB
2. **Peak Memory**: ì´ˆê¸°í™” ì‹œ ë” ë§ì€ ë©”ëª¨ë¦¬ í•„ìš” (temporary buffers)
3. **Fragmentation**: ì—°ì†ëœ ë©”ëª¨ë¦¬ ë¸”ë¡ í• ë‹¹ ì–´ë ¤ì›€

### í•„ìš”í•œ ìµœì†Œ VRAM

- **ê¶Œì¥**: 16 GB (RTX 4080, A4000 ì´ìƒ)
- **ìµœì†Œ**: 14-15 GB (ë©”ëª¨ë¦¬ ìµœì í™” ì ìš© ì‹œ)

## ì¶”ê°€ ìµœì í™” ë°©ì•ˆ (ë¯¸êµ¬í˜„)

### Option 1: Model Pruning
- ë¶ˆí•„ìš”í•œ decoder ì œê±° (mesh decoderëŠ” GSë¡œ ëŒ€ì²´ ê°€ëŠ¥)
- ì˜ˆìƒ ì ˆê°: ~0.3-0.5 GB

### Option 2: Quantization
- INT8 quantization ì ìš©
- ì˜ˆìƒ ì ˆê°: ~40% (ì´ë¡ ì )
- **ì£¼ì˜**: ì •í™•ë„ ì†ì‹¤ ê°€ëŠ¥

### Option 3: Gradient Checkpointing
- Inferenceì—ëŠ” ë¶ˆí•„ìš” (training only)

### Option 4: Model Sharding
- CPUì™€ GPU ê°„ ëª¨ë¸ ë¶„í• 
- ì„±ëŠ¥ ì €í•˜ ì‹¬ê° (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)

### Option 5: Sequential Loading
- í•„ìš”í•œ ëª¨ë¸ë§Œ ìˆœì°¨ì ìœ¼ë¡œ ë¡œë“œ
- Preprocessing â†’ Generator â†’ Decoder ìˆœì„œ
- ê° ë‹¨ê³„ í›„ ë©”ëª¨ë¦¬ í•´ì œ
- **ê°€ì¥ í˜„ì‹¤ì ì¸ ë°©ì•ˆ**

## ê¶Œì¥ ì‚¬ìš© ë°©ë²•

### 1. ë‹¨ì¼ Inference (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)

```python
processor = SAM3DProcessor(enable_fp16=True)

try:
    # Inference with auto cleanup
    output = processor.reconstruct_3d(
        frame, mask,
        cleanup_after=True  # ìë™ ì •ë¦¬
    )

    # Use output immediately
    processor.export_mesh(output, 'result.ply')

except RuntimeError as e:
    if "GPU OOM" in str(e):
        print("ë©”ëª¨ë¦¬ ë¶€ì¡±: GPU ì—…ê·¸ë ˆì´ë“œ ë˜ëŠ” ì´ë¯¸ì§€ í•´ìƒë„ ì¶•ì†Œ í•„ìš”")
```

### 2. Batch Processing

```python
processor = SAM3DProcessor(enable_fp16=True)

for frame, mask in data_loader:
    try:
        # Check memory before each inference
        status = processor.get_memory_status()
        if status['allocated_gb'] > 10.0:
            processor.cleanup_model()  # ìˆ˜ë™ ì •ë¦¬

        # Inference
        output = processor.reconstruct_3d(frame, mask)

        # Save result
        save_output(output)

    except RuntimeError:
        processor.cleanup_model()
        continue
```

### 3. Interactive Mode (Web GUI)

```python
# ì²« ì‚¬ìš© ì‹œì—ë§Œ ë¡œë“œ (lazy)
output = processor.reconstruct_3d(frame, mask)

# ì„¸ì…˜ ì¢…ë£Œ ì‹œ ì •ë¦¬
processor.cleanup_model()
```

## ê²°ë¡ 

### ì„±ê³µí•œ ìµœì í™”

âœ… Lazy loadingìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ëª¨ë¸ ë¡œë“œ ë°©ì§€
âœ… Memory cleanupìœ¼ë¡œ ëª…ì‹œì  ë©”ëª¨ë¦¬ ê´€ë¦¬
âœ… Memory monitoringìœ¼ë¡œ ì‹¤ì‹œê°„ ì¶”ì 
âœ… FP16 ì§€ì› ì¶”ê°€ (íš¨ê³¼ëŠ” GPU/ëª¨ë¸ ì˜ì¡´ì )

### ì—¬ì „íˆ í•´ê²°ë˜ì§€ ì•Šì€ ë¬¸ì œ

âŒ RTX 3060 12GBëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë¶ˆê°€ëŠ¥
âŒ ìµœì†Œ 14-16 GB VRAM í•„ìš”
âŒ Sequential loading ë¯¸êµ¬í˜„ (ê°€ì¥ í˜„ì‹¤ì ì¸ í•´ê²°ì±…)

### ì°¨ì„ ì±…

1. **ì´ë¯¸ì§€ í•´ìƒë„ ì¶•ì†Œ**: 518x518 â†’ 384x384 (ë©”ëª¨ë¦¬ ~30% ì ˆê°)
2. **GPU ì—…ê·¸ë ˆì´ë“œ**: RTX 4080 (16GB) ì´ìƒ
3. **Sequential loading êµ¬í˜„**: ë‹¨ê³„ë³„ ëª¨ë¸ ë¡œë“œ/ì–¸ë¡œë“œ

## ì°¸ê³  ìë£Œ

- PyTorch Memory Management: https://pytorch.org/docs/stable/notes/cuda.html#memory-management
- CUDA Out of Memory Guide: https://pytorch.org/docs/stable/notes/cuda.html#cuda-out-of-memory
- Model Optimization Best Practices: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html
